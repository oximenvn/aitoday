<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Chào mừng GPT OSS, dòng mô hình mã nguồn mở mới từ OpenAI! | AI Today - SkyAI</title>

<meta name="description" content="Chào mừng GPT OSS, dòng mô hình mã nguồn mở mới từ OpenAI!">
      <link rel="stylesheet" href="/css/main.min.584aed796ba9d245f84009d57da9662e9c02059611972780263728b442fc25ff.css" integrity="sha256-WErteWup0kX4QAnVfalmLpwCBZYRlyeAJjcotEL8Jf8=" crossorigin="anonymous">

<link rel="icon" type="image/svg+xml" href="https://aitoday.skyai.vn/favicon.svg"> 
<link rel="icon" type="image/x-icon" href="https://aitoday.skyai.vn/favicon.ico"> 
<link rel="icon" type="image/png" sizes="16x16" href="https://aitoday.skyai.vn/favicon.png"> 
<link rel="icon" type="image/png" sizes="32x32" href="https://aitoday.skyai.vn/favicon-32.png"> 
<link rel="icon" type="image/png" sizes="64x64" href="https://aitoday.skyai.vn/favicon-64.png"> 

</head>
<body>
  <header class="py-6 border-b"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8 flex flex-col">
	<div class="flex items-center">
		<div class="flex items-center">
			<button class="flex items-center space-x-2 rounded-full border py-1 pr-[5px] pl-3 group bg-zinc-100 hover:bg-zinc-200 toggle-button" data-target="menu-bar">
				<svg width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
					<path fill-rule="evenodd" d="M2.5 12a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5zm0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5zm0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5z"></path>
				</svg>
				<span class="bg-blue-500 fill-white rounded-full p-1.5">
					<svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="group-hover:animate-spin bi bi-egg-fried fill-white" viewBox="0 0 16 16">
						<path d="M8 11a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"></path>
						<path d="M13.997 5.17a5 5 0 0 0-8.101-4.09A5 5 0 0 0 1.28 9.342a5 5 0 0 0 8.336 5.109 3.5 3.5 0 0 0 5.201-4.065 3.001 3.001 0 0 0-.822-5.216zm-1-.034a1 1 0 0 0 .668.977 2.001 2.001 0 0 1 .547 3.478 1 1 0 0 0-.341 1.113 2.5 2.5 0 0 1-3.715 2.905 1 1 0 0 0-1.262.152 4 4 0 0 1-6.67-4.087 1 1 0 0 0-.2-1 4 4 0 0 1 3.693-6.61 1 1 0 0 0 .8-.2 4 4 0 0 1 6.48 3.273z"></path>
					</svg>
				</span>
			</button>
			<div class="relative rounded-full py-1.5 px-6 bg-zinc-100 hover:bg-zinc-200 text-xl font-bold uppercase mx-2">
				<h2><a class="before:content-[''] before:z-10 before:top-0 before:right-0 before:left-0 before:bottom-0 before:absolute before:pointer-events-auto" href="https://aitoday.skyai.vn/">AI Today - SkyAI</a></h2>
			</div>
		</div>
		<div class="flex items-center ml-auto">
			<button class="flex items-center rounded-full p-3 bg-zinc-100 hover:bg-zinc-200 toggle-button" data-target="search-bar">
				<svg width="18" height="18" fill="currentColor" viewBox="0 0 16 16">
					<path id="path1" class="transition-all ease-linear" d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path>
					<path id="path2" class="transition-all ease-linear hidden" d="M2.146 2.854a.5.5 0 1 1 .708-.708L8 7.293l5.146-5.147a.5.5 0 0 1 .708.708L8.707 8l5.147 5.146a.5.5 0 0 1-.708.708L8 8.707l-5.146 5.147a.5.5 0 0 1-.708-.708L7.293 8 2.146 2.854Z"></path>
				</svg>
			</button>
		</div>
	</div>
  <nav id="menu-bar" class="block mt-3 close">
    <ul class="flex items-center flex flex-nowrap whitespace-nowrap overflow-x-auto space-x-4">
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/">Home</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200">About</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/tags/">Tags</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/tags/">Taxonomy</a>
    </li>
    </ul>
  </nav>
<div id="search-bar" class="block mt-3 close">
	<form id="search" class="flex items-stretch">
		<input class="w-full block px-6 py-2 rounded-l-full focus:outline-none border border-zinc-200" type="text" placeholder="Search...">
		<button class="flex items-center px-7 py-2.5 rounded-r-full border border-zinc-200 ">
			<svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="group-hover:animate-pulse" viewBox="0 0 16 16">
				<path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path>
			</svg>
		</button>
	</form>
</div>
</div></header>
  <main class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8"><div id="breadcrumb" class="max-w-7xl mx-auto py-8">
	<ul class="flex space-x-4 text-sm text-zinc-500">
		<li class="after:content-['❯'] after:ml-4 after:opacity-30 last:after:content-none uppercase">
			<a href="https://aitoday.skyai.vn/">AI Today - SkyAI</a>
		</li>
		<li class="after:content-['❯'] after:ml-4 after:opacity-30 last:after:content-none uppercase">
			<a href="https://aitoday.skyai.vn/posts/">Posts</a>
		</li>
	</ul>
</div><div class="grid grid-cols-1 md:grid-cols-3 gap-6 mb-14">

  <article class="md:col-span-2 prose lg:prose-lg">

    <header class="not-prose">
      
      <h1 id="title" class="text-4xl font-bold leading-normal">Chào mừng GPT OSS, dòng mô hình mã nguồn mở mới từ OpenAI!</h1>

      <div id="lead" class="my-6">

        <p class="font-bold">Chào mừng GPT OSS, dòng mô hình mã nguồn mở mới từ OpenAI! </p>

      </div>
      
      <div id="writer" class="flex items-center space-x-4"><ul class="flex items-center space-x-4 flex-nowrap whitespace-nowrap overflow-x-auto">
          
          <li class="font-semibold my-2"></li>
          
          <li class="before:content-['•'] before:mr-2 before:opacity-50 my-2"><time datetime="2025-08-12T00:00:00&#43;00:00">August 12, 2025</time>
          </li>

          <li class="before:content-['•'] before:mr-2 before:opacity-50 my-2">
            19 min read
          </li>

        </ul>

      </div>
      
    </header>

    <figure id="featureimage" class="rounded-xl aspect-video">

          <img class="rounded-lg" src="https://huggingface.co/blog/assets/openai/openai-hf-thumbnail.png" alt="Chào mừng GPT OSS, dòng mô hình mã nguồn mở mới từ OpenAI!">
        <figcaption class="text-center italic text-xs">Chào mừng GPT OSS, dòng mô hình mã nguồn mở mới từ OpenAI!</figcaption>

    </figure>

    <div id="content" class="mb-14">
      <h1 id="chào-mừng-gpt-oss-dòng-mô-hình-mã-nguồn-mở-mới-từ-openai">Chào mừng GPT OSS, dòng mô hình mã nguồn mở mới từ OpenAI!</h1>
<p>GPT OSS là một bản phát hành mã nguồn mở được mong đợi rất nhiều từ OpenAI, được thiết kế để lý luận mạnh mẽ, các tác vụ tác nhân và các trường hợp sử dụng linh hoạt cho nhà phát triển. Nó bao gồm hai mô hình: một mô hình lớn với 117 tỷ tham số (<!-- raw HTML omitted -->gpt-oss-120b<!-- raw HTML omitted -->) và một mô hình nhỏ hơn với 21 tỷ tham số (<!-- raw HTML omitted -->gpt-oss-20b<!-- raw HTML omitted -->). Cả hai đều là mixture-of-experts (MoE) và sử dụng lược đồ lượng tử hóa 4 bit (MXFP4), cho phép suy luận nhanh (nhờ ít tham số hoạt động hơn, xem chi tiết bên dưới) trong khi vẫn giữ mức sử dụng tài nguyên thấp. Mô hình lớn phù hợp với một GPU H100 duy nhất, trong khi mô hình nhỏ chạy trong vòng 16GB bộ nhớ và hoàn hảo cho phần cứng tiêu dùng và các ứng dụng trên thiết bị.</p>
<p>Để làm cho nó tốt hơn và có tác động hơn nữa đối với cộng đồng, các mô hình được cấp phép theo <strong>giấy phép Apache 2.0</strong>, cùng với chính sách sử dụng tối thiểu:</p>
<blockquote>
<p>Chúng tôi mong muốn các công cụ của mình được sử dụng một cách an toàn, có trách nhiệm và dân chủ, đồng thời tối đa hóa khả năng kiểm soát của bạn đối với cách bạn sử dụng chúng. Bằng cách sử dụng gpt-oss, bạn đồng ý tuân thủ tất cả luật hiện hành.</p></blockquote>
<p>Theo OpenAI, bản phát hành này là một bước có ý nghĩa trong cam kết của họ đối với hệ sinh thái mã nguồn mở, phù hợp với sứ mệnh đã nêu của họ là làm cho lợi ích của AI có thể truy cập rộng rãi. Nhiều trường hợp sử dụng dựa vào các triển khai riêng tư và/hoặc cục bộ, và chúng tôi tại Hugging Face rất vui mừng chào đón <!-- raw HTML omitted -->OpenAI<!-- raw HTML omitted --> đến với cộng đồng. Chúng tôi tin rằng đây sẽ là những mô hình lâu dài, truyền cảm hứng và có tác động.</p>
<h2 id="mục-lục">Mục lục</h2>
<ul>
<li><a href="#welcome-gpt-oss-the-new-open-source-model-family-from-openai">Giới thiệu</a></li>
<li><a href="#overview-of-capabilities-and-architecture">Tổng quan</a></li>
<li><a href="#api-access-through-inference-providers">Truy cập API thông qua các nhà cung cấp suy luận</a></li>
<li><a href="#local-inference">Suy luận cục bộ</a>
<ul>
<li><a href="#using-transformers">Sử dụng transformers</a></li>
<li><a href="#llamacpp">llama.cpp</a></li>
<li><a href="#vllm">vLLM</a></li>
<li><a href="#transformers-serve">transformers serve</a></li>
</ul>
</li>
<li><a href="#fine-tuning">Tinh chỉnh</a></li>
<li><a href="#deploy-on-hugging-face-partners">Triển khai trên các đối tác Hugging Face</a>
<ul>
<li><a href="#azure">Azure</a></li>
<li><a href="#dell">Dell</a></li>
</ul>
</li>
<li><a href="#evaluating-the-model">Đánh giá mô hình</a></li>
<li><a href="#chats-and-chat-templates">Trò chuyện và các mẫu trò chuyện</a>
<ul>
<li><a href="#system-and-developer-messages">Tin nhắn hệ thống và nhà phát triển</a></li>
<li><a href="#tool-use-with-transformers">Sử dụng công cụ với transformers</a></li>
</ul>
</li>
</ul>
<h2 id="tổng-quan-về-khả-năng-và-kiến-trúc">Tổng quan về Khả năng và Kiến trúc</h2>
<ul>
<li>Tổng cộng 21B và 117B tham số, với 3,6B và 5,1B tham số <em>hoạt động</em>, tương ứng.</li>
<li>Lược đồ lượng tử hóa 4-bit sử dụng định dạng <!-- raw HTML omitted -->mxfp4<!-- raw HTML omitted -->. Chỉ áp dụng trên trọng số MoE. Như đã nêu, 120B phù hợp với một GPU 80 GB duy nhất và 20B phù hợp với một GPU 16GB duy nhất.</li>
<li>Mô hình chỉ có văn bản, lý luận; với chuỗi suy nghĩ và mức độ nỗ lực suy luận có thể điều chỉnh.</li>
<li>Hỗ trợ sử dụng công cụ và tuân theo hướng dẫn.</li>
<li>Triển khai suy luận bằng cách sử dụng transformers, vLLM, llama.cpp và ollama.</li>
<li><!-- raw HTML omitted -->Responses API<!-- raw HTML omitted --> được khuyến nghị để suy luận.</li>
<li>Giấy phép: Apache 2.0, với chính sách sử dụng bổ sung nhỏ.</li>
</ul>
<p><strong>Kiến trúc</strong></p>
<ul>
<li>MoE lựa chọn mã thông báo với các kích hoạt SwiGLU.</li>
<li>Khi tính toán trọng số MoE, một softmax được thực hiện trên các chuyên gia đã chọn (softmax-after-topk).</li>
<li>Mỗi lớp chú ý sử dụng RoPE với ngữ cảnh 128K.</li>
<li>Các lớp chú ý thay thế: toàn bộ ngữ cảnh và cửa sổ trượt 128 mã thông báo.</li>
<li>Các lớp chú ý sử dụng <em>attention sink được học</em> trên mỗi đầu, trong đó mẫu số của softmax có một giá trị cộng thêm.</li>
<li>Nó sử dụng cùng một trình mã hóa thông báo như GPT-4o và các mô hình OpenAI API khác.
<ul>
<li>Một số mã thông báo mới đã được tích hợp để cho phép khả năng tương thích với Responses API.</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="truy-cập-api-thông-qua-các-nhà-cung-cấp-suy-luận">Truy cập API thông qua các nhà cung cấp suy luận</h2>
<p>Các mô hình OpenAI GPT OSS có thể truy cập thông qua dịch vụ <!-- raw HTML omitted -->Inference Providers<!-- raw HTML omitted --> của Hugging Face, cho phép bạn gửi yêu cầu đến bất kỳ nhà cung cấp được hỗ trợ nào bằng cùng một mã JavaScript hoặc Python. Đây là cùng một cơ sở hạ tầng cung cấp năng lượng cho bản demo chính thức của OpenAI trên <!-- raw HTML omitted -->gpt-oss.com<!-- raw HTML omitted --> và bạn có thể sử dụng nó cho các dự án của riêng mình.</p>
<p>Dưới đây là một ví dụ sử dụng Python và nhà cung cấp Cerebras siêu nhanh. Để biết thêm thông tin và các đoạn mã bổ sung, hãy kiểm tra <!-- raw HTML omitted -->phần nhà cung cấp suy luận trong thẻ mô hình<!-- raw HTML omitted --> và <!-- raw HTML omitted -->hướng dẫn chuyên dụng mà chúng tôi đã tạo cho các mô hình này<!-- raw HTML omitted -->.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> openai <span style="color:#f92672">import</span> OpenAI
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>client <span style="color:#f92672">=</span> OpenAI(
</span></span><span style="display:flex;"><span>    base_url<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;https://router.huggingface.co/v1&#34;</span>,
</span></span><span style="display:flex;"><span>    api_key<span style="color:#f92672">=</span>os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#34;HF_TOKEN&#34;</span>],
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>completion <span style="color:#f92672">=</span> client<span style="color:#f92672">.</span>chat<span style="color:#f92672">.</span>completions<span style="color:#f92672">.</span>create(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;openai/gpt-oss-120b:cerebras&#34;</span>,
</span></span><span style="display:flex;"><span>    messages<span style="color:#f92672">=</span>[
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;How many rs are in the word &#39;strawberry&#39;?&#34;</span>,
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(completion<span style="color:#f92672">.</span>choices[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>message)
</span></span></code></pre></div><p>Inference Providers cũng triển khai Responses API tương thích với OpenAI, giao diện OpenAI tiên tiến nhất cho các mô hình trò chuyện, được thiết kế để tương tác linh hoạt và trực quan hơn.
Dưới đây là một ví dụ sử dụng Responses API với nhà cung cấp Fireworks AI. Để biết thêm chi tiết, hãy xem dự án <!-- raw HTML omitted -->responses.js<!-- raw HTML omitted --> mã nguồn mở.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> openai <span style="color:#f92672">import</span> OpenAI
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>client <span style="color:#f92672">=</span> OpenAI(
</span></span><span style="display:flex;"><span>    base_url<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;https://router.huggingface.co/v1&#34;</span>,
</span></span><span style="display:flex;"><span>    api_key<span style="color:#f92672">=</span>os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;HF_TOKEN&#34;</span>),
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> client<span style="color:#f92672">.</span>responses<span style="color:#f92672">.</span>create(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;openai/gpt-oss-20b:fireworks-ai&#34;</span>,
</span></span><span style="display:flex;"><span>    input<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;How many rs are in the word &#39;strawberry&#39;?&#34;</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(response)
</span></span></code></pre></div><h2 id="suy-luận-cục-bộ">Suy luận cục bộ</h2>
<h3 id="sử-dụng-transformers">Sử dụng Transformers</h3>
<p>Bạn cần cài đặt bản phát hành <code>transformers</code> mới nhất (v4.55 trở lên), cũng như <code>accelerate</code> và <code>kernels</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>pip install --upgrade accelerate transformers kernels
</span></span></code></pre></div><p>Trọng số mô hình được lượng tử hóa ở định dạng <code>mxfp4</code>, tương thích với GPU của các dòng Hopper hoặc Blackwell. Điều này bao gồm các thẻ trung tâm dữ liệu như H100, H200 hoặc GB200, cũng như các GPU tiêu dùng mới nhất trong dòng 50xx. Nếu bạn có một trong các thẻ này, <code>mxfp4</code> sẽ mang lại kết quả tốt nhất về tốc độ và mức tiêu thụ bộ nhớ. Để sử dụng nó, bạn cần <code>triton 3.4</code> và <code>triton_kernels</code>. Nếu các thư viện này không được cài đặt (hoặc bạn không có GPU tương thích), việc tải mô hình sẽ quay trở lại <code>bfloat16</code>, được giải nén từ trọng số đã lượng tử hóa.</p>
<p>Trong các thử nghiệm của chúng tôi, Triton 3.4 hoạt động tốt với phiên bản PyTorch mới nhất (2.7.x). Bạn có thể tùy chọn cài đặt PyTorch 2.8 thay thế – đó là phiên bản tiền phát hành tại thời điểm viết bài (<!-- raw HTML omitted -->mặc dù nó sẽ sớm được phát hành<!-- raw HTML omitted -->), nhưng đó là phiên bản đã được chuẩn bị cùng với triton 3.4, vì vậy chúng ổn định cùng nhau. Dưới đây là cách cài đặt PyTorch 2.8 (đi kèm với triton 3.4) và các kernel triton:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># Bước tùy chọn nếu bạn muốn PyTorch 2.8, nếu không chỉ cần `pip install torch`</span>
</span></span><span style="display:flex;"><span>pip install torch<span style="color:#f92672">==</span>2.8.0 --index-url https://download.pytorch.org/whl/test/cu128
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Cài đặt triton kernels để hỗ trợ mxfp4</span>
</span></span><span style="display:flex;"><span>pip install git+https://github.com/triton-lang/triton.git@main#subdirectory<span style="color:#f92672">=</span>python/triton_kernels
</span></span></code></pre></div><p>Đoạn mã sau đây cho thấy suy luận đơn giản với mô hình 20B. Nó chạy trên GPU 16 GB khi sử dụng <code>mxfp4</code> hoặc ~48 GB trong <code>bfloat16</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoModelForCausalLM, AutoTokenizer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;openai/gpt-oss-20b&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(model_id)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AutoModelForCausalLM<span style="color:#f92672">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>    model_id,
</span></span><span style="display:flex;"><span>    device_map<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;auto&#34;</span>,
</span></span><span style="display:flex;"><span>    torch_dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;auto&#34;</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>messages <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;How many rs are in the word &#39;strawberry&#39;?&#34;</span>},
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>inputs <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>apply_chat_template(
</span></span><span style="display:flex;"><span>    messages,
</span></span><span style="display:flex;"><span>    add_generation_prompt<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>,
</span></span><span style="display:flex;"><span>    return_dict<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>)<span style="color:#f92672">.</span>to(model<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>generated <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>generate(<span style="color:#f92672">**</span>inputs, max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)
</span></span><span style="display:flex;"><span>print(tokenizer<span style="color:#f92672">.</span>decode(generated[<span style="color:#ae81ff">0</span>][inputs[<span style="color:#e6db74">&#34;input_ids&#34;</span>]<span style="color:#f92672">.</span>shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]:]))
</span></span></code></pre></div><h4 id="flash-attention-3">Flash Attention 3</h4>
<p>Các mô hình sử dụng <em>attention sink</em>, một kỹ thuật mà nhóm vLLM đã làm cho tương thích với Flash Attention 3. Chúng tôi đã đóng gói và tích hợp kernel được tối ưu hóa của họ trong <!-- raw HTML omitted --><code>kernels-community/vllm-flash-attn3</code><!-- raw HTML omitted -->. Tại thời điểm viết bài, kernel siêu nhanh này đã được kiểm tra trên các thẻ Hopper với PyTorch 2.7 và 2.8. Chúng tôi hy vọng sẽ tăng cường phạm vi bảo hiểm trong những ngày tới. Nếu bạn chạy các mô hình trên thẻ Hopper (ví dụ: H100 hoặc H200), bạn cần <code>pip install --upgrade kernels</code> và thêm dòng sau vào đoạn mã của mình:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-diff" data-lang="diff"><span style="display:flex;"><span>from transformers import AutoModelForCausalLM, AutoTokenizer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_id = &#34;openai/gpt-oss-20b&#34;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tokenizer = AutoTokenizer.from_pretrained(model_id)
</span></span><span style="display:flex;"><span>model = AutoModelForCausalLM.from_pretrained(
</span></span><span style="display:flex;"><span>    model_id,
</span></span><span style="display:flex;"><span>    device_map=&#34;auto&#34;,
</span></span><span style="display:flex;"><span>    torch_dtype=&#34;auto&#34;,
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">+    # Flash Attention with Sinks
</span></span></span><span style="display:flex;"><span><span style="color:#a6e22e">+    attn_implementation=&#34;kernels-community/vllm-flash-attn3&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#a6e22e"></span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>messages = [
</span></span><span style="display:flex;"><span>    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;How many rs are in the word &#39;strawberry&#39;?&#34;},
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>inputs = tokenizer.apply_chat_template(
</span></span><span style="display:flex;"><span>    messages,
</span></span><span style="display:flex;"><span>    add_generation_prompt=True,
</span></span><span style="display:flex;"><span>    return_tensors=&#34;pt&#34;,
</span></span><span style="display:flex;"><span>    return_dict=True,
</span></span><span style="display:flex;"><span>).to(model.device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>generated = model.generate(**inputs, max_new_tokens=100)
</span></span><span style="display:flex;"><span>print(tokenizer.decode(generated[0][inputs[&#34;input_ids&#34;].shape[-1]:]))
</span></span></code></pre></div><p>Đoạn mã này sẽ tải xuống mã kernel được tối ưu hóa, được biên dịch trước từ <code>kernels-community</code>, như được giải thích trong <!-- raw HTML omitted -->bài đăng trên blog trước đây của chúng tôi<!-- raw HTML omitted -->. Nhóm transformers đã xây dựng, đóng gói và kiểm tra mã, vì vậy bạn hoàn toàn có thể sử dụng nó một cách an toàn.</p>
<h4 id="các-tối-ưu-hóa-khác">Các tối ưu hóa khác</h4>
<p>Nếu bạn có GPU Hopper trở lên, chúng tôi khuyên bạn nên sử dụng <code>mxfp4</code> vì những lý do đã giải thích ở trên. Nếu bạn có thể sử dụng thêm Flash Attention 3, thì hãy bật nó!</p>
<blockquote>
<p>Nếu GPU của bạn không tương thích với <code>mxfp4</code>, thì chúng tôi khuyên bạn nên sử dụng kernel MegaBlocks MoE để tăng tốc độ tốt. Để làm như vậy, bạn chỉ cần điều chỉnh mã suy luận của mình như sau:</p></blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-diff" data-lang="diff"><span style="display:flex;"><span>from transformers import AutoModelForCausalLM, AutoTokenizer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_id = &#34;openai/gpt-oss-20b&#34;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tokenizer = AutoTokenizer.from_pretrained(model_id)
</span></span><span style="display:flex;"><span>model = AutoModelForCausalLM.from_pretrained(
</span></span><span style="display:flex;"><span>    model_id,
</span></span><span style="display:flex;"><span>    device_map=&#34;auto&#34;,
</span></span><span style="display:flex;"><span>    torch_dtype=&#34;auto&#34;,
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">+    # Optimize MoE layers with downloadable` MegaBlocksMoeMLP
</span></span></span><span style="display:flex;"><span><span style="color:#a6e22e">+    use_kernels=True,
</span></span></span><span style="display:flex;"><span><span style="color:#a6e22e"></span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>messages = [
</span></span><span style="display:flex;"><span>    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;How many rs are in the word &#39;strawberry&#39;?&#34;},
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>inputs = tokenizer.apply_chat_template(
</span></span><span style="display:flex;"><span>    messages,
</span></span><span style="display:flex;"><span>    add_generation_prompt=True,
</span></span><span style="display:flex;"><span>    tokenize=True,
</span></span><span style="display:flex;"><span>    return_tensors=&#34;pt&#34;,
</span></span><span style="display:flex;"><span>    return_dict=True,
</span></span><span style="display:flex;"><span>).to(model.device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>generated = model.generate(**inputs, max_new_tokens=100)
</span></span><span style="display:flex;"><span>print(tokenizer.decode(generated[0][inputs[&#34;input_ids&#34;].shape[-1]:]))
</span></span></code></pre></div><blockquote>
<p>Kernel MoE được tối ưu hóa của MegaBlocks yêu cầu mô hình chạy trên <code>bfloat16</code>, vì vậy mức tiêu thụ bộ nhớ sẽ cao hơn so với chạy trên <code>mxfp4</code>. Chúng tôi khuyên bạn nên sử dụng <code>mxfp4</code> nếu bạn có thể, nếu không, hãy chọn MegaBlocks thông qua <code>use_kernels=True</code>.</p></blockquote>
<h4 id="hỗ-trợ-amd-rocm">Hỗ trợ AMD ROCm</h4>
<p>OpenAI GPT OSS đã được xác minh trên phần cứng AMD Instinct và chúng tôi rất vui mừng thông báo hỗ trợ ban đầu cho nền tảng ROCm của AMD trong thư viện kernel của chúng tôi, đặt nền tảng cho các kernel ROCm được tối ưu hóa sắp tới trong Transformers. Khả năng tăng tốc kernel MegaBlocks MoE đã có sẵn cho OpenAI GPT OSS trên AMD Instinct (ví dụ: dòng MI300), cho phép hiệu suất đào tạo và suy luận tốt hơn. Bạn có thể kiểm tra nó với cùng một mã suy luận được hiển thị ở trên.</p>
<p>AMD cũng đã chuẩn bị một <!-- raw HTML omitted -->Không gian<!-- raw HTML omitted --> Hugging Face để người dùng thử mô hình trên phần cứng AMD.</p>
<h4 id="tóm-tắt-các-tối-ưu-hóa-có-sẵn">Tóm tắt các Tối ưu hóa có sẵn</h4>
<p>Tại thời điểm viết bài, bảng này tóm tắt các <em>đề xuất</em> của chúng tôi dựa trên khả năng tương thích GPU và các thử nghiệm của chúng tôi. Chúng tôi hy vọng Flash Attention 3 (với attention sink) sẽ tương thích với các GPU bổ sung.</p>
<!-- raw HTML omitted -->
<p>Mặc dù mô hình 120B phù hợp với một GPU H100 duy nhất (sử dụng <code>mxfp4</code>), bạn cũng có thể chạy nó dễ dàng trên nhiều GPU bằng cách sử dụng <code>accelerate</code> hoặc <code>torchrun</code>. Transformers cung cấp một kế hoạch song song hóa mặc định và bạn cũng có thể tận dụng các kernel chú ý được tối ưu hóa. Đoạn mã sau đây có thể được chạy với <code>torchrun --nproc_per_node=4 generate.py</code> trên một hệ thống có 4 GPU:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoModelForCausalLM, AutoTokenizer
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers.distributed <span style="color:#f92672">import</span> DistributedConfig
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;openai/gpt-oss-120b&#34;</span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(model_path, padding_side<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;left&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>device_map <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;tp_plan&#34;</span>: <span style="color:#e6db74">&#34;auto&#34;</span>,    <span style="color:#75715e"># Bật song song hóa Tensor</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AutoModelForCausalLM<span style="color:#f92672">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>    model_path,
</span></span><span style="display:flex;"><span>    torch_dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;auto&#34;</span>,
</span></span><span style="display:flex;"><span>    attn_implementation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;kernels-community/vllm-flash-attn3&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">**</span>device_map,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>messages <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>     {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;Explain how expert parallelism works in large language models.&#34;</span>}
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>inputs <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>apply_chat_template(
</span></span><span style="display:flex;"><span>    messages,
</span></span><span style="display:flex;"><span>    add_generation_prompt<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>,
</span></span><span style="display:flex;"><span>    return_dict<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>)<span style="color:#f92672">.</span>to(model<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>generate(<span style="color:#f92672">**</span>inputs, max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Giải mã và in</span>
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>decode(outputs[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Model response:&#34;</span>, response<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;&lt;|channel|&gt;final&lt;|message|&gt;&#34;</span>)[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>strip())
</span></span></code></pre></div><p>Các mô hình OpenAI GPT OSS đã được đào tạo rộng rãi để tận dụng việc sử dụng công cụ như một phần trong nỗ lực lý luận của chúng. Mẫu trò chuyện mà chúng tôi đã tạo cho transformers cung cấp rất nhiều tính linh hoạt, vui lòng xem <!-- raw HTML omitted -->phần chuyên dụng của chúng tôi sau trong bài đăng này<!-- raw HTML omitted -->.</p>
<h3 id="llamacpp">Llama.cpp</h3>
<p>Llama.cpp cung cấp hỗ trợ MXFP4 gốc với Flash Attention, mang lại hiệu suất tối ưu trên nhiều backend khác nhau như Metal, CUDA và Vulkan, ngay từ bản phát hành ngày 0.</p>
<p>Để cài đặt nó, hãy làm theo hướng dẫn trong <!-- raw HTML omitted -->kho lưu trữ Github của llama.cpp<!-- raw HTML omitted -->.</p>
<pre tabindex="0"><code># MacOS
brew install llama.cpp

# Windows
winget install llama.cpp
</code></pre><p>Cách được khuyến nghị là sử dụng nó thông qua llama-server:</p>
<pre tabindex="0"><code>llama-server -hf ggml-org/gpt-oss-120b-GGUF -c 0 -fa --jinja --reasoning-format none

# Sau đó, truy cập http://localhost:8080
</code></pre><p>Chúng tôi hỗ trợ cả mô hình 120B và 20B. Để biết thêm thông tin chi tiết, hãy truy cập <!-- raw HTML omitted -->PR này<!-- raw HTML omitted --> hoặc <!-- raw HTML omitted -->bộ sưu tập mô hình GGUF<!-- raw HTML omitted -->.</p>
<h3 id="vllm">vLLM</h3>
<p>Như đã đề cập, vLLM đã phát triển các kernel Flash Attention 3 được tối ưu hóa hỗ trợ chú ý sink, vì vậy bạn sẽ nhận được kết quả tốt nhất trên các thẻ Hopper. Cả Chat Completion và Responses API đều được hỗ trợ. Bạn có thể cài đặt và khởi động máy chủ với đoạn mã sau đây, giả định sử dụng 2 GPU H100:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>vllm serve openai/gpt-oss-120b --tensor-parallel-size <span style="color:#ae81ff">2</span>
</span></span></code></pre></div><p>Hoặc, sử dụng trực tiếp trong Python như:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> vllm <span style="color:#f92672">import</span> LLM
</span></span><span style="display:flex;"><span>llm <span style="color:#f92672">=</span> LLM(<span style="color:#e6db74">&#34;openai/gpt-oss-120b&#34;</span>, tensor_parallel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>output <span style="color:#f92672">=</span> llm<span style="color:#f92672">.</span>generate(<span style="color:#e6db74">&#34;San Francisco is a&#34;</span>)
</span></span></code></pre></div><h3 id="transformers-serve"><code>transformers serve</code></h3>
<p>Bạn có thể sử dụng <!-- raw HTML omitted --><code>transformers serve</code><!-- raw HTML omitted --> để thử nghiệm cục bộ với các mô hình, mà không cần bất kỳ phụ thuộc nào khác. Bạn có thể khởi chạy máy chủ chỉ bằng:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>transformers serve
</span></span></code></pre></div><p>Mà bạn có thể gửi các yêu cầu bằng cách sử dụng <!-- raw HTML omitted -->Responses API<!-- raw HTML omitted -->.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># responses API</span>
</span></span><span style="display:flex;"><span>curl -X POST http://localhost:8000/v1/responses <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>-H <span style="color:#e6db74">&#34;Content-Type: application/json&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>-d <span style="color:#e6db74">&#39;{&#34;input&#34;: [{&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;hello&#34;}], &#34;temperature&#34;: 1.0, &#34;stream&#34;: true, &#34;model&#34;: &#34;openai/gpt-oss-120b&#34;}&#39;</span>
</span></span></code></pre></div><p>Bạn cũng có thể gửi các yêu cầu bằng cách sử dụng Completions API tiêu chuẩn:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># completions API</span>
</span></span><span style="display:flex;"><span>curl -X POST http://localhost:8000/v1/chat/completions <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>-H <span style="color:#e6db74">&#34;Content-Type: application/json&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>-d <span style="color:#e6db74">&#39;{&#34;messages&#34;: [{&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;hello&#34;}], &#34;temperature&#34;: 1.0, &#34;max_tokens&#34;: 1000, &#34;stream&#34;: true, &#34;model&#34;: &#34;openai/gpt-oss-120b&#34;}&#39;</span>
</span></span></code></pre></div><h2 id="tinh-chỉnh">Tinh chỉnh</h2>
<p>Các mô hình GPT OSS được tích hợp đầy đủ với <code>trl</code>. Chúng tôi đã phát triển một vài ví dụ tinh chỉnh bằng cách sử dụng <code>SFTTrainer</code> để giúp bạn bắt đầu:</p>
<ul>
<li>Một ví dụ LoRA trong <!-- raw HTML omitted -->sổ tay nấu ăn OpenAI<!-- raw HTML omitted -->, cho thấy cách có thể tinh chỉnh mô hình để lý luận bằng nhiều ngôn ngữ.</li>
<li><!-- raw HTML omitted -->Một tập lệnh tinh chỉnh cơ bản<!-- raw HTML omitted --> mà bạn có thể điều chỉnh cho phù hợp với nhu cầu của mình.</li>
</ul>
<h2 id="triển-khai-trên-các-đối-tác-hugging-face">Triển khai trên các đối tác Hugging Face</h2>
<h3 id="azure">Azure</h3>
<p>Hugging Face hợp tác với Azure trên Danh mục Mô hình AI của Azure để mang các mô hình mã nguồn mở phổ biến nhất — trải rộng trên văn bản, hình ảnh, giọng nói và các tác vụ đa phương thức — trực tiếp vào môi trường của khách hàng để triển khai an toàn vào các điểm cuối trực tuyến được quản lý, tận dụng cơ sở hạ tầng cấp doanh nghiệp, tự động mở rộng quy mô và giám sát của Azure.</p>
<p>Các mô hình GPT OSS hiện có sẵn trên Danh mục Mô hình AI của Azure (<!-- raw HTML omitted -->GPT OSS 20B<!-- raw HTML omitted -->, <!-- raw HTML omitted -->GPT OSS 120B<!-- raw HTML omitted -->), sẵn sàng được triển khai đến các điểm cuối trực tuyến để suy luận theo thời gian thực.</p>
<!-- raw HTML omitted -->
<h3 id="dell">Dell</h3>
<p>Dell Enterprise Hub là một cổng thông tin trực tuyến an toàn giúp đơn giản hóa việc đào tạo và triển khai các mô hình AI mở mới nhất tại chỗ bằng cách sử dụng các nền tảng của Dell. Được phát triển với sự hợp tác của Dell, nó cung cấp các container được tối ưu hóa, hỗ trợ gốc cho phần cứng Dell và các tính năng bảo mật cấp doanh nghiệp.</p>
<p>Các mô hình GPT OSS hiện có sẵn trên <!-- raw HTML omitted -->Dell Enterprise Hub<!-- raw HTML omitted -->, sẵn sàng được triển khai tại chỗ bằng cách sử dụng các nền tảng của Dell.</p>
<!-- raw HTML omitted -->
<h2 id="đánh-giá-mô-hình">Đánh giá mô hình</h2>
<p>Các mô hình GPT OSS là các mô hình lý luận: do đó, chúng yêu cầu kích thước tạo rất lớn (số lượng mã thông báo mới tối đa) để đánh giá, vì quá trình tạo của chúng sẽ chứa lý luận trước, sau đó là câu trả lời thực tế. Sử dụng kích thước tạo quá nhỏ có nguy cơ làm gián đoạn dự đoán ở giữa quá trình lý luận, điều này sẽ gây ra âm tính giả. Dấu vết lý luận sau đó cần được loại bỏ khỏi câu trả lời của mô hình trước khi tính toán các số liệu, để tránh lỗi phân tích cú pháp, đặc biệt là với các đánh giá về toán học hoặc hướng dẫn.</p>
<p>Dưới đây là một ví dụ về cách đánh giá các mô hình bằng lighteval (bạn cần cài đặt từ nguồn).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>git clone https://github.com/huggingface/lighteval
</span></span><span style="display:flex;"><span>pip install -e .<span style="color:#f92672">[</span>dev<span style="color:#f92672">]</span> <span style="color:#75715e"># đảm bảo bạn đã cài đặt đúng phiên bản transformers!</span>
</span></span><span style="display:flex;"><span>lighteval accelerate <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    <span style="color:#e6db74">&#34;model_name=openai/gpt-oss-20b,max_length=16384,skip_special_tokens=False,generation_parameters={temperature:1,top_p:1,top_k:40,min_p:0,max_new_tokens:16384}&#34;</span> <span style="color:#ae81ff">\ </span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;extended|ifeval|0|0,lighteval|aime25|0|0&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --save-details --output-dir <span style="color:#e6db74">&#34;openai_scores&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --remove-reasoning-tags --reasoning-tags<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;[(&#39;&lt;|channel|&gt;analysis&lt;|message|&gt;&#39;, &#39;&lt;|end|&gt;&lt;|start|&gt;assistant&lt;|channel|&gt;final&lt;|message|&gt;&#39;)]&#34;</span> 
</span></span></code></pre></div><p>Đối với mô hình 20B, điều này sẽ cung cấp cho bạn 69,5 (+/-1,9) cho IFEval (lời nhắc nghiêm ngặt) và 63,3 (+/-8,9) cho AIME25 (trong pass@1), các điểm số nằm trong phạm vi dự kiến đối với một mô hình lý luận có kích thước này.</p>
<p>Nếu bạn muốn thực hiện tập lệnh đánh giá tùy chỉnh của mình, lưu ý rằng để lọc ra các thẻ lý luận một cách chính xác, bạn sẽ cần sử dụng <code>skip_special_tokens=False</code> trong tokenizer, để nhận được dấu vết đầy đủ trong đầu ra của mô hình (để lọc lý luận bằng cùng các cặp chuỗi như trong ví dụ trên) - bạn có thể khám phá lý do bên dưới.</p>
<h2 id="trò-chuyện-và-các-mẫu-trò-chuyện">Trò chuyện và các mẫu trò chuyện</h2>
<p>OpenAI GPT OSS sử dụng khái niệm &ldquo;kênh&rdquo; trong các đầu ra của nó. Hầu hết thời gian, bạn sẽ thấy một kênh &ldquo;phân tích&rdquo; chứa những thứ không nhằm mục đích gửi cho người dùng cuối, như các chuỗi suy nghĩ và một kênh &ldquo;cuối cùng&rdquo; chứa các tin nhắn thực sự có ý định hiển thị cho người dùng.</p>
<p>Giả sử không có công cụ nào đang được sử dụng, cấu trúc của đầu ra mô hình trông như thế này:</p>
<pre tabindex="0"><code>&lt;|start|&gt;assistant&lt;|channel|&gt;analysis&lt;|message|&gt;CHAIN_OF_THOUGHT&lt;|end|&gt;&lt;|start|&gt;assistant&lt;|channel|&gt;final&lt;|message|&gt;ACTUAL_MESSAGE
</code></pre><p>Hầu hết thời gian, bạn nên bỏ qua mọi thứ ngoại trừ văn bản sau <strong>&lt;|channel|&gt;final&lt;|message|&gt;</strong>. Chỉ văn bản này nên được thêm vào cuộc trò chuyện dưới dạng tin nhắn của trợ lý hoặc hiển thị cho người dùng. Tuy nhiên, có hai ngoại lệ đối với quy tắc này: Bạn có thể cần phải bao gồm các tin nhắn <strong>phân tích</strong> trong lịch sử trong quá trình <strong>đào tạo</strong> hoặc nếu mô hình đang <strong>gọi các công cụ bên ngoài</strong>.</p>
<p><strong>Khi đào tạo:</strong>
Nếu bạn đang định dạng các ví dụ cho đào tạo, bạn thường muốn đưa chuỗi suy nghĩ vào tin nhắn cuối cùng. Vị trí thích hợp để làm điều này là trong khóa <strong>suy nghĩ</strong>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>chat <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;Hi there!&#34;</span>},
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;assistant&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;Hello!&#34;</span>},
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;Can you think about this one?&#34;</span>},
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;assistant&#34;</span>, <span style="color:#e6db74">&#34;thinking&#34;</span>: <span style="color:#e6db74">&#34;Thinking real hard...&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;Okay!&#34;</span>}
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># add_generation_prompt=False thường chỉ được sử dụng trong đào tạo, không phải suy luận</span>
</span></span><span style="display:flex;"><span>inputs <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>apply_chat_template(chat, add_generation_prompt<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span></code></pre></div><p>Bạn có thể thoải mái bao gồm các khóa <strong>suy nghĩ</strong> trong các lượt trước đó hoặc khi bạn đang thực hiện suy luận hơn là đào tạo, nhưng chúng thường sẽ bị bỏ qua. Mẫu trò chuyện sẽ chỉ bao gồm chuỗi suy nghĩ gần đây nhất và chỉ trong quá trình đào tạo (khi <code>add_generation_prompt=False</code> và lượt cuối cùng là một lượt của trợ lý).</p>
<p>Lý do chúng tôi làm theo cách này là rất tế nhị: Các mô hình OpenAI gpt-oss đã được đào tạo trên dữ liệu nhiều lượt trong đó tất cả các chuỗi suy nghĩ trừ chuỗi suy nghĩ cuối cùng đã bị loại bỏ. Điều này có nghĩa là khi bạn muốn tinh chỉnh một mô hình OpenAI <code>gpt-oss</code>, bạn nên làm tương tự.</p>
<ul>
<li>Hãy để mẫu trò chuyện loại bỏ tất cả các chuỗi suy nghĩ trừ chuỗi suy nghĩ cuối cùng</li>
<li>Che nhãn trên tất cả các lượt trừ lượt trợ lý cuối cùng, nếu không bạn sẽ đào tạo nó trên các lượt trước đó mà không có chuỗi suy nghĩ, điều này sẽ dạy nó phát ra các phản hồi mà không cần CoT. Điều này có nghĩa là bạn không thể đào tạo trên toàn bộ cuộc trò chuyện nhiều lượt làm một mẫu duy nhất; thay vào đó, bạn phải chia nó thành một mẫu cho mỗi lượt trợ lý chỉ với lượt trợ lý cuối cùng không bị che mỗi lần, để mô hình có thể học hỏi từ mỗi lượt trong khi vẫn chỉ thấy chính xác một chuỗi suy nghĩ trên tin nhắn cuối cùng mỗi lần.</li>
</ul>
<h3 id="tin-nhắn-hệ-thống-và-nhà-phát-triển">Tin nhắn Hệ thống và Nhà phát triển</h3>
<p>OpenAI GPT OSS là bất thường vì nó phân biệt giữa tin nhắn &ldquo;hệ thống&rdquo; và tin nhắn &ldquo;nhà phát triển&rdquo; khi bắt đầu cuộc trò chuyện, nhưng hầu hết các mô hình khác chỉ sử dụng &ldquo;hệ thống&rdquo;. Trong GPT OSS, tin nhắn hệ thống tuân theo một định dạng nghiêm ngặt và chứa thông tin như ngày hiện tại, danh tính mô hình và mức độ nỗ lực suy luận để sử dụng, và tin nhắn &ldquo;nhà phát triển&rdquo; tự do hơn, điều này làm cho nó (rất khó hiểu) tương tự như các tin nhắn &ldquo;hệ thống&rdquo; của hầu hết các mô hình khác.</p>
<p>Để giúp GPT OSS dễ sử dụng hơn với API tiêu chuẩn, mẫu trò chuyện sẽ coi một tin nhắn có vai trò &ldquo;hệ thống&rdquo; hoặc &ldquo;nhà phát triển&rdquo; là tin nhắn <strong>nhà phát triển</strong>. Nếu bạn muốn sửa đổi tin nhắn hệ thống thực tế, bạn có thể truyền các đối số cụ thể <strong>model_identity</strong> hoặc <strong>reasoning_effort</strong> cho mẫu trò chuyện:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>chat <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;system&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;This will actually become a developer message!&#34;</span>}
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tokenizer<span style="color:#f92672">.</span>apply_chat_template(
</span></span><span style="display:flex;"><span>    chat, 
</span></span><span style="display:flex;"><span>    model_identity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;You are OpenAI GPT OSS.&#34;</span>,
</span></span><span style="display:flex;"><span>    reasoning_effort<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;high&#34;</span>  <span style="color:#75715e"># Mặc định là &#34;medium&#34;, nhưng cũng chấp nhận &#34;high&#34; và &#34;low&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h3 id="sử-dụng-công-cụ-với-transformers">Sử dụng công cụ với Transformers</h3>
<p>GPT OSS hỗ trợ hai loại công cụ: Các công cụ &ldquo;tích hợp&rdquo; <strong>trình duyệt</strong> và <strong>python</strong> và các công cụ tùy chỉnh do người dùng cung cấp. Để kích hoạt các công cụ tích hợp, hãy truyền tên của chúng trong một danh sách cho đối số <strong>builtin_tools</strong> của mẫu trò chuyện, như được hiển thị bên dưới. Để truyền các công cụ tùy chỉnh, bạn có thể truyền chúng dưới dạng lược đồ JSON hoặc dưới dạng các hàm Python với các gợi ý loại và docstring bằng cách sử dụng đối số tools. Xem <!-- raw HTML omitted -->tài liệu công cụ mẫu trò chuyện<!-- raw HTML omitted --> để biết thêm chi tiết hoặc bạn chỉ cần sửa đổi ví dụ bên dưới:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_current_weather</span>(location: str):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Trả về trạng thái thời tiết hiện tại tại một địa điểm nhất định dưới dạng một chuỗi.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        location: Địa điểm để lấy thời tiết.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;Terrestrial.&#34;</span>  <span style="color:#75715e"># Chúng tôi chưa bao giờ nói đây là một công cụ thời tiết tốt</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>chat <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;What&#39;s the weather in Paris right now?&#34;</span>}
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>inputs <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>apply_chat_template(
</span></span><span style="display:flex;"><span>    chat, 
</span></span><span style="display:flex;"><span>    tools<span style="color:#f92672">=</span>[weather_tool], 
</span></span><span style="display:flex;"><span>    builtin_tools<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;browser&#34;</span>, <span style="color:#e6db74">&#34;python&#34;</span>],
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">### [Link bài viết gốc](https://huggingface.co/blog/welcome-openai-gpt-oss)</span>
</span></span></code></pre></div>

    <ul id="taxonomy" class="not-prose flex items-center space-x-4 flex-nowrap whitespace-nowrap overflow-x-auto">

      <li class="font-semibold my-4">Tags:</li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/ai/">Ai</a></li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/august-5-2025/">August 5, 2025</a></li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/huggingface.co/">Huggingface.co</a></li>
    </ul>
</div>

    <footer id="content-footer" class="not-prose">

      

      <div id="author-box" class="my-8 md:my-14 border p-8 bg-zinc-100 rounded-2xl">

        <img class="w-20 h-20 rounded-full" src="" alt="">

        <h2 class="text-xl font-bold my-4">Writter by : </h2>

        <p class="mb-4"></p>

        <ul class="flex flex-wrap space-x-4">
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://facebook.com/luat.skyai/" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                <path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951z"/>
              </svg>
              <span class="ml-2">Facebook</span>
            </a>
          </li>
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://facebook.com/luat.skyai/" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                  <path d="M12.6.75h2.454l-5.36 6.142L16 15.25h-4.937l-3.867-5.07-4.425 5.07H.316l5.733-6.57L0 .75h5.063l3.495 4.633L12.601.75Zm-.86 13.028h1.36L4.323 2.145H2.865l8.875 11.633Z"/>
              </svg>
              <span class="ml-2">Twitter</span>
            </a>
          </li>
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://luat.skyai.vn/" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                    <path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.917 3.917 0 0 0-1.417.923A3.927 3.927 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.916 3.916 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.926 3.926 0 0 0-.923-1.417A3.911 3.911 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0h.003zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599.28.28.453.546.598.92.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.47 2.47 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.478 2.478 0 0 1-.92-.598 2.48 2.48 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233 0-2.136.008-2.388.046-3.231.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92.28-.28.546-.453.92-.598.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045v.002zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92zm-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217zm0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334z"/>

              </svg>
              <span class="ml-2">Instagram</span>
            </a>
          </li>

        </ul>
        
      </div>

      <div id="related-post" class="">

        <h2 class="text-xl md:text-2xl font-bold mb-6 md:mb-8">Recommended for You</h2><div class="grid grid-cols-1 md:grid-cols-2 gap-6"><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl">
	
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-08-11/523b4c/"></a>

	<figure class="w-full aspect-video overflow-hidden rounded-3xl">

			<img class="object-cover group-hover:scale-105 transition duration-500 cursor-pointer" src="https://huggingface.co/blog/assets/aisheets/aisheets-thumb.gif" alt="Giới thiệu AI Sheets- một công cụ để làm việc với các bộ dữ liệu bằng cách sử dụng các mô hình AI mở!" style="width: 100%; height: 100%; object-fit: cover;">

	  </figure>

	<div class="p-6">

		<time datetime="2025-08-11T00:00:00&#43;00:00">Aug 11, 2025</time>

		<h3 class="my-4 text-2xl font-bold">Giới thiệu AI Sheets- một công cụ để làm việc với các bộ dữ liệu bằng cách sử dụng các mô hình AI mở!</h3>

		<p class="text-normal leading-normal text-zinc-500 line-clamp-2"></p>

	</div>
</article><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl">
	
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-08-11/c4a805/"></a>

	<figure class="w-full aspect-video overflow-hidden rounded-3xl">

			<img class="object-cover group-hover:scale-105 transition duration-500 cursor-pointer" src="https://huggingface.co/avatars/huggingface.co/v1/production/uploads/6306ae10df993a789e6a75b2/qjfU7Z4jBf7_B_oZBnUw0.jpeg" alt="Những gì tôi đã học được khi nâng cấp ảnh Midjourney đường dài với Stable Diffusion PLUS mở hộp Qwen Image &amp; Wan 2.2" style="width: 100%; height: 100%; object-fit: cover;">

	  </figure>

	<div class="p-6">

		<time datetime="2025-08-11T00:00:00&#43;00:00">Aug 11, 2025</time>

		<h3 class="my-4 text-2xl font-bold">Những gì tôi đã học được khi nâng cấp ảnh Midjourney đường dài với Stable Diffusion PLUS mở hộp Qwen Image &amp; Wan 2.2</h3>

		<p class="text-normal leading-normal text-zinc-500 line-clamp-2">Những gì tôi đã học được khi nâng cấp ảnh Midjourney đường dài với Stable Diffusion PLUS mở hộp Qwen Image &amp; Wan 2.2</p>

	</div>
</article>
          
        </div>
      </div>

    </footer>

  </article>

  
  <aside class="md:col-span-1"><div class="lg:sticky lg:top-8"><div class="rounded-2xl p-4 bg-zinc-100 mb-10">

		<img class="aspect-video rounded" src="/images/email-3249062_1280_by-Muhammad-Ribkhan-Pixabay.jpg" alt="Banner Email Marketing">

		<p class="text-right text-xs mt-2 leading-none text-zinc-500">Adversitiment</p>

</div><div class="space-y-6">

		<h2 class="font-bold text-xl mb-8">Recent Post</h2>

		<article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-10-27/18e1a6/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://huggingface.co/blog/assets/lerobot-release-v040/thumbnail.png" alt="LeRobot v0.4.0- Siêu nạp cho học tập Robotics OSS" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">LeRobot v0.4.0- Siêu nạp cho học tập Robotics OSS</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-10-27/f11de9/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/ddad92700787ec1bf1d80359c0c5e6ca305682b0-1000x1000.svg" alt="Mở rộng việc sử dụng TPU và Dịch vụ Google Cloud của chúng tôi" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Mở rộng việc sử dụng TPU và Dịch vụ Google Cloud của chúng tôi</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-10-27/555574/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/b5c98d26c46edc43193e7f7e28a00633a538bb9c-1000x1000.svg" alt="Seoul trở thành văn phòng thứ ba của Anthropic ở Châu Á - Thái Bình Dương khi chúng tôi tiếp tục phát triển quốc tế" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Seoul trở thành văn phòng thứ ba của Anthropic ở Châu Á - Thái Bình Dương khi chúng tôi tiếp tục phát triển quốc tế</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-10-27/7f9a8f/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://huggingface.co/blog/assets/huggingface-hub-v1/thumbnail.png" alt="Trung tâm Hugging Face v1.0- Năm năm xây dựng nền tảng của Máy học Mở" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Trung tâm Hugging Face v1.0- Năm năm xây dựng nền tảng của Máy học Mở</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-10-23/e3340c/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">&#39;AI đang xé nát các công ty&#39;- CEO của Writer AI chỉ trích các nhà lãnh đạo Fortune 500 vì quản lý sai công nghệ</h3>
	</div>
</article>
		
	</div>

</div>
</aside>

</div>
  
</main>
  <footer class="bg-zinc-100 py-10 md:py-14"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8">

  <div class="flex flex-wrap space-y-6 mb-4">

    <div class="w-full md:w-3/5 flex flex-col space-y-4 md:pr-8 lg:pr-10">

      <a class="flex items-center group" href="https://aitoday.skyai.vn/">
        <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" fill="currentColor" class="mr-2 group-hover:animate-spin" viewBox="0 0 16 16">
          <path d="M8 11a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/>
          <path d="M13.997 5.17a5 5 0 0 0-8.101-4.09A5 5 0 0 0 1.28 9.342a5 5 0 0 0 8.336 5.109 3.5 3.5 0 0 0 5.201-4.065 3.001 3.001 0 0 0-.822-5.216zm-1-.034a1 1 0 0 0 .668.977 2.001 2.001 0 0 1 .547 3.478 1 1 0 0 0-.341 1.113 2.5 2.5 0 0 1-3.715 2.905 1 1 0 0 0-1.262.152 4 4 0 0 1-6.67-4.087 1 1 0 0 0-.2-1 4 4 0 0 1 3.693-6.61 1 1 0 0 0 .8-.2 4 4 0 0 1 6.48 3.273z"/>
        </svg>
        
        <span class="text-2xl font-semibold uppercase">AI Today - SkyAI</span>
      </a>

      <p class="font-semibold">
        Theo dõi các tiến bộ mới nhất về Trí tuệ nhân tạo.<br>Trực tiếp từ các nhà phát hành AI trên thế giới.
      </p>

      <p>Đem trí tuệ nhân tạo đến mọi người dân, doanh nghiệp Việt, góp phần giúp Việt Nam phát triển mạnh mẽ trong kỷ nguyên số. Nội dung được cập nhật tự động bằng máy.</p>

    </div>

    <div class="self-center flex flex-col w-full md:w-2/5">

<ul id="social-media" class="flex items-center space-x-4">
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.facebook.com/luat.skyai/" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 16 16">
				<path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.twitter.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M12.6.75h2.454l-5.36 6.142L16 15.25h-4.937l-3.867-5.07-4.425 5.07H.316l5.733-6.57L0 .75h5.063l3.495 4.633L12.601.75Zm-.86 13.028h1.36L4.323 2.145H2.865l8.875 11.633Z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.github.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.instagram.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.917 3.917 0 0 0-1.417.923A3.927 3.927 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.916 3.916 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.926 3.926 0 0 0-.923-1.417A3.911 3.911 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0h.003zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599.28.28.453.546.598.92.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.47 2.47 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.478 2.478 0 0 1-.92-.598 2.48 2.48 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233 0-2.136.008-2.388.046-3.231.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92.28-.28.546-.453.92-.598.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045v.002zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92zm-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217zm0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334z"></path>
			</svg>
		</a>
	</li>
</ul>

    </div>
    
  </div>

  <div class="my-8">
    <ul class="flex items-center space-x-4">
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/">Home</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="">About</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/tags/">Tags</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/tags/">Taxonomy</a></li>
      
    </ul>
  </div>

  <div class="border-t pt-4">

    <p class="text-sm">Copyright © 2025. All rights reserved. <a href="https://home.skyai.vn/" target="_blank" rel="noopener">SkyAI.vn</a></p>

  </div>
  
</div>
</footer>
      <script defer src="/js/insertoggle.474f9b0e08021c6519cff4e46df14ccf148285b2d3a23d6321d6e10f25c291fb.js" integrity="sha256-R0&#43;bDggCHGUZz/TkbfFMzxSChbLToj1jIdbhDyXCkfs=" crossorigin="anonymous"></script>
</body>
</html>