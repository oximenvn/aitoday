<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Differential Transformer V2 | AI Today - SkyAI</title>

<meta name="description" content="Differential Transformer V2">
      <link rel="stylesheet" href="/css/main.min.584aed796ba9d245f84009d57da9662e9c02059611972780263728b442fc25ff.css" integrity="sha256-WErteWup0kX4QAnVfalmLpwCBZYRlyeAJjcotEL8Jf8=" crossorigin="anonymous">

<link rel="icon" type="image/svg+xml" href="https://aitoday.skyai.vn/favicon.svg"> 
<link rel="icon" type="image/x-icon" href="https://aitoday.skyai.vn/favicon.ico"> 
<link rel="icon" type="image/png" sizes="16x16" href="https://aitoday.skyai.vn/favicon.png"> 
<link rel="icon" type="image/png" sizes="32x32" href="https://aitoday.skyai.vn/favicon-32.png"> 
<link rel="icon" type="image/png" sizes="64x64" href="https://aitoday.skyai.vn/favicon-64.png"> 

</head>
<body>
  <header class="py-6 border-b"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8 flex flex-col">
	<div class="flex items-center">
		<div class="flex items-center">
			<button class="flex items-center space-x-2 rounded-full border py-1 pr-[5px] pl-3 group bg-zinc-100 hover:bg-zinc-200 toggle-button" data-target="menu-bar">
				<svg width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
					<path fill-rule="evenodd" d="M2.5 12a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5zm0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5zm0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5z"></path>
				</svg>
				<span class="bg-blue-500 fill-white rounded-full p-1.5">
					<svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="group-hover:animate-spin bi bi-egg-fried fill-white" viewBox="0 0 16 16">
						<path d="M8 11a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"></path>
						<path d="M13.997 5.17a5 5 0 0 0-8.101-4.09A5 5 0 0 0 1.28 9.342a5 5 0 0 0 8.336 5.109 3.5 3.5 0 0 0 5.201-4.065 3.001 3.001 0 0 0-.822-5.216zm-1-.034a1 1 0 0 0 .668.977 2.001 2.001 0 0 1 .547 3.478 1 1 0 0 0-.341 1.113 2.5 2.5 0 0 1-3.715 2.905 1 1 0 0 0-1.262.152 4 4 0 0 1-6.67-4.087 1 1 0 0 0-.2-1 4 4 0 0 1 3.693-6.61 1 1 0 0 0 .8-.2 4 4 0 0 1 6.48 3.273z"></path>
					</svg>
				</span>
			</button>
			<div class="relative rounded-full py-1.5 px-6 bg-zinc-100 hover:bg-zinc-200 text-xl font-bold uppercase mx-2">
				<h2><a class="before:content-[''] before:z-10 before:top-0 before:right-0 before:left-0 before:bottom-0 before:absolute before:pointer-events-auto" href="https://aitoday.skyai.vn/">AI Today - SkyAI</a></h2>
			</div>
		</div>
		<div class="flex items-center ml-auto">
			<button class="flex items-center rounded-full p-3 bg-zinc-100 hover:bg-zinc-200 toggle-button" data-target="search-bar">
				<svg width="18" height="18" fill="currentColor" viewBox="0 0 16 16">
					<path id="path1" class="transition-all ease-linear" d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path>
					<path id="path2" class="transition-all ease-linear hidden" d="M2.146 2.854a.5.5 0 1 1 .708-.708L8 7.293l5.146-5.147a.5.5 0 0 1 .708.708L8.707 8l5.147 5.146a.5.5 0 0 1-.708.708L8 8.707l-5.146 5.147a.5.5 0 0 1-.708-.708L7.293 8 2.146 2.854Z"></path>
				</svg>
			</button>
		</div>
	</div>
  <nav id="menu-bar" class="block mt-3 close">
    <ul class="flex items-center flex flex-nowrap whitespace-nowrap overflow-x-auto space-x-4">
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/">Home</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200">About</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/tags/">Tags</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/tags/">Taxonomy</a>
    </li>
    </ul>
  </nav>
<div id="search-bar" class="block mt-3 close">
	<form id="search" class="flex items-stretch">
		<input class="w-full block px-6 py-2 rounded-l-full focus:outline-none border border-zinc-200" type="text" placeholder="Search...">
		<button class="flex items-center px-7 py-2.5 rounded-r-full border border-zinc-200 ">
			<svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="group-hover:animate-pulse" viewBox="0 0 16 16">
				<path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path>
			</svg>
		</button>
	</form>
</div>
</div></header>
  <main class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8"><div id="breadcrumb" class="max-w-7xl mx-auto py-8">
	<ul class="flex space-x-4 text-sm text-zinc-500">
		<li class="after:content-['‚ùØ'] after:ml-4 after:opacity-30 last:after:content-none uppercase">
			<a href="https://aitoday.skyai.vn/">AI Today - SkyAI</a>
		</li>
		<li class="after:content-['‚ùØ'] after:ml-4 after:opacity-30 last:after:content-none uppercase">
			<a href="https://aitoday.skyai.vn/posts/">Posts</a>
		</li>
	</ul>
</div><div class="grid grid-cols-1 md:grid-cols-3 gap-6 mb-14">

  <article class="md:col-span-2 prose lg:prose-lg">

    <header class="not-prose">
      
      <h1 id="title" class="text-4xl font-bold leading-normal">Differential Transformer V2</h1>

      <div id="lead" class="my-6">

        <p class="font-bold">Differential Transformer V2 </p>

      </div>
      
      <div id="writer" class="flex items-center space-x-4"><ul class="flex items-center space-x-4 flex-nowrap whitespace-nowrap overflow-x-auto">
          
          <li class="font-semibold my-2"></li>
          
          <li class="before:content-['‚Ä¢'] before:mr-2 before:opacity-50 my-2"><time datetime="2026-02-02T00:00:00&#43;00:00">February 2, 2026</time>
          </li>

          <li class="before:content-['‚Ä¢'] before:mr-2 before:opacity-50 my-2">
            12 min read
          </li>

        </ul>

      </div>
      
    </header>

    <figure id="featureimage" class="rounded-xl aspect-video">

          <img class="rounded-lg" src="http://cdn-avatars.huggingface.co/v1/production/uploads/1583646260758-5e64858c87403103f9f1055d.png" alt="Differential Transformer V2">
        <figcaption class="text-center italic text-xs">Differential Transformer V2</figcaption>

    </figure>

    <div id="content" class="mb-14">
      <h1 id="differential-transformer-v2">Differential Transformer V2</h1>
<p>Tianzhu Ye, Li Dong, Yutao Sun, Furu Wei</p>
<p><a href="https://github.com/microsoft/unilm/blob/master/Diff-Transformer/Diff-Transformer-V2">Github Link</a></p>
<p><a href="https://spiky-homegrown-4cb.notion.site/Differential-Transformer-V2-2e7baa052def80ecaa93d4d67d125417">Notion Link (for better readability)</a></p>
<!-- raw HTML omitted -->
<h2 id="abstract">Abstract</h2>
<p>We introduce <strong>Differential Transformer V2</strong> (DIFF V2), an improved version of <a href="https://arxiv.org/abs/2410.05258">Differential Transformer</a> (DIFF V1). This revision focuses on inference efficiency, training stability for production-level LLMs, and architectural elegance.</p>
<p>Key improvements:</p>
<ol>
<li>
<p><strong>Faster Inference &amp; No Need of Custom Attention Kernels</strong>
Instead of forcing the attention parameter count to match the baseline <a href="https://arxiv.org/abs/1706.03762">Transformer</a> (as in DIFF V1), we introduce additional parameters (borrowed from other parts of the model) for $Q_2$.
This design allows DIFF V2 to match the baseline Transformer&rsquo;s decoding speed and directly use <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention</a> without custom kernels.</p>
</li>
<li>
<p><strong>Improved Training Stability</strong>
We remove the per-head RMSNorm after differential attention. We find the per-head RMSNorm can lead to instability in later stages of large-scale pretraining of LLM.</p>
</li>
<li>
<p><strong>Simpler Parameterization &amp; Initialization</strong>
We replace the globally shared $\lambda$ with a token-specific, head-wise projected $\lambda$. This eliminates the exponential re-parameterization and initialization of $\lambda$.</p>
</li>
</ol>
<p>We conduct pretraining experiments on production-scale LLMs, including dense models and a 30A3 MoE on trillions of tokens using large learning rate of 6e-4 to 1e-3. Experimental observations:</p>
<ul>
<li><strong>Notably lower language modeling loss</strong> compared to Transformer.</li>
<li><strong>Reduced loss and gradient spikes during training</strong>, particularly under large learning rate settings where the Transformer baseline becomes unstable.</li>
<li><strong>Reduced activation outliers magnitude.</strong></li>
</ul>
<p>The experiments are still running. We expect to explore in later stages of training:</p>
<ul>
<li>If learning efficiency is improved in mid- and post-training.</li>
<li>If performance on downstream long-context benchmarks improves (alleviating context rot).</li>
</ul>
<p>After the experiments complete and we evaluate the results, we will prepare a more formal report.</p>
<!-- raw HTML omitted -->
<h2 id="code">Code</h2>
<p>We compare DIFF V2 with DIFF V1 below:
(For simplicity, we omit the batch dimension and assume that both the input and output of the following <code>flash_attn_func</code> are three-dimensional tensors <code>(tokens, heads, head dimension)</code>. Heads belonging to the same GQA group are arranged contiguously in the output)</p>
<p><strong>Note DIFF V2 subtracts two heads that are in the same GQA group, which means they share the same key and value. This is crucial to performance.</strong> See design ablations section and Github code.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">DiffAttnV1</span>(
</span></span><span style="display:flex;"><span>        layer_index, q1, q2, k1, k2, v,
</span></span><span style="display:flex;"><span>        lam_q1, lam_k1, lam_q2, lam_k2,
</span></span><span style="display:flex;"><span>):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      q1, q2: (N, h/2, d)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      k1, k2: (N, h_kv/2, d)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      v:      (N, h_kv/2, 2d)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      lam_*: (d,)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>      attn1 <span style="color:#f92672">=</span> flash_attn_func(q1, k1, v)
</span></span><span style="display:flex;"><span>        attn2 <span style="color:#f92672">=</span> flash_attn_func(q2, k2, v)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        lam_init <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.8</span> <span style="color:#f92672">-</span> <span style="color:#ae81ff">0.6</span> <span style="color:#f92672">*</span>
</span></span><span style="display:flex;"><span>            exp(<span style="color:#f92672">-</span><span style="color:#ae81ff">0.3</span> <span style="color:#f92672">*</span> layer_index)
</span></span><span style="display:flex;"><span>        lam1 <span style="color:#f92672">=</span> exp(sum(lam_q1 <span style="color:#f92672">*</span> lam_k1)
</span></span><span style="display:flex;"><span>    lam2 <span style="color:#f92672">=</span> exp(sum(lam_q2 <span style="color:#f92672">*</span> lam_k2)
</span></span><span style="display:flex;"><span>    lam <span style="color:#f92672">=</span> lam1 <span style="color:#f92672">-</span> lam2 <span style="color:#f92672">+</span> lam_init
</span></span><span style="display:flex;"><span>    attn <span style="color:#f92672">=</span> attn1 <span style="color:#f92672">-</span> lam <span style="color:#f92672">*</span> attn2
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    attn <span style="color:#f92672">=</span> rmsnorm(attn)
</span></span><span style="display:flex;"><span>    attn <span style="color:#f92672">=</span> attn <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> lam_init)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> attn
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">DiffAttnV2</span>(
</span></span><span style="display:flex;"><span>        q, k, v, lam
</span></span><span style="display:flex;"><span>):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      q:   (N, 2h, d)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      k:   (N, h_kv, d)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      v:   (N, h_kv, d)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      lam: (N, h, 1)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        attn <span style="color:#f92672">=</span> flash_attn_func(q, k, v)
</span></span><span style="display:flex;"><span>        attn1, attn2 <span style="color:#f92672">=</span> (attn[:, <span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">2</span>], 
</span></span><span style="display:flex;"><span>                        attn[:, <span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">2</span>])
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        lam_val <span style="color:#f92672">=</span> sigmoid(lam)
</span></span><span style="display:flex;"><span>        attn <span style="color:#f92672">=</span> attn1 <span style="color:#f92672">-</span> lam_val <span style="color:#f92672">*</span> attn2
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> attn
</span></span></code></pre></div><p>Full code at: <a href="https://github.com/microsoft/unilm/tree/master/Diff-Transformer/Diff-Transformer-V2">unilm/Diff-Transformer/Diff-Transformer-V2 at master ¬∑ microsoft/unilm</a>
In the script, <code>h</code> represents number of query heads, <code>h_kv</code> represents number of key-value heads, and <code>d</code> means head dimension. The $\lambda$ in DIFF V2 is projected from $X$ for each token each head.</p>
<p>DIFF V2 doubles number of query heads while maintaining number of key value heads, and the extra dimension is reduced back to $h*d$ after the differential operation so the $W_O$ projection remains the same as baseline Transformer.</p>
<h2 id="motivation">Motivation</h2>
<h3 id="faster-decoding--no-custom-kernels"><strong>Faster Decoding &amp; No Custom Kernels</strong></h3>
<p>DIFF V2 introduces additional query heads compared to the baseline Transformer, <strong>but does not increase the number of key-value (KV) heads</strong>. Since LLM decoding is typically memory-bound, this design allows DIFF V2 to achieve decoding speeds on par with standard Transformer. <strong>Besides, since head dimension is aligned between query, key and value, there is no need for custom attention kernels for DIFF V2</strong>. In contrast, DIFF V1 can be slower during decoding because the value cache must be loaded twice, and a custom attention kernel is needed. DIFF V2 can also increase the arithmetic intensity of the attention module during decoding.</p>
<p><strong>During pretraining</strong>, when using cutting-edge FlashAttention kernels on H-series and B-series GPUs, the throughput reduction introduced by DIFF V2 is negligible. <strong>For long-sequence prefilling</strong>, we recommend combining DIFF V2 with techniques such as <a href="https://arxiv.org/abs/2405.05254">YOCO</a> (also used in <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma3n/modeling_gemma3n.py">Gemma 3n</a>), which already reduces prefilling complexity to linear time with respect to sequence length.</p>
<p><strong>An alternative perspective is to compare DIFF V2 with a Transformer that has the same query dimension</strong> $2h*d$. Under this comparison, both models exhibit same attention kernel speed, while DIFF V2 has less parameters and flops in output projection.</p>
<h3 id="softmax-magnitude-constraint"><strong>Softmax Magnitude Constraint</strong></h3>
<p>In the standard Scaled Dot-Product Attention (SDPA), let $Q, K, V \in \mathbb{R}^{n \times d}$ be the queries, keys, and values. The context vector $C$ is defined as:
$C = \text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V = AV$
Where $A \in \mathbb{R}^{n \times n}$ is the attention weight matrix. Let&rsquo;s focus on a single row of $C$, denoted as $\mathbf{c}_i$, which is a weighted sum of value vectors $\mathbf{v}<em>j$:
$\mathbf{c}<em>i = \sum</em>{j=1}^{n} a</em>{ij} \mathbf{v}<em>j$
We define the <strong>Context RMS</strong> (Root Mean Square) to represent the magnitude of this output:
$\text{RMS}(\mathbf{c}<em>i) = \sqrt{\frac{1}{d} |\mathbf{c}<em>i|^2}$
The weights $a</em>{ij}$ are non-negative and sum to 1 ($ \sum</em>{j=1}^{n} a</em>{ij} = 1 $). Assume the value vectors $\mathbf{v}_j$ are uncorrelated and have an RMS of 1, <strong>the Context RMS is strictly bounded in range $[\frac{1}{\sqrt{n}},1)$ however the attention distribution changes</strong>:</p>
<ul>
<li>If the attention is focused entirely on one token, the Context RMS is $1$.</li>
<li>If the attention is spread equally across all tokens ($a_{ij} = \frac{1}{n}$), the Context RMS drops to $\frac{1}{\sqrt{n}}$.</li>
</ul>
<p>In DIFF V1 we add a per-head RMSNorm on context vectors:
$\mathbf{\hat{c}}_i = \frac{\mathbf{c}_i}{\text{RMS}(\mathbf{c}_i)}$
If the model learns a uniform attention distribution in a head, the Context RMS is approximately $1/\sqrt{n}$. To normalize this back to $1$, RMSNorm must multiply the vector by a scale of $\sqrt{n}$. For $n = 8192$, $\sqrt{n} \approx 90.5$. This means the RMSNorm layer applies a <strong>100x</strong> magnification to the output. In large-scale pretraining, we find this leads to massive gradients and numerical instability.</p>
<p>A typical phenomenon is that when DIFF V1 is pre-trained at a large learning rate, the gradient norm experiences a larger increase compared to Transformer in the later stages, along with higher variance. <strong>In DIFF V2, after removing the per-head RMSNorm, the gradient norm scale becomes comparable to that of Transformer, and the gradient norm spike is reduced</strong> (will be discussed further below).</p>
<p>We adopted the per-head RMSNorm design in DIFF V1 primarily because of the doubled value head dimension and the globally shared $\lambda$ across all tokens. Given the modifications made to these two aspects in DIFF V2, we found that removing RMSNorm is now safe.</p>
<h3 id="beyond-softmax-constraint--elimination-of-attention-sinks"><strong>Beyond Softmax Constraint &amp; Elimination of Attention Sinks</strong></h3>
<p>We demonstrate DIFF V2 can overcome the constraint of Softmax mentioned above. It can also help eliminate <a href="https://arxiv.org/abs/2309.17453">attention sinks</a>.</p>
<ul>
<li>
<p>In original Softmax attention:
$a_{ij} = \text{Softmax}(z_{ij}) = \frac{\exp(z_{ij})}{\sum_{k=1}^{n} \exp(z_{ik})} \
\mathbf{c}<em>i = \sum</em>{j=1}^{n} a_{ij} \mathbf{v}<em>j = \sum</em>{j=1}^{n} \text{Softmax}(z_{ij}) \mathbf{v}_j \
\text{RMS}(\mathbf{c}_i) \in \left[\frac{1}{\sqrt{n}},1\right)$</p>
</li>
<li>
<p>In DIFF V2 we introduce a projected $\lambda_i$ for each token and each head:
$\mathbf{c}<em>i = \sum</em>{j=1}^{n} (\text{Softmax}(z_{ij}^\text{1}) - \text{sigmoid}(\lambda_i) \cdot \text{Softmax}(z_{ij}^\text{2})) \mathbf{v}_j \
\text{RMS}(\mathbf{c}_i) \in \left(0, \sqrt{2}\right)$
The projected $\lambda_i$ helps to control the context RMS. We observe that <strong>lowering the lower bound of the context RMS to zero is particularly important</strong>. <strong>It can help eliminate attention sinks and improve training stability</strong>. The upper bound only needs to remain bounded.</p>
<p>Note that our analysis here consider RMS before output projection $W_O$. Although the RMS can be recovered and adjusted after the output projection, the lack of freedom at Softmax still affects the learning performance.</p>
<p>Other recent works alleviate this constraint as well:</p>
</li>
<li>
<p>In <a href="https://www.evanmiller.org/attention-is-off-by-one.html">Attention Is Off By One</a>:
$a_{ij}^{\text{off}} = \frac{\exp(z_{ij})}{1 + \sum_{k=1}^{n} \exp(z_{ik})} \
\ \
\mathbf{c}<em>i = \sum</em>{j=1}^{n} a_{ij}^{\text{off}} \mathbf{v}<em>j = \frac{\sum</em>{k=1}^{n} \exp(z_{ik})}{1 + \sum_{k=1}^{n} \exp(z_{ik})} \sum_{j=1}^{n} \text{Softmax}(z_{ij}) \mathbf{v}_j \
\ \
\text{RMS}(\mathbf{c}_i) \in \left(0, 1\right)$</p>
</li>
<li>
<p>In <a href="https://openai.com/index/introducing-gpt-oss/">gpt-oss</a>, a learnable scalar $s$ is introduced for each head:
$a_{ij}^{\text{oss}} = \frac{\exp(z_{ij})}{\exp(s) + \sum_{k=1}^{n} \exp(z_{ik})} \
\ \
\mathbf{c}<em>i = \sum</em>{j=1}^{n} a_{ij}^{\text{oss}} \mathbf{v}<em>j = \frac{\sum</em>{k=1}^{n} \exp(z_{ik})}{\exp(s) + \sum_{k=1}^{n} \exp(z_{ik})} \sum_{j=1}^{n} \text{Softmax}(z_{ij}) \mathbf{v}_j \
\ \
\text{RMS}(\mathbf{c}_i) \in \left(0, 1\right)$</p>
</li>
<li>
<p>In <a href="https://arxiv.org/abs/2505.06708">Gated Attention</a>, a projected element-wise sigmoid gate is multiplied:
$\mathbf{c}<em>i = \text{sigmoid} (\mathbf{g}<em>i) \odot \sum</em>{j=1}^{n} \text{Softmax}(z</em>{ij}) \mathbf{v}_j \
\text{RMS}(\mathbf{c}_i) \in \left(0, 1\right)$</p>
</li>
</ul>
<h2 id="experimental-observations">Experimental Observations</h2>
<p>We conduct pretraining experiments on production-scale LLMs, including dense models and a 30A3 MoE on trillions of tokens using large learning rate of 6e-4 to 1e-3.</p>
<p>The experiments are still running. What we have observed now:</p>
<ul>
<li><strong>Notably lower language modeling loss</strong> compared to Transformer (a gap of 0.02 to 0.03 at 1T training tokens).</li>
<li><strong>Reduced loss and gradient spikes during training</strong>, particularly under large learning rate settings where the Transformer baseline becomes unstable.</li>
<li><strong>Reduced activation outliers magnitude.</strong></li>
</ul>
<p>We expect to explore in later stages of training:</p>
<ul>
<li>Learning efficiency in mid- and post-training.</li>
<li>Performance on downstream long-context benchmarks (alleviating context rot).</li>
</ul>
<h2 id="discussions">Discussions</h2>
<h3 id="construction-of-differential-operation">Construction of Differential Operation</h3>
<p>In theory, a standard Transformer with $2h$ attention heads can learn the differential operation by learning $W_O^{2i}=-W_O^{2i+1}, i=0,1,\ldots,h-1$, where $W_O^{i}$ denotes the output projection of head $i$, and head $2i$ and $2i+1$ belong to the same GQA group.</p>
<p><strong>Assumption 1.</strong> In practice, such a solution is difficult to learn through optimization, as it requires two sets of parameters to converge to exact negatives of each other.</p>
<p><strong>Assumption 2.</strong> The differential operation can be learned by the model and the model chooses to learn it in the training. <strong>Then explicitly constructing it before the output projection as in DIFF V2 can save half of the $W_O$ parameters</strong>. The number of saved parameters is also non-trivial. Under the current GQA setting, the parameters in the attention module are dominated by $W_Q$ and $W_O$; Therefore, approximately <strong>25% of the attention-module parameters can be saved.</strong> The saved parameter budget can then be reallocated to other parts of the model.</p>
<p>Even if DIFF V2, after reallocating parameters, does not achieve a lower loss than the baseline but merely matches it, <strong>the method is still worthwhile if it provides additional benefits</strong> such as improved training stability, better control of outliers, or higher training efficiency. This is analogous to <a href="https://arxiv.org/abs/2305.13245">GQA</a>, which matches the loss of MHA while reducing KV-cache as an additional benefit. So the key question becomes empirical performance.</p>
<h3 id="design-ablations">Design Ablations</h3>
<ol>
<li>
<p>Subtracting two heads that are <strong>not</strong> in the same GQA group, which means they <strong>do not</strong> share the same key and value.</p>
<p>(For simplicity, we omit the batch dimension and assume that both the input and output of the following <code>flash_attn_func</code> are three-dimensional tensors <code>(tokens, heads, head dimension)</code>. Heads belonging to the same GQA group are arranged contiguously in the output)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Ablation 1</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ‚ùå Wrong Implementation of DIFF V2!</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>attn <span style="color:#f92672">=</span> flash_attn_func(q, k, v)
</span></span><span style="display:flex;"><span>nh <span style="color:#f92672">=</span> attn<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>attn1, attn2 <span style="color:#f92672">=</span> (attn[:, :nh<span style="color:#f92672">//</span><span style="color:#ae81ff">2</span>], 
</span></span><span style="display:flex;"><span>                    attn[:, nh<span style="color:#f92672">//</span><span style="color:#ae81ff">2</span>:])
</span></span><span style="display:flex;"><span><span style="color:#f92672">...</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># DIFF V2</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ‚úÖ Correct Implementation of DIFF V2</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>attn <span style="color:#f92672">=</span> flash_attn_func(q, k, v)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>attn1, attn2 <span style="color:#f92672">=</span> (attn[:, <span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">2</span>], 
</span></span><span style="display:flex;"><span>                    attn[:, <span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">2</span>])
</span></span><span style="display:flex;"><span><span style="color:#f92672">...</span>
</span></span></code></pre></div><p>In our large learning rate setting, the ablation 1 setting exhibits obvious training instability (much more loss and gradient spikes) and higher loss comparing to DIFF V2. The value should be shared in the two subtraction heads to construct differential operation, as discussed in DIFF V1 paper.</p>
</li>
<li>
<p>Subtracting two attention maps without $\lambda$ scaling factor, i.e., <code>attn1 - attn2</code> instead of <code>attn1 - lam_val * attn2</code>. This results in an excessively small context RMS at initialization.</p>
</li>
<li>
<p>Directly using projected $\lambda$ without applying <code>sigmoid</code> operation. The context RMS is unbounded from above.</p>
<p>Both ablation 2 and ablation 3 lead to higher language modeling loss than DIFF V2. Ablation 2 maintains training stability similar to DIFF V2, whereas ablation 3 is less stable (still more stable than ablation 1).</p>
</li>
<li>
<p>A Transformer with $1.5*h$ heads which aligns parameter with DIFF V2.</p>
<p>Ablation 4 also has higher training loss comparing to DIFF V2.</p>
</li>
</ol>
<h3 id="miscellaneous">Miscellaneous</h3>
<ul>
<li>In DIFF, the outliers in qk logits can be smaller than those in the baseline. This was already analyzed in DIFF V1: DIFF can achieve attention sparsity comparable to the baseline while using smaller qk logits. We further propose that DIFF&rsquo;s differential mechanism, which cancels out small attention values, <strong>may help mitigate the attention rounding error issue discussed in this <a href="https://spaces.ac.cn/archives/11371">blog</a> and <a href="https://arxiv.org/abs/2510.04212">paper</a></strong>.</li>
<li><strong>DIFF V2 is compatible with sparse attention</strong>. In many existing sparse attention frameworks, query heads within the same GQA group are required to attend to the same key-value blocks in order to maximize speedup. A common strategy is to select key-value blocks based on the average attention logits across heads.
For DIFF V2, the problem shifts to designing an effective block-selection strategy for a larger GQA group that contains pairs of differential heads. This may require handling the two types of differential heads separately during selection, or maybe a simple average of attention logits might already be sufficient in practice. Conceptually, this does not introduce any fundamental differences compared to block sparse attention of standard Transformers.</li>
</ul>
<h3 id="community">Community</h3>
<ul>
<li>Good to see a V2.
<ul>
<li>by dhruv3006 13 days ago
üöÄ (3)</li>
</ul>
</li>
<li>very cool!
<ul>
<li>by allendorf 13 days ago</li>
</ul>
</li>
<li>This is a fascinating and thorough update on the Differential Transformer architecture. The transition from DIFF V1 to V2 addresses some critical practical hurdles in a very elegant way.
The key design choice of doubling query heads within shared GQA groups is clever. It successfully decouples the innovative &ldquo;differential&rdquo; attention operation from the need for custom kernels, making it a much more viable drop-in replacement for standard attention. The analysis of how this design overcomes the softmax magnitude constraint and helps eliminate attention sinks is particularly convincing.
The reported early results‚Äîlower loss, reduced gradient spikes, and better control of activation outliers, especially at large learning rates‚Äîare highly promising. It suggests DIFF V2 isn&rsquo;t just a parameter-saving trick but may offer fundamental improvements in training dynamics and stability.
I have a couple of questions out of curiosity:
Long-Context Performance: You mention exploring &ldquo;context rot&rdquo; alleviation in later stages. Given the modified attention output dynamics, do you have any early hypotheses on whether DIFF V2 might inherently improve performance on very long sequences compared to a baseline Transformer with similar parameter budgets?
Broader Application: The principle seems powerful yet simple. Beyond the dense and MoE models tested here, do you see potential for applying this differential attention mechanism in other architectures, like state-space models or multimodal transformers?
<ul>
<li>by ProgramerSalar 9 days ago
üî• (2)</li>
</ul>
</li>
</ul>
<hr>
<h2 id="company">Company</h2>
<ul>
<li>TOS</li>
<li>Privacy</li>
<li>About</li>
<li>Careers</li>
</ul>
<h2 id="website">Website</h2>
<ul>
<li>Models</li>
<li>Datasets</li>
<li>Spaces</li>
<li>Pricing</li>
<li>Docs</li>
</ul>
<h3 id="link-b√†i-vi·∫øt-g·ªëc"><a href="https://huggingface.co/blog/microsoft/diff-attn-v2">Link b√†i vi·∫øt g·ªëc</a></h3>


    <ul id="taxonomy" class="not-prose flex items-center space-x-4 flex-nowrap whitespace-nowrap overflow-x-auto">

      <li class="font-semibold my-4">Tags:</li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/ai/">Ai</a></li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/20-january-2026/">20 January 2026</a></li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/huggingface.co/">Huggingface.co</a></li>
    </ul>
</div>

    <footer id="content-footer" class="not-prose">

      

      <div id="author-box" class="my-8 md:my-14 border p-8 bg-zinc-100 rounded-2xl">

        <img class="w-20 h-20 rounded-full" src="" alt="">

        <h2 class="text-xl font-bold my-4">Writter by : </h2>

        <p class="mb-4"></p>

        <ul class="flex flex-wrap space-x-4">
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://facebook.com/luat.skyai/" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                <path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951z"/>
              </svg>
              <span class="ml-2">Facebook</span>
            </a>
          </li>
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://facebook.com/luat.skyai/" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                  <path d="M12.6.75h2.454l-5.36 6.142L16 15.25h-4.937l-3.867-5.07-4.425 5.07H.316l5.733-6.57L0 .75h5.063l3.495 4.633L12.601.75Zm-.86 13.028h1.36L4.323 2.145H2.865l8.875 11.633Z"/>
              </svg>
              <span class="ml-2">Twitter</span>
            </a>
          </li>
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://luat.skyai.vn/" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                    <path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.917 3.917 0 0 0-1.417.923A3.927 3.927 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.916 3.916 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.926 3.926 0 0 0-.923-1.417A3.911 3.911 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0h.003zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599.28.28.453.546.598.92.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.47 2.47 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.478 2.478 0 0 1-.92-.598 2.48 2.48 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233 0-2.136.008-2.388.046-3.231.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92.28-.28.546-.453.92-.598.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045v.002zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92zm-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217zm0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334z"/>

              </svg>
              <span class="ml-2">Instagram</span>
            </a>
          </li>

        </ul>
        
      </div>

      <div id="related-post" class="">

        <h2 class="text-xl md:text-2xl font-bold mb-6 md:mb-8">Recommended for You</h2><div class="grid grid-cols-1 md:grid-cols-2 gap-6"><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl">
	
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2026-02-02/4e096a/"></a>

	<figure class="w-full aspect-video overflow-hidden rounded-3xl">

			<img class="object-cover group-hover:scale-105 transition duration-500 cursor-pointer" src="http://cdn-avatars.huggingface.co/v1/production/uploads/1665155571839-60d35a0307da9c17c7270909.jpeg" alt="Gi·ªõi thi·ªáu Waypoint-1- Video khu·∫øch t√°n t∆∞∆°ng t√°c th·ªùi gian th·ª±c t·ª´ Overworld" style="width: 100%; height: 100%; object-fit: cover;">

	  </figure>

	<div class="p-6">

		<time datetime="2026-02-02T00:00:00&#43;00:00">Feb 02, 2026</time>

		<h3 class="my-4 text-2xl font-bold">Gi·ªõi thi·ªáu Waypoint-1- Video khu·∫øch t√°n t∆∞∆°ng t√°c th·ªùi gian th·ª±c t·ª´ Overworld</h3>

		<p class="text-normal leading-normal text-zinc-500 line-clamp-2">Gi·ªõi thi·ªáu Waypoint-1- Video khu·∫øch t√°n t∆∞∆°ng t√°c th·ªùi gian th·ª±c t·ª´ Overworld</p>

	</div>
</article><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl">
	
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2026-02-02/e0b79b/"></a>

	<figure class="w-full aspect-video overflow-hidden rounded-3xl">

			<img class="object-cover group-hover:scale-105 transition duration-500 cursor-pointer" src="http://cdn-avatars.huggingface.co/v1/production/uploads/1583856921041-5dd96eb166059660ed1ee413.png" alt="M·ªôt nƒÉm k·ªÉ t·ª´ &#39;Kho·∫£nh kh·∫Øc DeepSeek&#39;" style="width: 100%; height: 100%; object-fit: cover;">

	  </figure>

	<div class="p-6">

		<time datetime="2026-02-02T00:00:00&#43;00:00">Feb 02, 2026</time>

		<h3 class="my-4 text-2xl font-bold">M·ªôt nƒÉm k·ªÉ t·ª´ &#39;Kho·∫£nh kh·∫Øc DeepSeek&#39;</h3>

		<p class="text-normal leading-normal text-zinc-500 line-clamp-2">M·ªôt nƒÉm k·ªÉ t·ª´ &#39;Kho·∫£nh kh·∫Øc DeepSeek&#39;</p>

	</div>
</article>
          
        </div>
      </div>

    </footer>

  </article>

  
  <aside class="md:col-span-1"><div class="lg:sticky lg:top-8"><div class="rounded-2xl p-4 bg-zinc-100 mb-10">

		<img class="aspect-video rounded" src="/images/email-3249062_1280_by-Muhammad-Ribkhan-Pixabay.jpg" alt="Banner Email Marketing">

		<p class="text-right text-xs mt-2 leading-none text-zinc-500">Adversitiment</p>

</div><div class="space-y-6">

		<h2 class="font-bold text-xl mb-8">Recent Post</h2>

		<article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2026-02-02/7ad673/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/d6058e0db8e477dc782dacae46e2ec6663d165d9-1000x1000.svg" alt="Anthropic h·ª£p t√°c v·ªõi Allen Institute v√† Howard Hughes Medical Institute ƒë·ªÉ ƒë·∫©y nhanh kh√°m ph√° khoa h·ªçc" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Anthropic h·ª£p t√°c v·ªõi Allen Institute v√† Howard Hughes Medical Institute ƒë·ªÉ ƒë·∫©y nhanh kh√°m ph√° khoa h·ªçc</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2026-02-02/90ee26/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="http://image-link" alt="Chuy·ªÉn ƒë·ªïi Ti√™n phong trong lƒ©nh v·ª±c b√°n l·∫ª- Robot AI t·ª± tr·ªã ƒëang ƒë·ªãnh nghƒ©a l·∫°i tr·∫£i nghi·ªám c·ª≠a h√†ng" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Chuy·ªÉn ƒë·ªïi Ti√™n phong trong lƒ©nh v·ª±c b√°n l·∫ª- Robot AI t·ª± tr·ªã ƒëang ƒë·ªãnh nghƒ©a l·∫°i tr·∫£i nghi·ªám c·ª≠a h√†ng</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2026-02-02/720a75/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="http://cdn-avatars.huggingface.co/v1/production/uploads/1583646260758-5e64858c87403103f9f1055d.png" alt="Differential Transformer V2" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Differential Transformer V2</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2026-02-02/293a1d/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="http://image-link" alt="Gi·ªõi thi·ªáu c√°c ƒë·ªïi m·ªõi v√† ch∆∞∆°ng tr√¨nh c·ªßa Microsoft ƒë·ªÉ h·ªó tr·ª£ gi·∫£ng d·∫°y v√† h·ªçc t·∫≠p b·∫±ng AI" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Gi·ªõi thi·ªáu c√°c ƒë·ªïi m·ªõi v√† ch∆∞∆°ng tr√¨nh c·ªßa Microsoft ƒë·ªÉ h·ªó tr·ª£ gi·∫£ng d·∫°y v√† h·ªçc t·∫≠p b·∫±ng AI</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2026-02-02/4e096a/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="http://cdn-avatars.huggingface.co/v1/production/uploads/1665155571839-60d35a0307da9c17c7270909.jpeg" alt="Gi·ªõi thi·ªáu Waypoint-1- Video khu·∫øch t√°n t∆∞∆°ng t√°c th·ªùi gian th·ª±c t·ª´ Overworld" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Gi·ªõi thi·ªáu Waypoint-1- Video khu·∫øch t√°n t∆∞∆°ng t√°c th·ªùi gian th·ª±c t·ª´ Overworld</h3>
	</div>
</article>
		
	</div>

</div>
</aside>

</div>
  
</main>
  <footer class="bg-zinc-100 py-10 md:py-14"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8">

  <div class="flex flex-wrap space-y-6 mb-4">

    <div class="w-full md:w-3/5 flex flex-col space-y-4 md:pr-8 lg:pr-10">

      <a class="flex items-center group" href="https://aitoday.skyai.vn/">
        <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" fill="currentColor" class="mr-2 group-hover:animate-spin" viewBox="0 0 16 16">
          <path d="M8 11a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/>
          <path d="M13.997 5.17a5 5 0 0 0-8.101-4.09A5 5 0 0 0 1.28 9.342a5 5 0 0 0 8.336 5.109 3.5 3.5 0 0 0 5.201-4.065 3.001 3.001 0 0 0-.822-5.216zm-1-.034a1 1 0 0 0 .668.977 2.001 2.001 0 0 1 .547 3.478 1 1 0 0 0-.341 1.113 2.5 2.5 0 0 1-3.715 2.905 1 1 0 0 0-1.262.152 4 4 0 0 1-6.67-4.087 1 1 0 0 0-.2-1 4 4 0 0 1 3.693-6.61 1 1 0 0 0 .8-.2 4 4 0 0 1 6.48 3.273z"/>
        </svg>
        
        <span class="text-2xl font-semibold uppercase">AI Today - SkyAI</span>
      </a>

      <p class="font-semibold">
        Theo d√µi c√°c ti·∫øn b·ªô m·ªõi nh·∫•t v·ªÅ Tr√≠ tu·ªá nh√¢n t·∫°o.<br>Tr·ª±c ti·∫øp t·ª´ c√°c nh√† ph√°t h√†nh AI tr√™n th·∫ø gi·ªõi.
      </p>

      <p>ƒêem tr√≠ tu·ªá nh√¢n t·∫°o ƒë·∫øn m·ªçi ng∆∞·ªùi d√¢n, doanh nghi·ªáp Vi·ªát, g√≥p ph·∫ßn gi√∫p Vi·ªát Nam ph√°t tri·ªÉn m·∫°nh m·∫Ω trong k·ª∑ nguy√™n s·ªë. N·ªôi dung ƒë∆∞·ª£c c·∫≠p nh·∫≠t t·ª± ƒë·ªông b·∫±ng m√°y.</p>

    </div>

    <div class="self-center flex flex-col w-full md:w-2/5">

<ul id="social-media" class="flex items-center space-x-4">
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.facebook.com/luat.skyai/" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 16 16">
				<path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.twitter.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M12.6.75h2.454l-5.36 6.142L16 15.25h-4.937l-3.867-5.07-4.425 5.07H.316l5.733-6.57L0 .75h5.063l3.495 4.633L12.601.75Zm-.86 13.028h1.36L4.323 2.145H2.865l8.875 11.633Z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.github.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.instagram.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.917 3.917 0 0 0-1.417.923A3.927 3.927 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.916 3.916 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.926 3.926 0 0 0-.923-1.417A3.911 3.911 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0h.003zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599.28.28.453.546.598.92.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.47 2.47 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.478 2.478 0 0 1-.92-.598 2.48 2.48 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233 0-2.136.008-2.388.046-3.231.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92.28-.28.546-.453.92-.598.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045v.002zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92zm-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217zm0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334z"></path>
			</svg>
		</a>
	</li>
</ul>

    </div>
    
  </div>

  <div class="my-8">
    <ul class="flex items-center space-x-4">
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/">Home</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="">About</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/tags/">Tags</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/tags/">Taxonomy</a></li>
      
    </ul>
  </div>

  <div class="border-t pt-4">

    <p class="text-sm">Copyright ¬© 2026. All rights reserved. <a href="https://home.skyai.vn/" target="_blank" rel="noopener">SkyAI.vn</a></p>

  </div>
  
</div>
</footer>
      <script defer src="/js/insertoggle.474f9b0e08021c6519cff4e46df14ccf148285b2d3a23d6321d6e10f25c291fb.js" integrity="sha256-R0&#43;bDggCHGUZz/TkbfFMzxSChbLToj1jIdbhDyXCkfs=" crossorigin="anonymous"></script>
</body>
</html>