<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Tiến bộ trong các hệ thống AI thông qua sự tiến bộ trong nhận thức, định vị và lý luận | AI Today - SkyAI</title>

<meta name="description" content="Meta FAIR đang phát hành một số hiện vật nghiên cứu mới giúp nâng cao sự hiểu biết của chúng tôi về nhận thức và hỗ trợ mục tiêu đạt được trí thông minh máy móc tiên tiến (AMI).">
    <link rel="stylesheet" href="/css/main.css">

<link rel="icon" type="image/svg+xml" href="http://localhost:1313/favicon.svg"> 
<link rel="icon" type="image/x-icon" href="http://localhost:1313/favicon.ico"> 
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon.png"> 
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32.png"> 
<link rel="icon" type="image/png" sizes="64x64" href="http://localhost:1313/favicon-64.png"> 

</head>
<body>
  <header class="py-6 border-b"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8 flex flex-col">
	<div class="flex items-center">
		<div class="flex items-center">
			<button class="flex items-center space-x-2 rounded-full border py-1 pr-[5px] pl-3 group bg-zinc-100 hover:bg-zinc-200 toggle-button" data-target="menu-bar">
				<svg width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
					<path fill-rule="evenodd" d="M2.5 12a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5zm0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5zm0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5z"></path>
				</svg>
				<span class="bg-blue-500 fill-white rounded-full p-1.5">
					<svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="group-hover:animate-spin bi bi-egg-fried fill-white" viewBox="0 0 16 16">
						<path d="M8 11a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"></path>
						<path d="M13.997 5.17a5 5 0 0 0-8.101-4.09A5 5 0 0 0 1.28 9.342a5 5 0 0 0 8.336 5.109 3.5 3.5 0 0 0 5.201-4.065 3.001 3.001 0 0 0-.822-5.216zm-1-.034a1 1 0 0 0 .668.977 2.001 2.001 0 0 1 .547 3.478 1 1 0 0 0-.341 1.113 2.5 2.5 0 0 1-3.715 2.905 1 1 0 0 0-1.262.152 4 4 0 0 1-6.67-4.087 1 1 0 0 0-.2-1 4 4 0 0 1 3.693-6.61 1 1 0 0 0 .8-.2 4 4 0 0 1 6.48 3.273z"></path>
					</svg>
				</span>
			</button>
			<div class="relative rounded-full py-1.5 px-6 bg-zinc-100 hover:bg-zinc-200 text-xl font-bold uppercase mx-2">
				<h2><a class="before:content-[''] before:z-10 before:top-0 before:right-0 before:left-0 before:bottom-0 before:absolute before:pointer-events-auto" href="http://localhost:1313/">AI Today - SkyAI</a></h2>
			</div>
		</div>
		<div class="flex items-center ml-auto">
			<button class="flex items-center rounded-full p-3 bg-zinc-100 hover:bg-zinc-200 toggle-button" data-target="search-bar">
				<svg width="18" height="18" fill="currentColor" viewBox="0 0 16 16">
					<path id="path1" class="transition-all ease-linear" d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path>
					<path id="path2" class="transition-all ease-linear hidden" d="M2.146 2.854a.5.5 0 1 1 .708-.708L8 7.293l5.146-5.147a.5.5 0 0 1 .708.708L8.707 8l5.147 5.146a.5.5 0 0 1-.708.708L8 8.707l-5.146 5.147a.5.5 0 0 1-.708-.708L7.293 8 2.146 2.854Z"></path>
				</svg>
			</button>
		</div>
	</div>
  <nav id="menu-bar" class="block mt-3 close">
    <ul class="flex items-center flex flex-nowrap whitespace-nowrap overflow-x-auto space-x-4">
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/">Home</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200">About</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/tags/">Tags</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200">Taxonomy</a>
    </li>
    </ul>
  </nav>
<div id="search-bar" class="block mt-3 close">
	<form id="search" class="flex items-stretch">
		<input class="w-full block px-6 py-2 rounded-l-full focus:outline-none border border-zinc-200" type="text" placeholder="Search...">
		<button class="flex items-center px-7 py-2.5 rounded-r-full border border-zinc-200 ">
			<svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="group-hover:animate-pulse" viewBox="0 0 16 16">
				<path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path>
			</svg>
		</button>
	</form>
</div>
</div></header>
  <main class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8"><div id="breadcrumb" class="max-w-7xl mx-auto py-8">
	<ul class="flex space-x-4 text-sm text-zinc-500">
		<li class="after:content-['❯'] after:ml-4 after:opacity-30 last:after:content-none uppercase">
			<a href="http://localhost:1313/">AI Today - SkyAI</a>
		</li>
		<li class="after:content-['❯'] after:ml-4 after:opacity-30 last:after:content-none uppercase">
			<a href="http://localhost:1313/posts/">Posts</a>
		</li>
	</ul>
</div><div class="grid grid-cols-1 md:grid-cols-3 gap-6 mb-14">

  <article class="md:col-span-2 prose lg:prose-lg">

    <header class="not-prose">
      
      <h1 id="title" class="text-4xl font-bold leading-normal">Tiến bộ trong các hệ thống AI thông qua sự tiến bộ trong nhận thức, định vị và lý luận</h1>

      <div id="lead" class="my-6">

        <p class="font-bold">Meta FAIR đang phát hành một số hiện vật nghiên cứu mới giúp nâng cao sự hiểu biết của chúng tôi về nhận thức và hỗ trợ mục tiêu đạt được trí thông minh máy móc tiên tiến (AMI). </p>

      </div>
      
      <div id="writer" class="flex items-center space-x-4"><ul class="flex items-center space-x-4 flex-nowrap whitespace-nowrap overflow-x-auto">
          
          <li class="font-semibold my-2"></li>
          
          <li class="before:content-['•'] before:mr-2 before:opacity-50 my-2"><time datetime="2025-04-19T00:00:00&#43;00:00">April 19, 2025</time>
          </li>

          <li class="before:content-['•'] before:mr-2 before:opacity-50 my-2">
            14 min read
          </li>

        </ul>

      </div>
      
    </header>

    <figure id="featureimage" class="rounded-xl aspect-video">

          <img class="rounded-lg" src="https://scontent.fhan15-2.fna.fbcdn.net/v/t39.2365-6/492216842_576463918172339_3408275426150658552_n.png?_nc_cat=107&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=_mkRDzBZ93cQ7kNvwFB3ECK&amp;_nc_oc=AdmW_dBLMWf4lNNbwDrQDlc39xLK-KZDzpEgwqlJPV9qeccSsBM-8-YRcxCaoY_9T1k2xufc7GkFzi8UxGwUqwol&amp;_nc_zt=14&amp;_nc_ht=scontent.fhan15-2.fna&amp;_nc_gid=jaU2opRS1eUCnDPExNecYQ&amp;oh=00_AfGKdOZBtyukJnhLl7woM1K9M7nc1cRf_9VCWk3_TIQFQg&amp;oe=681DD416" alt="Tiến bộ trong các hệ thống AI thông qua sự tiến bộ trong nhận thức, định vị và lý luận">
        <figcaption class="text-center italic text-xs">Meta FAIR đang phát hành một số hiện vật nghiên cứu mới giúp nâng cao sự hiểu biết của chúng tôi về nhận thức và hỗ trợ mục tiêu đạt được trí thông minh máy móc tiên tiến (AMI).</figcaption>

    </figure>

    <div id="content" class="mb-14">
      <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-markdown" data-lang="markdown"><span style="display:flex;"><span># Cải tiến hệ thống AI thông qua những tiến bộ trong nhận thức, định vị và lý luận
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Meta FAIR đang phát hành một số sản phẩm nghiên cứu mới nhằm nâng cao sự hiểu biết của chúng ta về nhận thức và hỗ trợ mục tiêu đạt được trí tuệ máy móc tiên tiến (AMI).
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Công việc chúng tôi chia sẻ bao gồm Meta Perception Encoder, nhằm xây dựng các hệ thống thị giác máy tính tiên tiến hơn có thể hỗ trợ mọi người trong các tác vụ hàng ngày, chẳng hạn như nhận dạng hình ảnh và phát hiện đối tượng. Chúng tôi cũng chia sẻ những tiến bộ trong việc hiểu cảnh 3D và bản địa hóa các đối tượng từ các truy vấn ngôn ngữ tự nhiên—tất cả đều là những phát triển quan trọng trên con đường đạt được các hệ thống AI phức tạp hơn.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Chúng tôi cũng giới thiệu Collaborative Reasoner, một khuôn khổ để đánh giá và cải thiện các kỹ năng lý luận hợp tác của các mô hình ngôn ngữ lớn, đây là một bước quan trọng hướng tới việc xây dựng các tác nhân xã hội hợp tác.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Bằng cách cung cấp rộng rãi nghiên cứu của mình, chúng tôi mong muốn cung cấp khả năng tiếp cận dễ dàng cho cộng đồng nghiên cứu và giúp thúc đẩy một hệ sinh thái mở cho AI nhằm đẩy nhanh tiến độ và khám phá.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## Những điểm chính
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">*</span>   Meta FAIR đang phát hành một số sản phẩm nghiên cứu mới nhằm nâng cao sự hiểu biết của chúng ta về nhận thức và hỗ trợ mục tiêu đạt được trí tuệ máy móc tiên tiến (AMI).
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">*</span>   Công việc chúng tôi chia sẻ bao gồm Meta Perception Encoder, nhằm xây dựng các hệ thống thị giác máy tính tiên tiến hơn có thể hỗ trợ mọi người trong các tác vụ hàng ngày, chẳng hạn như nhận dạng hình ảnh và phát hiện đối tượng. Chúng tôi cũng chia sẻ những tiến bộ trong việc hiểu cảnh 3D và bản địa hóa các đối tượng từ các truy vấn ngôn ngữ tự nhiên—tất cả đều là những phát triển quan trọng trên con đường đạt được các hệ thống AI phức tạp hơn.
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">*</span>   Chúng tôi cũng giới thiệu Collaborative Reasoner, một khuôn khổ để đánh giá và cải thiện các kỹ năng lý luận hợp tác của các mô hình ngôn ngữ lớn, đây là một bước quan trọng hướng tới việc xây dựng các tác nhân xã hội hợp tác.
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">*</span>   Bằng cách cung cấp rộng rãi nghiên cứu của mình, chúng tôi mong muốn cung cấp khả năng tiếp cận dễ dàng cho cộng đồng nghiên cứu và giúp thúc đẩy một hệ sinh thái mở cho AI nhằm đẩy nhanh tiến độ và khám phá.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Khi chúng tôi nỗ lực để đạt được mục tiêu trí tuệ máy móc tiên tiến (AMI), điều quan trọng là phải có các mô hình, điểm chuẩn và bộ dữ liệu tập trung vào nhận thức. Chúng ta cần những cỗ máy có khả năng thu thập, xử lý và diễn giải thông tin cảm giác về thế giới xung quanh và có thể sử dụng thông tin này để đưa ra quyết định với trí thông minh và tốc độ giống như con người. Hôm nay, chúng tôi rất vui mừng được công khai phát hành năm công trình mới từ nhóm Nghiên cứu AI Cơ bản Meta (FAIR) của chúng tôi, những công trình này đưa chúng ta đến gần hơn mục tiêu đó.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## Meta Perception Encoder: Thiết lập các tiêu chuẩn mới cho mô hình hóa tầm nhìn phù hợp với ngôn ngữ
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span>Chúng tôi rất vui mừng được giới thiệu Perception Encoder, một bộ mã hóa tầm nhìn quy mô lớn vượt trội trong một số tác vụ tầm nhìn cho hình ảnh và video. Bộ mã hóa tầm nhìn đóng vai trò là &#34;đôi mắt&#34; cho phép các hệ thống AI diễn giải thông tin trực quan và hiểu rõ hơn về thế giới. Khi các hệ thống AI trở nên tiên tiến hơn, việc xây dựng bộ mã hóa tầm nhìn đáp ứng mọi kỳ vọng về trí thông minh tiên tiến càng trở nên khó khăn hơn. Để đạt được điều này, bộ mã hóa tầm nhìn nên kết nối tầm nhìn và ngôn ngữ, hoạt động tốt trên hình ảnh và video, đồng thời mạnh mẽ trước các điều kiện đầy thách thức và có khả năng đối kháng khác nhau. Bộ mã hóa tầm nhìn cũng có thể nhận ra một loạt các khái niệm đồng thời đủ nhạy bén để phân biệt các khác biệt tinh tế, chẳng hạn như các loài động vật khác nhau.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Perception Encoder thể hiện hiệu suất vượt trội trong phân loại và truy xuất không ảnh chụp hình ảnh và video, vượt qua tất cả các mô hình độc quyền và mã nguồn mở hiện có cho các tác vụ như vậy. Nó cũng hoạt động đặc biệt tốt trên các tác vụ “khó”, chẳng hạn như nhận ra một con cá đuối gai độc ẩn mình dưới đáy biển, xác định một con chim sẻ vàng nhỏ bé ở hậu cảnh của một hình ảnh hoặc bắt một con chuột lang đang chạy trốn trên một máy ảnh động vật hoang dã nhìn đêm.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Những khả năng nhận thức mạnh mẽ này chuyển sang các nhiệm vụ ngôn ngữ hạ nguồn. Sau khi căn chỉnh với một mô hình ngôn ngữ lớn, Perception Encoder vượt qua tất cả các bộ mã hóa tầm nhìn khác cho câu hỏi và trả lời bằng hình ảnh và video, chú thích, hiểu tài liệu và tiếp đất. Perception Encoder cũng cho phép cải thiện đáng kể các nhiệm vụ khó khăn theo truyền thống đối với các mô hình ngôn ngữ, chẳng hạn như cho biết liệu một đối tượng có ở phía sau đối tượng khác hay không hoặc liệu máy ảnh có đang di chuyển theo chiều kim đồng hồ xung quanh một đối tượng hay không.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Khi Perception Encoder bắt đầu được tích hợp vào các ứng dụng mới, chúng tôi rất vui mừng được thấy các khả năng thị giác tiên tiến của nó sẽ cho phép các hệ thống AI có khả năng hơn nữa như thế nào.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Tải xuống mô hình</span>](<span style="color:#a6e22e">https://huggingface.co/collections/facebook/perception-encoder-67f977c9a65ca5895a7f6ba1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Tải xuống mã</span>](<span style="color:#a6e22e">https://github.com/facebookresearch/perception_models</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Tải xuống bộ dữ liệu</span>](<span style="color:#a6e22e">https://ai.meta.com/datasets/pe-video/</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Đọc bài báo</span>](<span style="color:#a6e22e">https://ai.meta.com/research/publications/perception-encoder-the-best-visual-embeddings-are-not-at-the-output-of-the-network/</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## Meta Perception Language Model: Nâng cao hiểu biết của chúng ta về các tác vụ nhận thức thị giác
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span>Tiếp tục công việc của chúng tôi về nhận thức, chúng tôi đang phát hành Perception Language Model (PLM), một mô hình ngôn ngữ tầm nhìn mở và có thể tái tạo để giải quyết các nhiệm vụ nhận dạng trực quan đầy thách thức.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Chúng tôi đã đào tạo PLM bằng cách sử dụng dữ liệu tổng hợp được tạo ở quy mô lớn và mở các bộ dữ liệu hiểu ngôn ngữ tầm nhìn, mà không cần chắt lọc từ các mô hình bên ngoài. Sau đó, chúng tôi đã xác định các lỗ hổng chính trong dữ liệu hiện có để hiểu video và thu thập 2,5 triệu mẫu video QA và chú thích không gian-thời gian mới, được gắn nhãn bởi con người để lấp đầy những lỗ hổng này, tạo thành bộ dữ liệu lớn nhất thuộc loại này cho đến nay.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>PLM được đào tạo trên bộ dữ liệu khổng lồ này, sử dụng kết hợp dữ liệu tổng hợp và được gắn nhãn bởi con người để tạo ra một mô hình mạnh mẽ, chính xác và có thể tái tạo hoàn toàn. PLM cung cấp các biến thể với 1, 3 và 8 tỷ tham số, khiến nó rất phù hợp cho nghiên cứu học thuật hoàn toàn minh bạch.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Chúng tôi cũng đang chia sẻ một điểm chuẩn mới, PLM-VideoBench, tập trung vào các tác vụ mà các điểm chuẩn hiện có bỏ lỡ: hiểu hoạt động chi tiết và lý luận dựa trên không gian-thời gian. Chúng tôi hy vọng rằng bộ dữ liệu quy mô lớn và mở, điểm chuẩn đầy thách thức và các mô hình mạnh mẽ của chúng tôi cùng nhau cho phép cộng đồng nguồn mở xây dựng các hệ thống thị giác máy tính có khả năng hơn.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Tải xuống mô hình</span>](<span style="color:#a6e22e">https://huggingface.co/collections/facebook/perception-lm-67f9783f171948c383ee7498</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Tải xuống mã</span>](<span style="color:#a6e22e">https://github.com/facebookresearch/perception_models</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Tải xuống bộ dữ liệu</span>](<span style="color:#a6e22e">https://ai.meta.com/datasets/plm-data/</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Đọc bài báo</span>](<span style="color:#a6e22e">https://ai.meta.com/research/publications/perceptionlm-open-access-data-and-models-for-detailed-visual-understanding/</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Meta Locate 3D: Một biên giới mới trong bản địa hóa đối tượng mở
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Hãy tưởng tượng bạn nói, “Này robot, mang cho tôi chiếc cốc màu đỏ trên bàn,” và có một con robot hoàn thành nhiệm vụ. Để các hệ thống AI hỗ trợ chúng ta hiệu quả trong thế giới vật chất, điều cần thiết là chúng phải có sự hiểu biết về thế giới 3D dựa trên ngôn ngữ tự nhiên. Để thực hiện các tác vụ như vậy, một con robot cần phải bản địa hóa đối tượng trong môi trường 3D, điều hướng đến nó và nhặt nó lên.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Để giải quyết vấn đề này, chúng tôi đã xây dựng Meta Locate 3D, một mô hình đầu cuối có thể bản địa hóa chính xác các đối tượng từ các truy vấn từ vựng mở. Meta Locate 3D hoạt động trực tiếp trên các đám mây điểm 3D từ các cảm biến RGB-D được nhận từ một con robot. Khi đưa ra một lời nhắc bằng văn bản, chẳng hạn như “bình hoa gần bảng điều khiển TV”, Meta Locate 3D sẽ tính đến các mối quan hệ không gian và ngữ cảnh để xác định phiên bản đối tượng cụ thể, chẳng hạn như “bình hoa gần TV”, không phải “bình hoa trên bàn”, và có thể xác định vị trí chính xác của vật phẩm.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Meta Locate 3D bao gồm ba thành phần chính:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">*</span>   Một bước tiền xử lý đầu tiên nâng các tính năng nền 2D lên các đám mây điểm có tính năng 3D.
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">*</span>   Bộ mã hóa 3D-JEPA, một bộ mã hóa được đào tạo trước nhận các đám mây điểm có tính năng làm đầu vào và dự đoán một biểu diễn được làm mịn, theo ngữ cảnh của thế giới 3D.
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">*</span>   Bộ giải mã Locate 3D, lấy biểu diễn 3D-JEPA và một truy vấn ngôn ngữ và tạo ra cả hộp giới hạn và mặt nạ cho các đối tượng được chỉ định.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Chúng tôi cũng đang phát hành một bộ dữ liệu mới để bản địa hóa các đối tượng dựa trên các biểu thức tham chiếu. Bộ dữ liệu này bao gồm 130.000 chú thích ngôn ngữ trên ba bộ dữ liệu được sử dụng rộng rãi—ARKitScenes, ScanNet và ScanNet++—và bao gồm 1.346 cảnh, tăng gấp đôi hiệu quả các chú thích dữ liệu hiện có.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Bằng cách cho phép robot hiểu chính xác môi trường xung quanh và đặt sự hiểu biết của chúng vào ngôn ngữ tự nhiên, Meta Locate 3D hỗ trợ sự phát triển của các hệ thống robot phức tạp và có khả năng hơn, bao gồm cả Meta PARTNR. Với Meta Locate 3D, con người có thể tương tác tự nhiên với robot để yêu cầu hoặc cộng tác trong các tác vụ, điều này đánh dấu một bước tiến thú vị trong việc theo đuổi các cỗ máy thông minh và tự trị hơn.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Tải xuống mô hình</span>](<span style="color:#a6e22e">https://github.com/facebookresearch/locate-3d</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Thử bản demo</span>](<span style="color:#a6e22e">https://locate3d.atmeta.com/</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Tải xuống bộ dữ liệu</span>](<span style="color:#a6e22e">https://github.com/facebookresearch/locate-3d/tree/main/locate3d_data</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Đọc bài báo</span>](<span style="color:#a6e22e">https://ai.meta.com/research/publications/locate-3d-real-world-object-localization-via-self-supervised-learning-in-3d/</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Dynamic Byte Latent Transformer: Xác định lại các tiêu chuẩn về hiệu quả và độ mạnh mẽ
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Sau khi công bố bài báo nghiên cứu của chúng tôi vào cuối năm 2024, theo yêu cầu phổ biến, chúng tôi đang phát hành trọng số mô hình cho Dynamic Byte Latent Transformer tham số 8B của mình. Nghiên cứu này đánh dấu một tiến bộ đáng kể trong kiến trúc mô hình ngôn ngữ cấp byte, lần đầu tiên đạt được hiệu suất ở quy mô phù hợp với các mô hình ngôn ngữ dựa trên mã thông báo truyền thống. Công nghệ này nâng cao hiệu quả suy luận và cải thiện đáng kể độ mạnh mẽ.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Kiến trúc Dynamic Byte Latent Transformer vượt trội hơn các mô hình dựa trên bộ mã hóa mã thông báo trong các tác vụ khác nhau, với lợi thế mạnh mẽ trung bình là +7 điểm (trên HellaSwag bị nhiễu) và đạt đến +55 điểm trong các tác vụ từ điểm chuẩn hiểu mã thông báo CUTE. Điều này làm nổi bật tiềm năng của Dynamic Byte Latent Transformer để xác định lại các tiêu chuẩn về hiệu quả và độ tin cậy của mô hình ngôn ngữ, cung cấp một sự thay thế hấp dẫn cho các phương pháp mã hóa mã thông báo truyền thống.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Với mô hình mới này và cơ sở mã đã phát hành trước đó của chúng tôi, chúng tôi khuyến khích cộng đồng khám phá những ý tưởng mới, hy vọng sẽ mở đường cho những phát triển đột phá hơn nữa trong lĩnh vực mô hình hóa ngôn ngữ.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Tải xuống mô hình</span>](<span style="color:#a6e22e">https://huggingface.co/facebook/blt</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Tải xuống mã</span>](<span style="color:#a6e22e">https://github.com/facebookresearch/blt</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Đọc bài báo</span>](<span style="color:#a6e22e">https://arxiv.org/abs/2412.09871</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Collaborative Reasoner: Các tác nhân xã hội tự cải thiện bằng các cuộc trò chuyện tổng hợp
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Khi con người hợp tác, chúng ta thường đạt được kết quả tốt hơn cùng nhau. Tương tự như việc hợp tác nhóm của con người, mục tiêu của chúng tôi là phát triển các tác nhân AI xã hội có thể hợp tác với con người hoặc các tác nhân AI khác để hoàn thành các nhiệm vụ tốt hơn một tác nhân hoặc con người duy nhất. Hãy tưởng tượng một tác nhân giúp bạn hiểu một bài tập về nhà khó hoặc giúp bạn chuẩn bị cho một cuộc phỏng vấn xin việc. Những sự hợp tác này đầy thách thức bởi vì, ngoài việc giải quyết vấn đề, chúng còn đòi hỏi các kỹ năng xã hội như giao tiếp hiệu quả, cung cấp phản hồi, có sự đồng cảm và lý thuyết về tâm trí. Hơn nữa, loại hợp tác này thường biểu hiện qua nhiều lượt trò chuyện tự nhiên qua lại. Các điểm chuẩn đánh giá LLM hiện tại và các quy trình đào tạo không xem xét các loại kỹ năng xã hội và hợp tác này. Dữ liệu trò chuyện qua lại hợp tác rất tốn kém để thu thập, dành riêng cho từng lĩnh vực và ít có khả năng kiểm soát, gây khó khăn cho cả việc đánh giá và đào tạo.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Để giải quyết những thách thức này, chúng tôi đã xây dựng Collaborative Reasoner, một khuôn khổ để đánh giá và cải thiện các kỹ năng lý luận hợp tác của các mô hình ngôn ngữ. Collaborative Reasoner bao gồm một bộ các nhiệm vụ hướng đến mục tiêu đòi hỏi lý luận nhiều bước cần được hai tác nhân hoàn thành một cách hợp tác thông qua một cuộc trò chuyện nhiều lượt. Các nhiệm vụ và số liệu trong Collaborative Reasoner yêu cầu các tác nhân không đồng ý về các giải pháp, thuyết phục đối tác của họ về một giải pháp đúng và cuối cùng đồng ý về giải pháp tốt nhất với tư cách là một nhóm.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Đánh giá của chúng tôi cho thấy rằng các mô hình hiện tại không thể sử dụng nhất quán sự hợp tác để đạt được hiệu suất nhiệm vụ tốt hơn. Để cải thiện khả năng lý luận hợp tác của LLM, chúng tôi đề xuất một phương pháp tự cải thiện bằng cách sử dụng dữ liệu tương tác tổng hợp được lấy mẫu với sự tự hợp tác—nói cách khác, một tác nhân LLM hợp tác với chính nó. Để cho phép tạo ra dữ liệu đó ở quy mô lớn, chúng tôi cũng phát triển một công cụ phục vụ mô hình hiệu suất cao, linh hoạt cho suy luận quy mô lớn, được gọi là Matrix: Hạ tầng và thử nghiệm tạo dữ liệu nhiều tác nhân. Trong các tác vụ toán học (MATH), khoa học (MMLU-Pro, GPQA) và lý luận xã hội (ExploreToM, HiToM), phương pháp của chúng tôi mang lại những cải tiến lên đến 29,4% so với hiệu suất chuỗi suy nghĩ của một LLM tác nhân đơn tương đương.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Collaborative Reasoner mở đường cho việc phát triển các tác nhân xã hội có thể hợp tác với con người và các tác nhân khác. Chúng tôi đang mở nguồn dữ liệu tạo và mô hình hóa quy trình của mình để hỗ trợ nghiên cứu sâu hơn trong lĩnh vực này.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Tải xuống mã Collaborative Reasoner</span>](<span style="color:#a6e22e">https://github.com/facebookresearch/collaborative-reasoner</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Tải xuống mã MATRIX</span>](<span style="color:#a6e22e">https://github.com/facebookresearch/matrix</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Đọc bài báo</span>](<span style="color:#a6e22e">https://ai.meta.com/research/publications/collaborative-reasoner-self-improving-social-agents-with-synthetic-conversations/</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## Tham gia cùng chúng tôi trong việc theo đuổi những gì có thể với AI.
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span>[<span style="color:#f92672">Xem tất cả các vị trí đang mở</span>](<span style="color:#a6e22e">https://www.metacareers.com/jobs/?is_leadership=0&amp;sub_teams%5B0%5D=Artificial+Intelligence&amp;is_in_page=0&amp;fbclid=IwAR0O8BF7opOj5gASJmwYVGalPPXTLu-6xrl9w00eC7Rarp2HQ9uEH8tERFw</span>)
</span></span></code></pre></div><h3 id="link-bài-báo-gốc"><a href="https://ai.meta.com/blog/meta-fair-updates-perception-localization-reasoning/">Link bài báo gốc</a></h3>


    <ul id="taxonomy" class="not-prose flex items-center space-x-4 flex-nowrap whitespace-nowrap overflow-x-auto">

      <li class="font-semibold my-4">Tags:</li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/ai/">Ai</a></li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/april-17-2025/">April 17, 2025</a></li>
    </ul>
</div>

    <footer id="content-footer" class="not-prose">

      <div id="comment" class="border p-8 bg-zinc-100 rounded-2xl">

        <h2 class="text-xl font-semibold">Comment</h2>

        <p>Disqus comment here</p></div>

      <div id="author-box" class="my-8 md:my-14 border p-8 bg-zinc-100 rounded-2xl">

        <img class="w-20 h-20 rounded-full" src="" alt="">

        <h2 class="text-xl font-bold my-4">Writter by : </h2>

        <p class="mb-4"></p>

        <ul class="flex flex-wrap space-x-4">
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://facebook.com/#" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                <path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951z"/>
              </svg>
              <span class="ml-2">Facebook</span>
            </a>
          </li>
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://facebook.com/#" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                  <path d="M12.6.75h2.454l-5.36 6.142L16 15.25h-4.937l-3.867-5.07-4.425 5.07H.316l5.733-6.57L0 .75h5.063l3.495 4.633L12.601.75Zm-.86 13.028h1.36L4.323 2.145H2.865l8.875 11.633Z"/>
              </svg>
              <span class="ml-2">Twitter</span>
            </a>
          </li>
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://facebook.com/#" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                    <path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.917 3.917 0 0 0-1.417.923A3.927 3.927 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.916 3.916 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.926 3.926 0 0 0-.923-1.417A3.911 3.911 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0h.003zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599.28.28.453.546.598.92.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.47 2.47 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.478 2.478 0 0 1-.92-.598 2.48 2.48 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233 0-2.136.008-2.388.046-3.231.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92.28-.28.546-.453.92-.598.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045v.002zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92zm-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217zm0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334z"/>

              </svg>
              <span class="ml-2">Instagram</span>
            </a>
          </li>

        </ul>
        
      </div>

      <div id="related-post" class="">

        <h2 class="text-xl md:text-2xl font-bold mb-6 md:mb-8">Recommended for You</h2><div class="grid grid-cols-1 md:grid-cols-2 gap-6"><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl">
	
	<a class="insert-link" href="http://localhost:1313/posts/b1fd3f/"></a>

	<figure class="w-full aspect-video overflow-hidden rounded-3xl">

			<img class="object-cover group-hover:scale-105 transition duration-500 cursor-pointer" src="https://venturebeat.com/wp-content/uploads/2025/04/nuneybits_Vector_art_of_a_retro_computer_on_the_screen_is_a_lig_b7962004-900f-4fdd-8b99-620ed4be1597.webp?w=350&amp;h=175&amp;crop=1" alt="Gemini 2.5 Flash của Google giới thiệu &#39;ngân sách tư duy&#39; giúp cắt giảm chi phí AI tới 600% khi tắt" style="width: 100%; height: 100%; object-fit: cover;">

	  </figure>

	<div class="p-6">

		<time datetime="2025-04-19T00:00:00&#43;00:00">Apr 19, 2025</time>

		<h3 class="my-4 text-2xl font-bold">Gemini 2.5 Flash của Google giới thiệu &#39;ngân sách tư duy&#39; giúp cắt giảm chi phí AI tới 600% khi tắt</h3>

		<p class="text-normal leading-normal text-zinc-500 line-clamp-2">Bài viết mô tả cách Gemini 2.5 Flash của Google có thể giảm chi phí AI.</p>

	</div>
</article><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl">
	
	<a class="insert-link" href="http://localhost:1313/posts/9d39c0/"></a>

	<figure class="w-full aspect-video overflow-hidden rounded-3xl">

			<img class="object-cover group-hover:scale-105 transition duration-500 cursor-pointer" src="https://venturebeat.com/wp-content/uploads/2025/04/Capital-One-hero.png?w=350&amp;h=175&amp;crop=1" alt="Mã hóa mã thông báo đang tái tạo bảo mật dữ liệu như thế nào trong kỷ nguyên AI" style="width: 100%; height: 100%; object-fit: cover;">

	  </figure>

	<div class="p-6">

		<time datetime="2025-04-19T00:00:00&#43;00:00">Apr 19, 2025</time>

		<h3 class="my-4 text-2xl font-bold">Mã hóa mã thông báo đang tái tạo bảo mật dữ liệu như thế nào trong kỷ nguyên AI</h3>

		<p class="text-normal leading-normal text-zinc-500 line-clamp-2">Mã hóa mã thông báo đang tái tạo bảo mật dữ liệu như thế nào trong kỷ nguyên AI</p>

	</div>
</article>
          
        </div>
      </div>

    </footer>

  </article>

  
  <aside class="md:col-span-1"><div class="lg:sticky lg:top-8"><div class="rounded-2xl p-4 bg-zinc-100 mb-10">

		<img class="aspect-video rounded" src="/images/email-3249062_1280_by-Muhammad-Ribkhan-Pixabay.jpg" alt="Banner Email Marketing">

		<p class="text-right text-xs mt-2 leading-none text-zinc-500">Adversitiment</p>

</div><div class="space-y-6">

		<h2 class="font-bold text-xl mb-8">Recent Post</h2>

		<article class="relative group flex flex-row">
	<a class="insert-link" href="http://localhost:1313/posts/b1fd3f/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://venturebeat.com/wp-content/uploads/2025/04/nuneybits_Vector_art_of_a_retro_computer_on_the_screen_is_a_lig_b7962004-900f-4fdd-8b99-620ed4be1597.webp?w=350&amp;h=175&amp;crop=1" alt="Gemini 2.5 Flash của Google giới thiệu &#39;ngân sách tư duy&#39; giúp cắt giảm chi phí AI tới 600% khi tắt" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Gemini 2.5 Flash của Google giới thiệu &#39;ngân sách tư duy&#39; giúp cắt giảm chi phí AI tới 600% khi tắt</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="http://localhost:1313/posts/e7b9a4/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://lh3.googleusercontent.com/oLv3Jl8j5l64yXU6YCE-238fqaeNuq9DOmOhksHZwUjJNDlB18xVehDswPohGRaN1cdArPSFA5cdZsXSqhFU-Jy5F6THyhGfo9L_QkZfNQDeKzxP2S8=w400-h225-n-nu" alt="Giới thiệu Gemini 2.5 Flash" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Giới thiệu Gemini 2.5 Flash</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="http://localhost:1313/posts/9d39c0/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://venturebeat.com/wp-content/uploads/2025/04/Capital-One-hero.png?w=350&amp;h=175&amp;crop=1" alt="Mã hóa mã thông báo đang tái tạo bảo mật dữ liệu như thế nào trong kỷ nguyên AI" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Mã hóa mã thông báo đang tái tạo bảo mật dữ liệu như thế nào trong kỷ nguyên AI</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="http://localhost:1313/posts/b0f7f6/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://venturebeat.com/wp-content/uploads/2025/04/nuneybits_Vector_art_of_a_scientist_obersving_a_whale_in_an_obe_7fe399e3-ecf1-4568-9c64-35a0949a040b.webp?w=350&amp;h=175&amp;crop=1" alt="Phương pháp mới cho phép DeepSeek và các mô hình khác trả lời các câu hỏi ‘nhạy cảm’" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Phương pháp mới cho phép DeepSeek và các mô hình khác trả lời các câu hỏi ‘nhạy cảm’</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="http://localhost:1313/posts/376aef/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://venturebeat.com/wp-content/uploads/2025/04/spexi.jpg?w=350&amp;h=175&amp;crop=1" alt="Spexi ra mắt mạng lưới phi tập trung LayerDrone để cung cấp hình ảnh máy bay không người lái có độ phân giải cao về Trái đất" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Spexi ra mắt mạng lưới phi tập trung LayerDrone để cung cấp hình ảnh máy bay không người lái có độ phân giải cao về Trái đất</h3>
	</div>
</article>
		
	</div>

</div>
</aside>

</div>
  
</main>
  <footer class="bg-zinc-100 py-10 md:py-14"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8">

  <div class="flex flex-wrap space-y-6 mb-4">

    <div class="w-full md:w-3/5 flex flex-col space-y-4 md:pr-8 lg:pr-10">

      <a class="flex items-center group" href="http://localhost:1313/">
        <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" fill="currentColor" class="mr-2 group-hover:animate-spin" viewBox="0 0 16 16">
          <path d="M8 11a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/>
          <path d="M13.997 5.17a5 5 0 0 0-8.101-4.09A5 5 0 0 0 1.28 9.342a5 5 0 0 0 8.336 5.109 3.5 3.5 0 0 0 5.201-4.065 3.001 3.001 0 0 0-.822-5.216zm-1-.034a1 1 0 0 0 .668.977 2.001 2.001 0 0 1 .547 3.478 1 1 0 0 0-.341 1.113 2.5 2.5 0 0 1-3.715 2.905 1 1 0 0 0-1.262.152 4 4 0 0 1-6.67-4.087 1 1 0 0 0-.2-1 4 4 0 0 1 3.693-6.61 1 1 0 0 0 .8-.2 4 4 0 0 1 6.48 3.273z"/>
        </svg>
        
        <span class="text-2xl font-semibold uppercase">AI Today - SkyAI</span>
      </a>

      <p class="font-semibold">
        Theo dõi các tiến bộ mới nhất về Trí tuệ nhân tạo.<br>Trực tiếp từ các nhà phát hành AI trên thế giới.
      </p>

      <p>Đem trí tuệ nhân tạo đến mọi người dân, doanh nghiệp Việt, góp phần giúp Việt Nam phát triển mạnh mẽ trong kỷ nguyên số. Nội dung được cập nhật tự động bằng máy.</p>

    </div>

    <div class="self-center flex flex-col w-full md:w-2/5">

<ul id="social-media" class="flex items-center space-x-4">
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.facebook.com/luat.skyai/" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 16 16">
				<path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.twitter.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M12.6.75h2.454l-5.36 6.142L16 15.25h-4.937l-3.867-5.07-4.425 5.07H.316l5.733-6.57L0 .75h5.063l3.495 4.633L12.601.75Zm-.86 13.028h1.36L4.323 2.145H2.865l8.875 11.633Z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.github.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.instagram.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.917 3.917 0 0 0-1.417.923A3.927 3.927 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.916 3.916 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.926 3.926 0 0 0-.923-1.417A3.911 3.911 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0h.003zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599.28.28.453.546.598.92.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.47 2.47 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.478 2.478 0 0 1-.92-.598 2.48 2.48 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233 0-2.136.008-2.388.046-3.231.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92.28-.28.546-.453.92-.598.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045v.002zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92zm-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217zm0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334z"></path>
			</svg>
		</a>
	</li>
</ul>

    </div>
    
  </div>

  <div class="my-8">
    <ul class="flex items-center space-x-4">
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/">Home</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="">About</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/tags/">Tags</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="">Taxonomy</a></li>
      
    </ul>
  </div>

  <div class="border-t pt-4">

    <p class="text-sm">Copyright © 2025. All rights reserved. <a href="https://skyai.vn/" target="_blank" rel="noopener">SkyAI.vn</a></p>

  </div>
  
</div>
</footer>
      <script src="/js/insertoggle.js"></script>
</body>
</html>