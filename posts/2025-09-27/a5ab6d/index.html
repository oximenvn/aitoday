<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>CÃ¡c thá»§ thuáº­t tá»« OpenAI gpt-oss mÃ  Báº N ğŸ«µ cÃ³ thá»ƒ sá»­ dá»¥ng vá»›i transformers | AI Today - SkyAI</title>

<meta name="description" content="">
      <link rel="stylesheet" href="/css/main.min.584aed796ba9d245f84009d57da9662e9c02059611972780263728b442fc25ff.css" integrity="sha256-WErteWup0kX4QAnVfalmLpwCBZYRlyeAJjcotEL8Jf8=" crossorigin="anonymous">

<link rel="icon" type="image/svg+xml" href="https://aitoday.skyai.vn/favicon.svg"> 
<link rel="icon" type="image/x-icon" href="https://aitoday.skyai.vn/favicon.ico"> 
<link rel="icon" type="image/png" sizes="16x16" href="https://aitoday.skyai.vn/favicon.png"> 
<link rel="icon" type="image/png" sizes="32x32" href="https://aitoday.skyai.vn/favicon-32.png"> 
<link rel="icon" type="image/png" sizes="64x64" href="https://aitoday.skyai.vn/favicon-64.png"> 

</head>
<body>
  <header class="py-6 border-b"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8 flex flex-col">
	<div class="flex items-center">
		<div class="flex items-center">
			<button class="flex items-center space-x-2 rounded-full border py-1 pr-[5px] pl-3 group bg-zinc-100 hover:bg-zinc-200 toggle-button" data-target="menu-bar">
				<svg width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
					<path fill-rule="evenodd" d="M2.5 12a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5zm0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5zm0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5z"></path>
				</svg>
				<span class="bg-blue-500 fill-white rounded-full p-1.5">
					<svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="group-hover:animate-spin bi bi-egg-fried fill-white" viewBox="0 0 16 16">
						<path d="M8 11a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"></path>
						<path d="M13.997 5.17a5 5 0 0 0-8.101-4.09A5 5 0 0 0 1.28 9.342a5 5 0 0 0 8.336 5.109 3.5 3.5 0 0 0 5.201-4.065 3.001 3.001 0 0 0-.822-5.216zm-1-.034a1 1 0 0 0 .668.977 2.001 2.001 0 0 1 .547 3.478 1 1 0 0 0-.341 1.113 2.5 2.5 0 0 1-3.715 2.905 1 1 0 0 0-1.262.152 4 4 0 0 1-6.67-4.087 1 1 0 0 0-.2-1 4 4 0 0 1 3.693-6.61 1 1 0 0 0 .8-.2 4 4 0 0 1 6.48 3.273z"></path>
					</svg>
				</span>
			</button>
			<div class="relative rounded-full py-1.5 px-6 bg-zinc-100 hover:bg-zinc-200 text-xl font-bold uppercase mx-2">
				<h2><a class="before:content-[''] before:z-10 before:top-0 before:right-0 before:left-0 before:bottom-0 before:absolute before:pointer-events-auto" href="https://aitoday.skyai.vn/">AI Today - SkyAI</a></h2>
			</div>
		</div>
		<div class="flex items-center ml-auto">
			<button class="flex items-center rounded-full p-3 bg-zinc-100 hover:bg-zinc-200 toggle-button" data-target="search-bar">
				<svg width="18" height="18" fill="currentColor" viewBox="0 0 16 16">
					<path id="path1" class="transition-all ease-linear" d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path>
					<path id="path2" class="transition-all ease-linear hidden" d="M2.146 2.854a.5.5 0 1 1 .708-.708L8 7.293l5.146-5.147a.5.5 0 0 1 .708.708L8.707 8l5.147 5.146a.5.5 0 0 1-.708.708L8 8.707l-5.146 5.147a.5.5 0 0 1-.708-.708L7.293 8 2.146 2.854Z"></path>
				</svg>
			</button>
		</div>
	</div>
  <nav id="menu-bar" class="block mt-3 close">
    <ul class="flex items-center flex flex-nowrap whitespace-nowrap overflow-x-auto space-x-4">
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/">Home</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200">About</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/tags/">Tags</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/tags/">Taxonomy</a>
    </li>
    </ul>
  </nav>
<div id="search-bar" class="block mt-3 close">
	<form id="search" class="flex items-stretch">
		<input class="w-full block px-6 py-2 rounded-l-full focus:outline-none border border-zinc-200" type="text" placeholder="Search...">
		<button class="flex items-center px-7 py-2.5 rounded-r-full border border-zinc-200 ">
			<svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="group-hover:animate-pulse" viewBox="0 0 16 16">
				<path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path>
			</svg>
		</button>
	</form>
</div>
</div></header>
  <main class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8"><div id="breadcrumb" class="max-w-7xl mx-auto py-8">
	<ul class="flex space-x-4 text-sm text-zinc-500">
		<li class="after:content-['â¯'] after:ml-4 after:opacity-30 last:after:content-none uppercase">
			<a href="https://aitoday.skyai.vn/">AI Today - SkyAI</a>
		</li>
		<li class="after:content-['â¯'] after:ml-4 after:opacity-30 last:after:content-none uppercase">
			<a href="https://aitoday.skyai.vn/posts/">Posts</a>
		</li>
	</ul>
</div><div class="grid grid-cols-1 md:grid-cols-3 gap-6 mb-14">

  <article class="md:col-span-2 prose lg:prose-lg">

    <header class="not-prose">
      
      <h1 id="title" class="text-4xl font-bold leading-normal">CÃ¡c thá»§ thuáº­t tá»« OpenAI gpt-oss mÃ  Báº N ğŸ«µ cÃ³ thá»ƒ sá»­ dá»¥ng vá»›i transformers</h1>

      <div id="lead" class="my-6">

        <p class="font-bold"> </p>

      </div>
      
      <div id="writer" class="flex items-center space-x-4"><ul class="flex items-center space-x-4 flex-nowrap whitespace-nowrap overflow-x-auto">
          
          <li class="font-semibold my-2"></li>
          
          <li class="before:content-['â€¢'] before:mr-2 before:opacity-50 my-2"><time datetime="2025-09-27T00:00:00&#43;00:00">September 27, 2025</time>
          </li>

          <li class="before:content-['â€¢'] before:mr-2 before:opacity-50 my-2">
            17 min read
          </li>

        </ul>

      </div>
      
    </header>

    <figure id="featureimage" class="rounded-xl aspect-video">

          <img class="rounded-lg" src="https://huggingface.co/blog/assets/faster-transformers/thumbnail.png" alt="CÃ¡c thá»§ thuáº­t tá»« OpenAI gpt-oss mÃ  Báº N ğŸ«µ cÃ³ thá»ƒ sá»­ dá»¥ng vá»›i transformers">
        <figcaption class="text-center italic text-xs"></figcaption>

    </figure>

    <div id="content" class="mb-14">
      <h1 id="thá»§-thuáº­t-tá»«-openai-gpt-oss-má»i-ngÆ°á»i--cÃ³-thá»ƒ-sá»­-dá»¥ng-vá»›i-transformers">Thá»§ thuáº­t tá»« OpenAI gpt-oss Má»ŒI NGÆ¯á»œI ğŸ«µ cÃ³ thá»ƒ sá»­ dá»¥ng vá»›i transformers</h1>
<p>OpenAI gáº§n Ä‘Ã¢y Ä‘Ã£ phÃ¡t hÃ nh <a href="https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4">bá»™ mÃ´ hÃ¬nh GPT-OSS</a>. CÃ¡c mÃ´ hÃ¬nh nÃ y cÃ³ má»™t sá»‘ ká»¹ thuáº­t má»›i nhÆ° lÆ°á»£ng tá»­ hÃ³a MXFP4, kernel hiá»‡u quáº£, Ä‘á»‹nh dáº¡ng chat hoÃ n toÃ n má»›i vÃ  hÆ¡n tháº¿ ná»¯a. Äá»ƒ cho phÃ©p phÃ¡t hÃ nh gpt-oss thÃ´ng qua <code>transformers</code>, chÃºng tÃ´i Ä‘Ã£ nÃ¢ng cáº¥p Ä‘Ã¡ng ká»ƒ <a href="https://github.com/huggingface/transformers/">thÆ° viá»‡n</a>. CÃ¡c báº£n cáº­p nháº­t giÃºp viá»‡c <strong>táº£i</strong>, <strong>cháº¡y</strong> vÃ  <strong>tinh chá»‰nh</strong> cÃ¡c mÃ´ hÃ¬nh trá»Ÿ nÃªn ráº¥t hiá»‡u quáº£.</p>
<p>Trong bÃ i Ä‘Äƒng trÃªn blog nÃ y, chÃºng ta sáº½ nÃ³i vá» táº¥t cáº£ cÃ¡c nÃ¢ng cáº¥p má»™t cÃ¡ch chuyÃªn sÃ¢u vÃ  cÃ¡ch chÃºng trá»Ÿ thÃ nh má»™t pháº§n cá»§a bá»™ cÃ´ng cá»¥ transformers Ä‘á»ƒ cÃ¡c mÃ´ hÃ¬nh khÃ¡c (hiá»‡n táº¡i vÃ  tÆ°Æ¡ng lai) cÃ³ thá»ƒ hÆ°á»Ÿng lá»£i tá»« chÃºng. Cung cáº¥p cÃ¡c triá»ƒn khai rÃµ rÃ ng vá» cÃ¡c phÆ°Æ¡ng phÃ¡p má»›i trong transformers cÅ©ng cho phÃ©p cá»™ng Ä‘á»“ng nhanh chÃ³ng hiá»ƒu vÃ  Ã¡p dá»¥ng chÃºng. CÃ¡c framework nhÆ° <a href="https://github.com/ml-explore/mlx-lm/pull/354"><code>MLX</code></a>, <a href="https://github.com/ggml-org/llama.cpp/discussions/15396"><code>llama.cpp</code></a> hoáº·c <a href="https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html"><code>vLLM</code></a> cÃ³ thá»ƒ sá»­ dá»¥ng mÃ£ transformers lÃ m tÃ i liá»‡u tham kháº£o Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c triá»ƒn khai cá»§a riÃªng há».</p>
<p>Äá»‘i vá»›i báº£n phÃ¡t hÃ nh nÃ y, chÃºng tÃ´i Ä‘Ã£ lÃ m viá»‡c trÃªn:</p>
<ul>
<li><a href="#zero-build-kernels-downloadable-from-the-hub">Kernel khÃ´ng cáº§n build, cÃ³ thá»ƒ táº£i xuá»‘ng tá»« Hub</a></li>
<li><a href="#mxfp4-quantization">LÆ°á»£ng tá»­ hÃ³a MXFP4</a></li>
<li><a href="#tensor-parallelism">Song song hÃ³a Tensor</a></li>
<li><a href="#expert-parallelism">Song song hÃ³a Expert</a></li>
<li><a href="#dynamic-sliding-window-layer--cache">Lá»›p Cá»­a sá»• TrÆ°á»£t Äá»™ng &amp; Bá»™ nhá»› Cache</a></li>
<li><a href="#continuous-batching--paged-attention">Batching liÃªn tá»¥c &amp; ChÃº Ã½ theo Trang</a></li>
<li><a href="#load-larger-models-faster">Táº£i cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n nhanh hÆ¡n</a></li>
</ul>
<blockquote>
<p>Pháº§n hay nháº¥t: Háº§u háº¿t cÃ¡c tÃ­nh nÄƒng nÃ y sáº½ hoáº¡t Ä‘á»™ng trÃªn táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh chÃ­nh trong <code>transformers</code>!</p></blockquote>
<h2 id="kernel-khÃ´ng-cáº§n-build-cÃ³-thá»ƒ-táº£i-xuá»‘ng-tá»«-hub">Kernel khÃ´ng cáº§n build, cÃ³ thá»ƒ táº£i xuá»‘ng tá»« Hub</h2>
<p>Kernel lÃ  má»™t chÆ°Æ¡ng trÃ¬nh <em>chuyÃªn dá»¥ng</em>, nhá» gá»n cháº¡y trÃªn cÃ¡c bá»™ tÄƒng tá»‘c Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c tÃ¡c vá»¥ nhÆ° nhÃ¢n ma tráº­n, kÃ­ch hoáº¡t hoáº·c chuáº©n hÃ³a. Trong PyTorch eager, cÃ¡c hoáº¡t Ä‘á»™ng kÃ­ch hoáº¡t cÃ¡c kernel riÃªng láº» má»™t cÃ¡ch tuáº§n tá»±, Ä‘iá»u nÃ y ráº¥t Ä‘Æ¡n giáº£n nhÆ°ng cÃ³ thá»ƒ phÃ¡t sinh thÃªm cÃ¡c chuyá»ƒn bá»™ nhá»› vÃ  chi phÃ­ khá»Ÿi cháº¡y. <code>torch.compile</code> cá»§a PyTorch 2.0 vá»›i cÃ¡c backend nhÆ° <code>TorchInductor</code> giáº£i quyáº¿t váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch tá»± Ä‘á»™ng há»£p nháº¥t vÃ  tá»‘i Æ°u hÃ³a cÃ¡c kernel, mang láº¡i hiá»‡u suáº¥t tÄƒng tá»« <code>2â€“10Ã—</code>.</p>
<p>NgoÃ i ra, cá»™ng Ä‘á»“ng Ä‘Ã£ táº¡o ra cÃ¡c kernel tÃ¹y chá»‰nh cho cÃ¡c káº¿t há»£p hoáº¡t Ä‘á»™ng thÆ°á»ng xuyÃªn, <em>khÃ´ng chá»‰ cÃ¡c op PyTorch riÃªng láº» nhÆ° matmul</em>. VÃ­ dá»¥: Flash Attention Ä‘Æ°á»£c táº¡o ra Ä‘á»ƒ tá»‘i Æ°u hÃ³a khá»‘i attention quan trá»ng xÃ¡c Ä‘á»‹nh kiáº¿n trÃºc transformers vÃ  cÃ³ máº·t trong nhiá»u mÃ´ hÃ¬nh, bao gá»“m háº§u háº¿t LLM. Báº±ng cÃ¡ch káº¿t há»£p cáº©n tháº­n táº¥t cáº£ cÃ¡c hoáº¡t Ä‘á»™ng attention bÃªn trong má»™t kernel duy nháº¥t, cÃ¡c chuyá»ƒn bá»™ nhá»› Ä‘Æ°á»£c giáº£m thiá»ƒu, viá»‡c sá»­ dá»¥ng bá»™ nhá»› Ä‘Æ°á»£c giáº£m vÃ  cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c.</p>
<p>Váº¥n Ä‘á» lÃ  táº¥t cáº£ cÃ¡c kernel khÃ¡c nhau nÃ y Ä‘á»u cÃ³ sáºµn trong cÃ¡c thÆ° viá»‡n riÃªng biá»‡t, Ä‘iá»u nÃ y táº¡o ra sá»± phÃ¬nh to vá» dependency náº¿u chÃºng Ä‘Æ°á»£c thÃªm vÃ o thÆ° viá»‡n transformers. HÆ¡n ná»¯a, cÃ¡c kernel nÃ y khÃ´ng chá»‰ lÃ  mÃ£ Python, chÃºng bao gá»“m mÃ£ cuda cáº¥p tháº¥p, Ä‘Æ°á»£c gáº¯n láº¡i vá»›i C++ vÃ  Ä‘Æ°á»£c hiá»ƒn thá»‹ thÃ´ng qua má»™t lá»›p Python. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  chÃºng pháº£i Ä‘Æ°á»£c biÃªn dá»‹ch trong há»‡ thá»‘ng Ä‘Ã­ch, do Ä‘Ã³ yÃªu cáº§u báº¥t ká»³ há»‡ thá»‘ng build nÃ o Ä‘Æ°á»£c yÃªu cáº§u bá»Ÿi má»—i thÆ° viá»‡n kernel.</p>
<p><a href="https://huggingface.co/blog/hello-hf-kernels">GÃ³i kernels</a> giáº£i quyáº¿t váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch táº£i xuá»‘ng cÃ¡c binary Ä‘Æ°á»£c build sáºµn cá»§a cÃ¡c kernel Ä‘Æ°á»£c há»— trá»£ tá»« Hub. Báº¡n chá»‰ cáº§n chá»‰ ra kernel báº¡n muá»‘n sá»­ dá»¥ng vÃ  <code>kernels</code> sáº½ tÃ¬m kiáº¿m má»™t phiÃªn báº£n tÆ°Æ¡ng thÃ­ch vá»›i há»‡ thá»‘ng cá»§a báº¡n vÃ  táº£i xuá»‘ng khi sá»­ dá»¥ng láº§n Ä‘áº§u.</p>
<h3 id="kernel-tÃ¹y-chá»‰nh-cho-gpt-oss">Kernel tÃ¹y chá»‰nh cho GPT-OSS</h3>
<p><a href="https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src/transformers/models/gpt_oss/modeling_gpt_oss.py">GPT-OSS</a>, má»™t mÃ´ hÃ¬nh Mixture of Experts (MoE), lÃ  má»™t ngÆ°á»i dÃ¹ng lá»›n cá»§a Kernel tá»« Hub. NÃ³ táº­n dá»¥ng má»™t sá»‘ kernel tÃ¹y chá»‰nh:</p>
<ol>
<li>Liger RMSNorm, Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m <a href="https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src/transformers/models/gpt_oss/modeling_gpt_oss.py#L46"><code>@use_kernel_forward_from_hub(&quot;RMSNorm&quot;)</code></a></li>
<li>Kernel Megablocks MoE: <a href="https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src/transformers/models/gpt_oss/modular_gpt_oss.py#L160"><code>@use_kernel_forward_from_hub(&quot;MegaBlocksMoeMLP&quot;)</code></a></li>
<li>Flash Attention 3 vá»›i <a href="https://huggingface.co/kernels-community/vllm-flash-attn3">há»— trá»£ cho attention sinks</a>.</li>
<li>Kernel triton MXFP4 (Ä‘Æ°á»£c Ä‘á» cáº­p <a href="#mxfp4-in-transformers">sau</a>)</li>
</ol>
<p>HÃ£y xem hai kernel Ä‘áº§u tiÃªn.</p>
<p>Äáº±ng sau háº­u trÆ°á»ng, cÃ¡c decorator (1 vÃ  2) chá»‰ Ä‘Æ¡n giáº£n lÃ  trá» Ä‘áº¿n cÃ¡c kernel do cá»™ng Ä‘á»“ng Ä‘Ã³ng gÃ³p. VÃ­ dá»¥: <code>RMSNorm</code> Ä‘áº¿n tá»« <a href="https://huggingface.co/kernels-community/liger_kernels"><code>liger_kernels</code></a>, trong khi kernel <code>MegaBlocksMoeMLP</code> Ä‘áº¿n tá»« <a href="https://huggingface.co/kernels-community/megablocks"><code>megablocks</code></a>. TÃ¹y thuá»™c vÃ o thiáº¿t bá»‹ cá»§a báº¡n (CUDA hoáº·c ROCm) vÃ  báº¡n Ä‘ang huáº¥n luyá»‡n hay cháº¡y suy luáº­n, kernel phÃ¹ há»£p sáº½ Ä‘Æ°á»£c kÃ©o vÃ o tá»± Ä‘á»™ng.</p>
<p>Thiáº¿t káº¿ nÃ y vá»«a <strong>cá»¥ thá»ƒ vá»«a tá»•ng quÃ¡t</strong>: cÃ¡c kernel RMSNorm liger Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng láº¡i trÃªn nhiá»u mÃ´ hÃ¬nh vÃ  kernel MoE cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho MoE trong tÆ°Æ¡ng lai.</p>
<p>VÃ¬ <code>kernels</code> kÃ©o mÃ£ tá»« Hub, báº¡n pháº£i chá»n tham gia tÃ­nh nÄƒng nÃ y báº±ng cÃ¡ch chuyá»ƒn <code>use_kernels=True</code> trong khá»Ÿi táº¡o mÃ´ hÃ¬nh cá»§a báº¡n, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ bÃªn dÆ°á»›i. ChÃºng tÃ´i báº­t ghi log <code>INFO</code> trong vÃ­ dá»¥ Ä‘á»ƒ báº¡n cÃ³ thá»ƒ dá»… dÃ ng xÃ¡c minh ráº±ng cÃ¡c kernel cÃ³ thá»ƒ táº£i xuá»‘ng Ä‘ang Ä‘Æ°á»£c sá»­ dá»¥ng.</p>
<blockquote>
<p>CÃ¡c kernel nÃ y khÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i <code>mxfp4</code>, vÃ¬ váº­y suy luáº­n sáº½ xáº£y ra trong <code>bfloat16</code> náº¿u báº¡n sá»­ dá»¥ng chÃºng. Vui lÃ²ng benchmark há»‡ thá»‘ng cá»§a báº¡n Ä‘á»ƒ cÃ³ sá»± káº¿t há»£p tá»‘t nháº¥t vá» bá»™ nhá»› vÃ  thÃ´ng lÆ°á»£ng phÃ¹ há»£p vá»›i dá»± Ã¡n cá»§a báº¡n!</p></blockquote>
<p>python
from transformers import AutoTokenizer, AutoModelForCausalLM</p>
<p>import logging
logging.basicConfig(level=logging.INFO)</p>
<p>model_id = &ldquo;openai/gpt-oss-20b&rdquo;
tokenizer = AutoTokenizer.from_pretrained(model_id)</p>
<p>model = AutoModelForCausalLM.from_pretrained(
model_id,
dtype=&ldquo;auto&rdquo;,
device_map=&ldquo;auto&rdquo;,
use_kernels=True,
)</p>
<p>Cháº¡y má»™t tháº¿ há»‡ nhanh chÃ³ng táº¡o ra cÃ¡c thÃ´ng bÃ¡o log nhÆ°</p>
<p>shell
INFO:root:Using layer <code>LigerRMSNorm</code> from repo <code>kernels-community/liger_kernels</code>
INFO:root:Using layer <code>MegaBlocksMoeMLP</code> from repo <code>kernels-community/megablocks</code></p>
<p><strong>HÃ¬nh 1</strong> cho tháº¥y ráº±ng, trong há»‡ thá»‘ng chÃºng tÃ´i Ä‘Ã£ thá»­ nghiá»‡m, cÃ¡c kernel nÃ y hoáº¡t Ä‘á»™ng tá»‘t nháº¥t cho cÃ¡c batch size lá»›n hÆ¡n. ChÃºng tÃ´i luÃ´n khuyÃªn báº¡n nÃªn benchmark báº¥t ká»³ thay Ä‘á»•i nÃ o liÃªn quan Ä‘áº¿n hiá»‡u suáº¥t cÃ ng gáº§n vá»›i Ä‘iá»u kiá»‡n sáº£n xuáº¥t cá»§a báº¡n cÃ ng tá»‘t.</p>
<!-- raw HTML omitted -->
<blockquote>
<p>Báº¡n cÃ³ thá»ƒ khÃ¡m phÃ¡ vÃ  chÆ¡i vá»›i script benchmark <a href="https://huggingface.co/datasets/ariG23498/faster-transformers-scripts/blob/main/benchmark-kernels-with-without.py">táº¡i Ä‘Ã¢y</a></p></blockquote>
<h3 id="flash-attention-3">Flash Attention 3</h3>
<p>CÃ¡c mÃ´ hÃ¬nh OpenAI gpt-oss sá»­ dá»¥ng <em>attention sinks</em>, giÃºp cáº£i thiá»‡n cháº¥t lÆ°á»£ng vÃ  táº¡o Ä‘iá»u kiá»‡n thuáº­n lá»£i cho viá»‡c sá»­ dá»¥ng cÃ¡c ngá»¯ cáº£nh dÃ i hÆ¡n. NhÃ³m vLLM Ä‘Ã£ thÃªm tÃ­nh nÄƒng nÃ y vÃ o phiÃªn báº£n Flash Attention má»›i nháº¥t (Flash Attention 3) vÃ  kernel tÃ¹y chá»‰nh káº¿t quáº£ cÃ³ sáºµn <a href="https://huggingface.co/kernels-community/vllm-flash-attn3">trÃªn Hub</a>. Hiá»‡n táº¡i, kernel nÃ y tÆ°Æ¡ng thÃ­ch vá»›i kiáº¿n trÃºc Hopper. Náº¿u báº¡n cÃ³ má»™t kiáº¿n trÃºc, Ä‘Ã¢y lÃ  cÃ¡ch Ä‘á»ƒ báº­t nÃ³:</p>
<p>diff
model = AutoModelForCausalLM.from_pretrained(
model_id,
dtype=&ldquo;auto&rdquo;,
device_map=&ldquo;auto&rdquo;,</p>
<ul>
<li>
<h1 id="flash-attention-with-sinks">Flash Attention with Sinks</h1>
</li>
<li>attn_implementation=&ldquo;kernels-community/vllm-flash-attn3&rdquo;,
)</li>
</ul>
<h2 id="lÆ°á»£ng-tá»­-hÃ³a-mxfp4">LÆ°á»£ng tá»­ hÃ³a MXFP4</h2>
<p>CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n ráº¥t tá»‘n bá»™ nhá»›. LÆ°á»£ng tá»­ hÃ³a lÃ m giáº£m footprint bá»™ nhá»› báº±ng cÃ¡ch lÆ°u trá»¯ weights (vÃ  Ä‘Ã´i khi lÃ  activations) á»Ÿ cÃ¡c Ä‘á»‹nh dáº¡ng cÃ³ Ä‘á»™ chÃ­nh xÃ¡c tháº¥p hÆ¡n. Äá»ƒ tham kháº£o, <code>FP32</code> sá»­ dá»¥ng 32 bit cho má»—i sá»‘ vÃ  <code>BF16</code> sá»­ dá»¥ng 16. Báº±ng cÃ¡ch giáº£m Ä‘á»™ rá»™ng bit, chÃºng ta Ä‘Ã¡nh Ä‘á»•i má»™t sá»‘ Ä‘á»™ chÃ­nh xÃ¡c Ä‘á»ƒ cÃ³ cÃ¡c mÃ´ hÃ¬nh nhá» hÆ¡n vÃ  di chuyá»ƒn bá»™ nhá»› nhanh hÆ¡n.</p>
<p>Náº¿u báº¡n muá»‘n cÃ³ má»™t primer trá»±c quan vá» cÃ¡c trade-off lÆ°á»£ng tá»­ hÃ³a, bÃ i viáº¿t cá»§a <a href="https://huggingface.co/MaartenGr">Maarten Grootendorst</a> lÃ  tuyá»‡t vá»i: <a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization"><em>HÆ°á»›ng dáº«n trá»±c quan vá» lÆ°á»£ng tá»­ hÃ³a</em></a>.</p>
<h3 id="mxfp4-lÃ -gÃ¬">MXFP4 lÃ  gÃ¬</h3>
<!-- raw HTML omitted -->
<p><code>MXFP4</code> lÃ  má»™t Ä‘á»‹nh dáº¡ng dáº¥u pháº©y Ä‘á»™ng 4-bit vá»›i bá»‘ cá»¥c E2M1: 1 bit dáº¥u, 2 bit sá»‘ mÅ© vÃ  1 bit pháº§n Ä‘á»‹nh trá»‹, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong <strong>HÃ¬nh 2</strong>. Báº£n thÃ¢n E2M1 ráº¥t thÃ´. MXFP4 bÃ¹ Ä‘áº¯p báº±ng <strong>scaling theo khá»‘i</strong>:</p>
<ul>
<li>CÃ¡c vector Ä‘Æ°á»£c nhÃ³m thÃ nh cÃ¡c khá»‘i gá»“m 32 pháº§n tá»­.</li>
<li>Má»—i khá»‘i lÆ°u trá»¯ má»™t scale Ä‘Æ°á»£c chia sáº» khÃ´i phá»¥c pháº¡m vi Ä‘á»™ng khi dequantizing.</li>
<li>BÃªn trong má»—i khá»‘i, cÃ¡c giÃ¡ trá»‹ 4-bit Ä‘áº¡i diá»‡n cho cÃ¡c sá»‘ liÃªn quan Ä‘áº¿n scale Ä‘Ã³.</li>
</ul>
<p>LÆ°á»£c Ä‘á»“ theo khá»‘i nÃ y cho phÃ©p <code>MXFP4</code> giá»¯ pháº¡m vi trong khi sá»­ dá»¥ng ráº¥t Ã­t bit. Trong thá»±c táº¿, GPT-OSS 20B phÃ¹ há»£p vá»›i khoáº£ng <code>16 GB</code> VRAM vÃ  GPT-OSS 120B phÃ¹ há»£p vá»›i khoáº£ng <code>80 GB</code> khi <code>MXFP4</code> hoáº¡t Ä‘á»™ng, Ä‘Ã³ lÃ  sá»± khÃ¡c biá»‡t giá»¯a &ldquo;khÃ´ng thá»ƒ táº£i&rdquo; vÃ  &ldquo;cÃ³ thá»ƒ cháº¡y trÃªn má»™t GPU duy nháº¥t&rdquo;. Váº¥n Ä‘á» lÃ  cÃ¡c phÃ©p nhÃ¢n ma tráº­n hiá»‡n pháº£i tÃ´n trá»ng cÃ¡c scale khá»‘i. LÃ m Ä‘iá»u nÃ y má»™t cÃ¡ch hiá»‡u quáº£ á»Ÿ quy mÃ´ lá»›n Ä‘Ã²i há»i cÃ¡c kernel chuyÃªn dá»¥ng.</p>
<h3 id="mxfp4-trong-transformers">MXFP4 trong <code>transformers</code></h3>
<p><code>transformers</code> hiá»‡n bao gá»“m há»— trá»£ gá»‘c cho MXFP4, táº­n dá»¥ng cÃ¡c kernel <code>triton</code> (MXFP4) Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a Ä‘á»ƒ tÄƒng cÆ°á»ng hiá»‡u suáº¥t. Äiá»u nÃ y xÃ¢y dá»±ng trÃªn phÃ¢n phá»‘i kernel do cá»™ng Ä‘á»“ng Ä‘iá»u khiá»ƒn <a href="#zero-build-kernels-downloadable-from-the-hub">Ä‘Æ°á»£c tháº£o luáº­n trÆ°á»›c Ä‘Ã³</a>, sá»­ dá»¥ng cÃ¡c kernel Ä‘Æ°á»£c biÃªn dá»‹ch trÆ°á»›c tá»« Hub Ä‘á»ƒ Ä‘Æ¡n giáº£n hÃ³a viá»‡c triá»ƒn khai.</p>
<p>Chi tiáº¿t triá»ƒn khai chÃ­nh:</p>
<ul>
<li>Logic quantizer: ÄÆ°á»£c tÃ¬m tháº¥y trong <a href="https://github.com/huggingface/transformers/blob/0997c2f2ab08c32c8e2f90aaad06e29a7108535b/src/transformers/quantizers/quantizer_mxfp4.py">tá»‡p quantizer MXFP4</a>, Ä‘iá»u nÃ y xá»­ lÃ½ quÃ¡ trÃ¬nh lÆ°á»£ng tá»­ hÃ³a cá»‘t lÃµi cho MXFP4.</li>
<li>Integration hooks: <a href="https://github.com/huggingface/transformers/blob/0997c2f2ab08c32c8e2f90aaad06e29a7108535b/src/transformers/integrations/mxfp4.py">Tá»‡p tÃ­ch há»£p MXFP4</a> cho phÃ©p sá»­ dá»¥ng MXFP4 liá»n máº¡ch trong framework transformers.</li>
</ul>
<p>Äá»ƒ kiá»ƒm tra xem má»™t mÃ´ hÃ¬nh cÃ³ há»— trá»£ <code>MXFP4</code> hay khÃ´ng, hÃ£y kiá»ƒm tra cáº¥u hÃ¬nh cá»§a nÃ³:</p>
<p>py
from transformers import GptOssConfig</p>
<p>model_id = &ldquo;openai/gpt-oss-120b&rdquo;
cfg = GptOssConfig.from_pretrained(model_id)
print(cfg.quantization_config)</p>
<h1 id="vÃ­-dá»¥-Ä‘áº§u-ra">VÃ­ dá»¥ Ä‘áº§u ra:</h1>
<h1 id="heading">{</h1>
<h1 id="modules_to_not_convert-">&lsquo;modules_to_not_convert&rsquo;: [</h1>
<h1 id="modellayersself_attn">&lsquo;model.layers.*.self_attn&rsquo;,</h1>
<h1 id="modellayersmlprouter">&lsquo;model.layers.*.mlp.router&rsquo;,</h1>
<h1 id="modelembed_tokens">&lsquo;model.embed_tokens&rsquo;,</h1>
<h1 id="lm_head">&rsquo;lm_head&rsquo;</h1>
<h1 id="heading-1">],</h1>
<h1 id="quant_method-mxfp4">&lsquo;quant_method&rsquo;: &lsquo;mxfp4&rsquo;</h1>
<h1 id="heading-2">}</h1>
<p>Náº¿u cÃ³ <code>'quant_method': 'mxfp4'</code>, mÃ´ hÃ¬nh sáº½ tá»± Ä‘á»™ng sá»­ dá»¥ng Ä‘Æ°á»ng dáº«n MXFP4 vá»›i kernel Triton khi Ä‘Æ°á»£c há»— trá»£.</p>
<blockquote>
<p>Nhá» <a href="https://github.com/huggingface/transformers/pull/40176">pull request</a> nÃ y, báº¡n cÃ³ thá»ƒ tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh gpt-oss vÃ  lÆ°u chÃºng trá»±c tiáº¿p vÃ o Hub á»Ÿ Ä‘á»‹nh dáº¡ng MXFP4, há»£p lÃ½ hÃ³a viá»‡c triá»ƒn khai vá»›i hiá»‡u suáº¥t Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a.</p></blockquote>
<h3 id="yÃªu-cáº§u-vÃ -dá»±-phÃ²ng">YÃªu cáº§u vÃ  dá»± phÃ²ng</h3>
<p>Äá»ƒ cháº¡y <code>MXFP4</code> trÃªn GPU, báº¡n cáº§n:</p>
<ol>
<li><code>accelerate</code>, <code>kernels</code> vÃ  <code>triton&gt;=3.4</code> Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t. LÆ°u Ã½ ráº±ng <code>Pytorch 2.8</code> Ä‘Ã£ Ä‘i kÃ¨m vá»›i <code>triton 3.4</code>, vÃ¬ váº­y báº¡n chá»‰ cáº§n cÃ i Ä‘áº·t thá»§ cÃ´ng triton náº¿u sá»­ dá»¥ng <code>Pytorch 2.7</code>.</li>
<li>GPU NVIDIA vá»›i compute capability <code>â‰¥ 7.5</code>. Äiá»u nÃ y quay trá»Ÿ láº¡i Tesla, vÃ¬ váº­y báº¡n cÃ³ thá»ƒ cháº¡y <code>gpt-oss-20b</code> trÃªn cÃ¡c táº§ng miá»…n phÃ­ cá»§a Google Colab vÃ  Kaggle, vÃ  trÃªn nhiá»u GPU tiÃªu dÃ¹ng.</li>
</ol>
<p>Náº¿u cÃ¡c rÃ ng buá»™c nÃ y khÃ´ng Ä‘Æ°á»£c Ä‘Ã¡p á»©ng, <code>transformers</code> sáº½ quay trá»Ÿ láº¡i Ä‘Æ°á»ng dáº«n cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n (<code>bfloat16</code> Ä‘Æ°á»£c sá»­ dá»¥ng theo máº·c Ä‘á»‹nh), yÃªu cáº§u khoáº£ng 4 láº§n bá»™ nhá»› cá»§a MXFP4.</p>
<p><a href="https://huggingface.co/datasets/ariG23498/faster-transformers-scripts/blob/main/memory-requirements-quantized-vs-dequantized.py">Snippet</a> táº£i GPT-OSS hai láº§n trÃªn CUDA: má»™t láº§n vá»›i <code>Mxfp4Config(dequantize=True)</code> (tá»‘n nhiá»u bá»™ nhá»›) vÃ  má»™t láº§n trong Ä‘Æ°á»ng dáº«n lÆ°á»£ng tá»­ hÃ³a máº·c Ä‘á»‹nh (hiá»‡u quáº£ bá»™ nhá»›). <strong>HÃ¬nh 3</strong> hiá»ƒn thá»‹ lÆ°á»£ng VRAM Ä‘Æ°á»£c sá»­ dá»¥ng sau má»—i láº§n táº£i Ä‘á»ƒ báº¡n cÃ³ thá»ƒ hÃ¬nh dung cÃ¡c khoáº£n tiáº¿t kiá»‡m.</p>
<!-- raw HTML omitted -->
<h3 id="kernel-cho-mxfp4">Kernel cho MXFP4</h3>
<p><code>MXFP4</code> hiá»‡u quáº£ yÃªu cáº§u cÃ¡c kernel hiá»ƒu cÃ¡c khá»‘i 32 pháº§n tá»­ vÃ  scale cá»§a chÃºng trong quÃ¡ trÃ¬nh GEMM vÃ  fused ops. ÄÃ¢y lÃ  nÆ¡i <strong>Kernel tá»« Hub</strong> xuáº¥t hiá»‡n trá»Ÿ láº¡i. <code>transformers</code> tá»± Ä‘á»™ng kÃ©o vÃ o kernel Triton nháº­n biáº¿t <code>MXFP4</code> tá»« kho lÆ°u trá»¯ cá»™ng Ä‘á»“ng khi báº¡n táº£i má»™t mÃ´ hÃ¬nh cáº§n chÃºng. Kho lÆ°u trá»¯ sáº½ xuáº¥t hiá»‡n trong bá»™ nhá»› cache cá»¥c bá»™ cá»§a báº¡n vÃ  sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng trong quÃ¡ trÃ¬nh chuyá»ƒn tiáº¿p. Äá»‘i vá»›i cÃ¡c kernel <code>MXFP4</code>, ngÆ°á»i ta khÃ´ng cáº§n sá»­ dá»¥ng tham sá»‘ <code>use_kernels=True</code> nhÆ° trÆ°á»›c Ä‘Ã¢y, nÃ³ Ä‘Æ°á»£c Ä‘áº·t thÃ nh máº·c Ä‘á»‹nh trong <code>transformers</code>.</p>
<p>Kiá»ƒm tra nhanh chÃ³ng vá»›i CLI bá»™ nhá»› cache Hugging Face, sau khi cháº¡y <code>gpt-oss-20b</code> trÃªn má»™t GPU tÆ°Æ¡ng thÃ­ch vá»›i kernel MXFP4 triton:</p>
<p>shell
hf cache scan</p>
<p>Äáº§u ra máº«u:</p>
<p>shell
REPO ID                          REPO TYPE SIZE ON DISK</p>
<hr>
<p>kernels-community/triton_kernels model           536.2K
openai/gpt-oss-20b               model            13.8G</p>
<p>Äiá»u nÃ y chá»‰ ra ráº±ng cÃ¡c kernel MXFP4 Ä‘Ã£ Ä‘Æ°á»£c tÃ¬m náº¡p vÃ  cÃ³ sáºµn Ä‘á»ƒ thá»±c thi.</p>
<p>HÃ£y cháº¡y má»™t sá»‘ benchmark vÃ  xem kernel MXFP4 hoáº¡t Ä‘á»™ng tá»‘t nhÆ° tháº¿ nÃ o. Trong <strong>HÃ¬nh 4</strong>, chÃºng ta tháº¥y ráº±ng cÃ¡c kernel <code>MXFP4</code> tháº­m chÃ­ cÃ²n tá»‘t hÆ¡n cÃ¡c kernel MoE vÃ  RMSNorm tÃ¹y chá»‰nh cho cÃ¡c batch lá»›n hÆ¡n.</p>
<!-- raw HTML omitted -->
<blockquote>
<p>Báº¡n cÃ³ thá»ƒ khÃ¡m phÃ¡ vÃ  chÆ¡i vá»›i script benchmark <a href="https://huggingface.co/datasets/ariG23498/faster-transformers-scripts/blob/main/benchmark-mxfp4-kernels.py">táº¡i Ä‘Ã¢y</a></p></blockquote>
<h2 id="song-song-hÃ³a-tensor">Song song hÃ³a Tensor</h2>
<!-- raw HTML omitted -->
<p>Song song hÃ³a Tensor (TP) chia <strong>cÃ¡c tensor bÃªn trong má»™t lá»›p</strong> trÃªn nhiá»u GPU (nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong <strong>HÃ¬nh 5</strong>). Má»—i GPU nhÃ¢n shard cá»§a nÃ³ song song, vÃ  sau Ä‘Ã³ cÃ¡c káº¿t quáº£ má»™t pháº§n Ä‘Æ°á»£c thu tháº­p báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c hoáº¡t Ä‘á»™ng all-gather hoáº·c all-reduce.
Äiá»u nÃ y lÃ m giáº£m bá»™ nhá»› trÃªn má»—i GPU vÃ  giá»¯ cho táº¥t cáº£ cÃ¡c GPU hoáº¡t Ä‘á»™ng trÃªn <strong>cÃ¹ng má»™t lá»›p</strong>, Ä‘iá»u nÃ y cáº£i thiá»‡n thÃ´ng lÆ°á»£ng khi Ä‘á»™ dÃ i chuá»—i hoáº·c batch size tÄƒng lÃªn. TP tá»‘n nhiá»u giao tiáº¿p vÃ  thÆ°á»ng hoáº¡t Ä‘á»™ng tá»‘t nháº¥t trÃªn <strong>má»™t mÃ¡y duy nháº¥t vá»›i cÃ¡c liÃªn káº¿t intra-node nhanh</strong>.</p>
<h3 id="Ä‘iá»u-nÃ y-cho-phÃ©p-Ä‘iá»u-gÃ¬-trong-transformers">Äiá»u nÃ y cho phÃ©p Ä‘iá»u gÃ¬ trong <code>transformers</code></h3>
<p><code>transformers</code> triá»ƒn khai TP trá»±c tiáº¿p trong <code>from_pretrained</code>. Báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u vá»›i plan Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trÆ°á»›c:</p>
<p>python</p>
<h1 id="cháº¡y-vá»›i-torchrun-nproc-per-node-4-tp_gpt_osspy">cháº¡y vá»›i: torchrun &ndash;nproc-per-node 4 tp_gpt_oss.py</h1>
<p>import torch
from transformers import PreTrainedTokenizerFast, GptOssForCausalLM
from transformers.distributed import DistributedConfig</p>
<p>model_id = &ldquo;openai/gpt-oss-120b&rdquo;
tokenizer = PreTrainedTokenizerFast.from_pretrained(model_id)
model = GptOssForCausalLM.from_pretrained(
model_id,
tp_plan=&ldquo;auto&rdquo;, # built in TP support
dtype=&ldquo;auto&rdquo;,
).eval()</p>
<p>messages = [
{&ldquo;role&rdquo;: &ldquo;system&rdquo;, &ldquo;content&rdquo;: &ldquo;Be concise.&rdquo;},
{&ldquo;role&rdquo;: &ldquo;user&rdquo;, &ldquo;content&rdquo;: &ldquo;Explain KV caching briefly.&rdquo;},
]
inputs = tokenizer.apply_chat_template(
messages,
add_generation_prompt=True,
return_tensors=&ldquo;pt&rdquo;,
return_dict=True,
reasoning_effort=&ldquo;low&rdquo;,
).to(model.device)</p>
<p>with torch.inference_mode():
generations = model.generate(**inputs, max_new_tokens=128)</p>
<p>print(tokenizer.decode(generations[0][inputs[&ldquo;input_ids&rdquo;].shape[-1]:]))</p>
<p>Náº¿u báº¡n khÃ´ng cÃ³ cÆ¡ sá»Ÿ háº¡ táº§ng Ä‘á»ƒ cháº¡y Ä‘oáº¡n mÃ£ trÃªn, báº¡n chá»‰ cáº§n táº¡o má»™t process trÃªn GPU cá»§a chÃºng tÃ´i báº±ng cÃ¡ch sá»­ dá»¥ng <a href="https://huggingface.co/docs/huggingface_hub/en/guides/jobs">Hugging Face Jobs</a>!</p>
<p>bash
hf jobs run &ndash;detach &ndash;flavor l4x4 ghcr.io/astral-sh/uv:debian /bin/bash -c <br>
&ldquo;uv venv .venv &ndash;python 3.12 &amp;&amp; <br>
source .venv/bin/activate &amp;&amp; <br>
uv pip install &ndash;upgrade torch numpy transformers accelerate triton kernels &amp;&amp; <br>
wget <a href="https://huggingface.co/datasets/ariG23498/distributed/raw/main/tp_gpt_oss.py">https://huggingface.co/datasets/ariG23498/distributed/raw/main/tp_gpt_oss.py</a> &amp;&amp; <br>
torchrun &ndash;nproc-per-node=4 tp_gpt_oss.py&rdquo;</p>
<blockquote>
<p><a href="https://huggingface.co/docs/huggingface_hub/guides/jobs"><code>hf jobs</code></a> cÃ³ sáºµn cho táº¥t cáº£ ngÆ°á»i dÃ¹ng Hugging Face PRO &amp; Enterprise.</p></blockquote>
<p>BÃªn dÆ°á»›i, <code>tp_plan=&quot;auto&quot;</code> chá»n má»™t cÃ´ng thá»©c sharding Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trÆ°á»›c cho má»—i lá»›p vÃ  káº¿t ná»‘i cÃ¡c <a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=a0:_parallel_programming_crash_course">táº­p há»£p</a> cáº§n thiáº¿t. Báº¡n cÃ³ thá»ƒ kiá»ƒm tra plan Ä‘ang hoáº¡t Ä‘á»™ng báº±ng <code>print(model._tp_plan)</code> náº¿u báº¡n muá»‘n xÃ¡c minh nhá»¯ng gÃ¬ Ä‘ang Ä‘Æ°á»£c sharded.</p>
<h3 id="khi-nÃ o-nÃªn-sá»­-dá»¥ng-tp">Khi nÃ o nÃªn sá»­ dá»¥ng TP</h3>
<p>Sá»­ dá»¥ng TP khi mÃ´ hÃ¬nh quÃ¡ lá»›n Ä‘á»‘i vá»›i má»™t GPU vÃ  báº¡n muá»‘n <strong>tÃ­nh toÃ¡n song song</strong>, khÃ´ng chá»‰ vá»‹ trÃ­ bá»™ nhá»›. TP cÃ³ xu hÆ°á»›ng scale thÃ´ng lÆ°á»£ng vá»›i nhiá»u GPU hÆ¡n, Ä‘áº·c biá»‡t Ä‘á»‘i vá»›i cÃ¡c chuá»—i dÃ i hoáº·c cÃ¡c batch lá»›n hÆ¡n.</p>
<blockquote>
<p>Náº¿u báº¡n tÃ² mÃ² vá» cÃ¡ch TP khÃ¡c vá»›i <code>device_map=&quot;auto&quot;</code> (vá»‹ trÃ­ bá»™ nhá»›), <a href="https://stackoverflow.com/questions/78852192/choose-available-gpu-devices-with-device-map">cÃ¢u tráº£ lá»i Stack Overflow</a> ngáº¯n gá»n nÃ y giáº£i thÃ­ch sá»± khÃ¡c biá»‡t vÃ  khi nÃ o nÃªn sá»­ dá»¥ng má»—i loáº¡i.</p></blockquote>
<p>Äá»ƒ tÃ¬m hiá»ƒu thÃªm vá» TP, Ä‘Ã¢y lÃ  hai tÃ i nguyÃªn pháº£i Ä‘á»c:</p>
<ul>
<li><a href="https://huggingface.co/docs/transformers/en/perf_infer_gpu_multi">HÆ°á»›ng dáº«n <code>transformers</code></a>: Song song hÃ³a Tensor, cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c há»— trá»£, plan vÃ  cÃ¡c Ä‘iá»ƒm má»Ÿ rá»™ng.</li>
<li><a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=tensor_parallelism">Sá»• tay Ultra-Scale</a>: background vá» TP vÃ  má»‘i quan há»‡ cá»§a nÃ³ vá»›i cÃ¡c cháº¿ Ä‘á»™ song song hÃ³a khÃ¡c.</li>
</ul>
<h2 id="song-song-hÃ³a-expert">Song song hÃ³a Expert</h2>
<p>Expert Parallelism (EP) shard <strong>cÃ¡c expert bÃªn trong cÃ¡c lá»›p MoE</strong> trÃªn cÃ¡c GPU. Má»—i token Ä‘Æ°á»£c Ä‘á»‹nh tuyáº¿n Ä‘áº¿n má»™t hoáº·c má»™t vÃ i expert, vÃ¬ váº­y chá»‰ cÃ¡c expert Ä‘Ã³ cháº¡y feed-forward pass cá»§a chÃºng. VÃ¬ cÃ¡c expert lÃ  cÃ¡c MLP Ä‘á»™c láº­p, chÃºng ta cÃ³ thá»ƒ Ä‘áº·t cÃ¡c expert khÃ¡c nhau trÃªn cÃ¡c rank khÃ¡c nhau vÃ  chá»‰ trao Ä‘á»•i cÃ¡c tráº¡ng thÃ¡i áº©n cho cÃ¡c token Ä‘Æ°á»£c Ä‘á»‹nh tuyáº¿n. Äiá»u nÃ y giá»¯ cho cÃ¡c phÃ©p nhÃ¢n ma tráº­n cÃ²n nguyÃªn váº¹n trÃªn má»—i rank vÃ  thay tháº¿ slicing tensor báº±ng Ä‘á»‹nh tuyáº¿n vÃ  táº­p há»£p.</p>
<p>Cháº¡y vá»›i nhiá»u process báº±ng <code>torchrun</code>. EP Ä‘Æ°á»£c báº­t thÃ´ng qua cáº¥u hÃ¬nh phÃ¢n tÃ¡n vÃ  hoáº¡t Ä‘á»™ng vá»›i cÃ¡c lá»›p GPT-OSS MoE ngay láº­p tá»©c trong transformers.</p>
<p>python</p>
<h1 id="cháº¡y-vá»›i-torchrun-nproc-per-node-4-ep_gpt_osspy">cháº¡y vá»›i: torchrun &ndash;nproc-per-node 4 ep_gpt_oss.py</h1>
<p>import torch
from transformers import PreTrainedTokenizerFast, GptOssForCausalLM
from transformers.distributed import DistributedConfig</p>
<p>model_id = &ldquo;openai/gpt-oss-120b&rdquo;
tokenizer = PreTrainedTokenizerFast.from_pretrained(model_id)
model = GptOssForCausalLM.from_pretrained(
model_id,
distributed_config=DistributedConfig(enable_expert_parallel=True), # enabling EP
dtype=&ldquo;auto&rdquo;,
).eval()</p>
<p>messages = [
{&ldquo;role&rdquo;: &ldquo;system&rdquo;, &ldquo;content&rdquo;: &ldquo;Be concise.&rdquo;},
{&ldquo;role&rdquo;: &ldquo;user&rdquo;, &ldquo;content&rdquo;: &ldquo;Explain KV caching briefly.&rdquo;},
]
inputs = tokenizer.apply_chat_template(
messages,
add_generation_prompt=True,
return_tensors=&ldquo;pt&rdquo;,
return_dict=True,
reasoning_effort=&ldquo;low&rdquo;,
).to(model.device)</p>
<p>with torch.inference_mode():
generations = model.generate(**inputs, max_new_tokens=128)</p>
<p>print(tokenizer.decode(generations[0][inputs[&ldquo;input_ids&rdquo;].shape[-1]:]))</p>
<p>ÄÃ¢y lÃ  cÃ¡ch báº¡n sáº½ cháº¡y báº±ng <code>hf jobs</code></p>
<p>bash
hf jobs run &ndash;detach &ndash;flavor l4x4 ghcr.io/astral-sh/uv:debian /bin/bash -c <br>
&ldquo;uv venv .venv &ndash;python 3.12 &amp;&amp; <br>
source .venv/bin/activate &amp;&amp; <br>
uv pip install &ndash;upgrade torch numpy transformers accelerate triton kernels &amp;&amp; <br>
wget <a href="https://huggingface.co/datasets/ariG23498/distributed/raw/main/ep_gpt_oss.py">https://huggingface.co/datasets/ariG23498/distributed/raw/main/ep_gpt_oss.py</a> &amp;&amp; <br>
torchrun &ndash;nproc-per-node=4 ep_gpt_oss.py&rdquo;</p>
<blockquote>
<p>Khi báº¡n báº­t Expert Parallelism, Tensor Parallelism cÅ©ng Ä‘Æ°á»£c kÃ­ch hoáº¡t. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  báº¡n táº­n hÆ°á»Ÿng nhá»¯ng Ä‘iá»u tá»‘t nháº¥t cá»§a cáº£ hai tháº¿ giá»›i!</p></blockquote>
<h2 id="lá»›p-cá»­a-sá»•-trÆ°á»£t-Ä‘á»™ng--bá»™-nhá»›-cache">Lá»›p Cá»­a sá»• TrÆ°á»£t Äá»™ng &amp; Bá»™ nhá»› Cache</h2>
<p>Nhiá»u LLM gáº§n Ä‘Ã¢y sá»­ dá»¥ng attention <em>cá»­a sá»• trÆ°á»£t</em>, hoáº·c sá»± káº¿t há»£p cá»§a cÃ¡c lá»›p attention trÆ°á»£t vÃ  toÃ n cá»¥c, nhÆ° má»™t phÆ°Æ¡ng tiá»‡n Ä‘á»ƒ tiáº¿t kiá»‡m bá»™ nhá»› vÃ  giáº£m cÃ¡c matmul báº­c hai tá»‘n kÃ©m phÃ¡t triá»ƒn theo Ä‘á»™ dÃ i chuá»—i. Tuy nhiÃªn, viá»‡c triá»ƒn khai bá»™ nhá»› cache KV Ä‘á»™ng trong transformers váº«n tiáº¿p tá»¥c phÃ¢n bá»• khÃ´ng gian theo Ä‘á»™ dÃ i chuá»—i, mÃ  khÃ´ng xem xÃ©t cÃ¡c lá»›p attention riÃªng láº». Báº¡n luÃ´n cÃ³ thá»ƒ tá»‘i Æ°u hÃ³a bá»™ nhá»› báº±ng cÃ¡ch sá»­ dá»¥ng biÃªn dá»‹ch (nghÄ©a lÃ , cÃ¡c hÃ¬nh dáº¡ng cá»‘ Ä‘á»‹nh), nhÆ°ng Ä‘Ã³ lÃ  má»™t ká»‹ch báº£n riÃªng biá»‡t.</p>
<p><code>transformers</code> hiá»‡n cÃ³ má»™t <a href="https://github.com/huggingface/transformers/blob/64ae6e6b1de2c6822a53be46aba9db68f75ec595/src/transformers/cache_utils.py#L165"><code>DynamicSlidingWindowLayer</code></a> vÃ  má»™t <a href="https://github.com/huggingface/transformers/blob/64ae6e6b1de2c6822a53be46aba9db68f75ec595/src/transformers/cache_utils.py#L959"><code>DynamicCache</code></a> nháº­n biáº¿t <em>cáº¥u hÃ¬nh</em>. Náº¿u cáº¥u hÃ¬nh mÃ´ hÃ¬nh khai bÃ¡o attention cá»­a sá»• trÆ°á»£t hoáº·c attention káº¿t há»£p (cáº£ cÃ¡c lá»›p attention trÆ°á»£t vÃ  toÃ n cá»¥c Ä‘á»u Ä‘Æ°á»£c sá»­ dá»¥ng), bá»™ nhá»› cache sáº½ <strong>ngá»«ng phÃ¡t triá»ƒn vÆ°á»£t quÃ¡ cá»­a sá»•</strong> cho cÃ¡c lá»›p trÆ°á»£t. Náº¿u báº¡n khÃ´ng chuyá»ƒn cáº¥u hÃ¬nh, hÃ nh vi váº«n nhÆ° trÆ°á»›c (KV Ä‘áº§y Ä‘á»§, luÃ´n phÃ¡t triá»ƒn khi Ä‘á»™ dÃ i chuá»—i tÄƒng lÃªn).</p>
<p>Äá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh chá»‰ sá»­ dá»¥ng cÃ¡c lá»›p cá»­a sá»• trÆ°á»£t, cháº³ng háº¡n nhÆ° Mistral 7B, bá»™ nhá»› cache ngá»«ng phÃ¡t triá»ƒn khi chuá»—i Ä‘áº¡t Ä‘áº¿n kÃ­ch thÆ°á»›c cá»­a sá»• (4096, trong trÆ°á»ng há»£p nÃ y). Äiá»u nÃ y cÃ³ Ã½ nghÄ©a, vÃ¬ cÃ¡c lá»›p trÆ°á»£t khÃ´ng thá»ƒ nhÃ¬n xa hÆ¡n 4K token trÆ°á»›c Ä‘Ã³.</p>
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<p>OpenAI gpt-oss luÃ¢n phiÃªn giá»¯a cÃ¡c lá»›p attention trÆ°á»£t vÃ  toÃ n cá»¥c, dáº«n Ä‘áº¿n tá»•ng bá»™ nhá»› cache KV <em>giáº£m má»™t ná»­a</em>, nhÆ° chÃºng ta sáº½ tháº¥y, khi Ä‘á»™ dÃ i chuá»—i tÄƒng lÃªn.
Äiá»u nÃ y cung cáº¥p cho chÃºng ta:</p>
<ul>
<li><strong>Bá»™ nhá»› KV-cache tháº¥p hÆ¡n nhiá»u</strong> cho cÃ¡c mÃ´ hÃ¬nh cÃ³ attention trÆ°á»£t hoáº·c káº¿t há»£p (vÃ­ dá»¥: GPT-OSS). Tá»‘c Ä‘á»™ tÄƒng trÆ°á»Ÿng bá»™ nhá»› cache á»•n Ä‘á»‹nh sau khi Ä‘áº¡t Ä‘áº¿n cá»­a sá»• (vÃ­ dá»¥: 4K cho Mistral; 128 cho cÃ¡c lá»›p trÆ°á»£t GPT-OSS), thay vÃ¬ scale tuyáº¿n tÃ­nh vá»›i tá»•ng sá»‘ token Ä‘Æ°á»£c táº¡o. (<a href="https://github.com/huggingface/transformers/pull/40039">GitHub</a>, <a href="https://huggingface.co/docs/transformers/en/model_doc/mistral">Transformers</a>)</li>
<li><strong>Chiáº¿n tháº¯ng vá» tá»‘c Ä‘á»™/Ä‘á»™ trá»…</strong> trÃªn cÃ¡c prompt dÃ i/tháº¿ há»‡ dÃ i: cÃ¡c tensor KV nhá» hÆ¡n cÃ³ nghÄ©a lÃ  Ä‘á»c/ghi attention nháº¹ hÆ¡n vÃ  Ã­t Ã¡p lá»±c bÄƒng thÃ´ng bá»™ nhá»› hÆ¡n, Ä‘áº·c biá»‡t lÃ  sau khi Ä‘áº¡t Ä‘áº¿n cá»­a sá»•. (ÄÃ¢y lÃ  Ä‘á»™ng lá»±c trung tÃ¢m Ä‘áº±ng sau LLM cá»­a sá»• trÆ°á»£t/káº¿t há»£p.) (<a href="https://www.ai21.com/blog/rise-of-hybrid-llms/">AI21</a>, <a href="https://blog.vllm.ai/2025/08/05/gpt-oss.html">Blog vLLM</a>)</li>
</ul>
<h3 id="cÃ¡ch-sá»­-dá»¥ng-nÃ³">CÃ¡ch sá»­ dá»¥ng nÃ³</h3>
<p>Bá»™ nhá»› cache Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a Ä‘Æ°á»£c Ä‘áº·t theo máº·c Ä‘á»‹nh, Ä‘iá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  <strong>báº¡n khÃ´ng pháº£i thá»±c hiá»‡n báº¥t ká»³ thay Ä‘á»•i nÃ o</strong> Ä‘á»‘i vá»›i mÃ£ hiá»‡n cÃ³ cá»§a mÃ¬nh. Náº¿u báº¡n muá»‘n táº¡o <code>DynamicCache</code> má»™t cÃ¡ch rÃµ rÃ ng, Ä‘Ã¢y lÃ  cÃ¡ch báº¡n sáº½ lÃ m:</p>
<p>python
from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache</p>
<p>model_id = &ldquo;openai/gpt-oss-20b&rdquo;</p>
<p>tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
model_id,
dtype=&ldquo;auto&rdquo;,
device_map=&ldquo;auto&rdquo;,
).eval()</p>
<p>messages = [
{&ldquo;role&rdquo;: &ldquo;system&rdquo;, &ldquo;content&rdquo;: &ldquo;Always respond in riddles&rdquo;},
{&ldquo;role&rdquo;: &ldquo;user&rdquo;, &ldquo;content&rdquo;: &ldquo;What is the weather like in Madrid?&rdquo;},
]</p>
<p>inputs = tokenizer.apply_chat_template(
messages,
add_generation_prompt=True,
return_tensors=&ldquo;pt&rdquo;,
return_dict=True,
reasoning_effort=&ldquo;low&rdquo;,
).to(model.device)</p>
<p>cache = DynamicCache(config=model.config) # táº¡o bá»™ nhá»› cache vá»›i cáº¥u hÃ¬nh cá»§a mÃ´ hÃ¬nh</p>
<p>generated = model.generate(
**inputs,
max_new_tokens=500,
past_key_values=cache
)
print(tokenizer.decode(generated[0][inputs[&ldquo;input_ids&rdquo;].shape[-1]:]))</p>
<p><strong>HÃ¬nh 6</strong> cho tháº¥y sá»± khÃ¡c biá»‡t lá»›n nhÆ° tháº¿ nÃ o Ä‘á»‘i vá»›i chÃºng ta khi sá»­ dá»¥ng Dynamic KV Cache vá»›i attention cá»­a sá»• trÆ°á»£t.</p>
<!-- raw HTML omitted -->
<h2 id="batching-liÃªn-tá»¥c--chÃº-Ã½-theo-trang">Batching liÃªn tá»¥c &amp; ChÃº Ã½ theo Trang</h2>
<p>Má»™t quÃ¡ trÃ¬nh táº¡o tá»± há»“i quy Ä‘iá»ƒn hÃ¬nh trÃ´ng giá»‘ng nhÆ° <strong>HÃ¬nh 7</strong>. Báº¡n nháº­p cÃ¡c token prefill vÃ  mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n tá»«ng token má»›i má»™t sau má»™t cho Ä‘áº¿n khi nÃ³ dá»± Ä‘oÃ¡n token EOS (End of Sequence).</p>
<!-- raw HTML omitted -->
<p>HÃ£y xem quÃ¡ trÃ¬nh táº¡o trÃ´ng nhÆ° tháº¿ nÃ o khi chÃºng ta chuyá»ƒn má»™t <strong>batch</strong> cÃ¡c Ä‘áº§u vÃ o. Trong <strong>HÃ¬nh 8</strong>, báº¡n nháº­n tháº¥y ráº±ng má»™t sá»‘ tháº¿ há»‡ káº¿t thÃºc sá»›m hÆ¡n nhá»¯ng tháº¿ há»‡ khÃ¡c. Sá»± khÃ´ng phÃ¹</p>
<h3 id="link-bÃ i-viáº¿t-gá»‘c"><a href="https://huggingface.co/blog/faster-transformers">Link bÃ i viáº¿t gá»‘c</a></h3>


    <ul id="taxonomy" class="not-prose flex items-center space-x-4 flex-nowrap whitespace-nowrap overflow-x-auto">

      <li class="font-semibold my-4">Tags:</li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/ai/">Ai</a></li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/september-11-2025/">September 11, 2025</a></li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/huggingface.co/">Huggingface.co</a></li>
    </ul>
</div>

    <footer id="content-footer" class="not-prose">

      

      <div id="author-box" class="my-8 md:my-14 border p-8 bg-zinc-100 rounded-2xl">

        <img class="w-20 h-20 rounded-full" src="" alt="">

        <h2 class="text-xl font-bold my-4">Writter by : </h2>

        <p class="mb-4"></p>

        <ul class="flex flex-wrap space-x-4">
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://facebook.com/luat.skyai/" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                <path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951z"/>
              </svg>
              <span class="ml-2">Facebook</span>
            </a>
          </li>
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://facebook.com/luat.skyai/" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                  <path d="M12.6.75h2.454l-5.36 6.142L16 15.25h-4.937l-3.867-5.07-4.425 5.07H.316l5.733-6.57L0 .75h5.063l3.495 4.633L12.601.75Zm-.86 13.028h1.36L4.323 2.145H2.865l8.875 11.633Z"/>
              </svg>
              <span class="ml-2">Twitter</span>
            </a>
          </li>
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://luat.skyai.vn/" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                    <path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.917 3.917 0 0 0-1.417.923A3.927 3.927 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.916 3.916 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.926 3.926 0 0 0-.923-1.417A3.911 3.911 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0h.003zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599.28.28.453.546.598.92.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.47 2.47 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.478 2.478 0 0 1-.92-.598 2.48 2.48 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233 0-2.136.008-2.388.046-3.231.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92.28-.28.546-.453.92-.598.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045v.002zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92zm-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217zm0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334z"/>

              </svg>
              <span class="ml-2">Instagram</span>
            </a>
          </li>

        </ul>
        
      </div>

      <div id="related-post" class="">

        <h2 class="text-xl md:text-2xl font-bold mb-6 md:mb-8">Recommended for You</h2><div class="grid grid-cols-1 md:grid-cols-2 gap-6"><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl">
	
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-09-27/9008f5/"></a>

	<figure class="w-full aspect-video overflow-hidden rounded-3xl">

			<img class="object-cover group-hover:scale-105 transition duration-500 cursor-pointer" src="https://huggingface.co/blog/assets/embeddinggemma/thumbnail.png" alt="ChÃ o má»«ng EmbeddingGemma, mÃ´ hÃ¬nh nhÃºng hiá»‡u quáº£ má»›i cá»§a Google" style="width: 100%; height: 100%; object-fit: cover;">

	  </figure>

	<div class="p-6">

		<time datetime="2025-09-27T00:00:00&#43;00:00">Sep 27, 2025</time>

		<h3 class="my-4 text-2xl font-bold">ChÃ o má»«ng EmbeddingGemma, mÃ´ hÃ¬nh nhÃºng hiá»‡u quáº£ má»›i cá»§a Google</h3>

		<p class="text-normal leading-normal text-zinc-500 line-clamp-2"></p>

	</div>
</article><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl">
	
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-09-27/777761/"></a>

	<figure class="w-full aspect-video overflow-hidden rounded-3xl">

			<img class="object-cover group-hover:scale-105 transition duration-500 cursor-pointer" src="https://huggingface.co/blog/assets/riskrubric/thumbnail.png" alt="DÃ¢n chá»§ hÃ³a An toÃ n AI vá»›i RiskRubric.ai" style="width: 100%; height: 100%; object-fit: cover;">

	  </figure>

	<div class="p-6">

		<time datetime="2025-09-27T00:00:00&#43;00:00">Sep 27, 2025</time>

		<h3 class="my-4 text-2xl font-bold">DÃ¢n chá»§ hÃ³a An toÃ n AI vá»›i RiskRubric.ai</h3>

		<p class="text-normal leading-normal text-zinc-500 line-clamp-2"></p>

	</div>
</article>
          
        </div>
      </div>

    </footer>

  </article>

  
  <aside class="md:col-span-1"><div class="lg:sticky lg:top-8"><div class="rounded-2xl p-4 bg-zinc-100 mb-10">

		<img class="aspect-video rounded" src="/images/email-3249062_1280_by-Muhammad-Ribkhan-Pixabay.jpg" alt="Banner Email Marketing">

		<p class="text-right text-xs mt-2 leading-none text-zinc-500">Adversitiment</p>

</div><div class="space-y-6">

		<h2 class="font-bold text-xl mb-8">Recent Post</h2>

		<article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-11-22/a45b2c/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://cdn-avatars.huggingface.co/v1/production/uploads/66dca5d6f7d6d9129031431a/1igNGyjLni1PuZnPT4Bnf.jpeg" alt="20x Fine-tuning TRL nhanh hÆ¡n vá»›i RapidFire AI" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">20x Fine-tuning TRL nhanh hÆ¡n vá»›i RapidFire AI</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-11-22/d40979/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://www.anthropic.com" alt="Anthropic Ä‘áº§u tÆ° 50 tá»· Ä‘Ã´ la vÃ o cÆ¡ sá»Ÿ háº¡ táº§ng AI cá»§a Má»¹" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Anthropic Ä‘áº§u tÆ° 50 tá»· Ä‘Ã´ la vÃ o cÆ¡ sá»Ÿ háº¡ táº§ng AI cá»§a Má»¹</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-11-22/6cc940/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://www.anthropic.com" alt="Bang Maryland há»£p tÃ¡c vá»›i Anthropic Ä‘á»ƒ phá»¥c vá»¥ ngÆ°á»i dÃ¢n tá»‘t hÆ¡n" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Bang Maryland há»£p tÃ¡c vá»›i Anthropic Ä‘á»ƒ phá»¥c vá»¥ ngÆ°á»i dÃ¢n tá»‘t hÆ¡n</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-11-22/07808c/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://cdn-avatars.huggingface.co/v1/production/uploads/6384db7fb2906edaf835a91d/MOTXxaOmjlTZ8wONYifnD.jpeg" alt="Báº£ng xáº¿p háº¡ng Open ASR- Xu hÆ°á»›ng vÃ  cÃ¡i nhÃ¬n sÃ¢u sáº¯c vá»›i cÃ¡c Track Äa ngÃ´n ngá»¯ &amp; DÃ i háº¡n má»›i" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Báº£ng xáº¿p háº¡ng Open ASR- Xu hÆ°á»›ng vÃ  cÃ¡i nhÃ¬n sÃ¢u sáº¯c vá»›i cÃ¡c Track Äa ngÃ´n ngá»¯ &amp; DÃ i háº¡n má»›i</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-11-22/7781a3/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="http://lh3.googleusercontent.com/ipGH8HHMSu49DthGP28XZZ5J8trwbBeVwbkxHoxipOBB1rLI_MRjUeU2zswaIkPgWKRHu5eJGZmUIQWidxCPJ2vVB0KUqy7NwCtLjxu9FU71B-rxcg=w1440-h1440-n-nu" alt="Báº¯t Ä‘áº§u xÃ¢y dá»±ng vá»›i Gemini 3" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Báº¯t Ä‘áº§u xÃ¢y dá»±ng vá»›i Gemini 3</h3>
	</div>
</article>
		
	</div>

</div>
</aside>

</div>
  
</main>
  <footer class="bg-zinc-100 py-10 md:py-14"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8">

  <div class="flex flex-wrap space-y-6 mb-4">

    <div class="w-full md:w-3/5 flex flex-col space-y-4 md:pr-8 lg:pr-10">

      <a class="flex items-center group" href="https://aitoday.skyai.vn/">
        <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" fill="currentColor" class="mr-2 group-hover:animate-spin" viewBox="0 0 16 16">
          <path d="M8 11a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/>
          <path d="M13.997 5.17a5 5 0 0 0-8.101-4.09A5 5 0 0 0 1.28 9.342a5 5 0 0 0 8.336 5.109 3.5 3.5 0 0 0 5.201-4.065 3.001 3.001 0 0 0-.822-5.216zm-1-.034a1 1 0 0 0 .668.977 2.001 2.001 0 0 1 .547 3.478 1 1 0 0 0-.341 1.113 2.5 2.5 0 0 1-3.715 2.905 1 1 0 0 0-1.262.152 4 4 0 0 1-6.67-4.087 1 1 0 0 0-.2-1 4 4 0 0 1 3.693-6.61 1 1 0 0 0 .8-.2 4 4 0 0 1 6.48 3.273z"/>
        </svg>
        
        <span class="text-2xl font-semibold uppercase">AI Today - SkyAI</span>
      </a>

      <p class="font-semibold">
        Theo dÃµi cÃ¡c tiáº¿n bá»™ má»›i nháº¥t vá» TrÃ­ tuá»‡ nhÃ¢n táº¡o.<br>Trá»±c tiáº¿p tá»« cÃ¡c nhÃ  phÃ¡t hÃ nh AI trÃªn tháº¿ giá»›i.
      </p>

      <p>Äem trÃ­ tuá»‡ nhÃ¢n táº¡o Ä‘áº¿n má»i ngÆ°á»i dÃ¢n, doanh nghiá»‡p Viá»‡t, gÃ³p pháº§n giÃºp Viá»‡t Nam phÃ¡t triá»ƒn máº¡nh máº½ trong ká»· nguyÃªn sá»‘. Ná»™i dung Ä‘Æ°á»£c cáº­p nháº­t tá»± Ä‘á»™ng báº±ng mÃ¡y.</p>

    </div>

    <div class="self-center flex flex-col w-full md:w-2/5">

<ul id="social-media" class="flex items-center space-x-4">
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.facebook.com/luat.skyai/" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 16 16">
				<path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.twitter.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M12.6.75h2.454l-5.36 6.142L16 15.25h-4.937l-3.867-5.07-4.425 5.07H.316l5.733-6.57L0 .75h5.063l3.495 4.633L12.601.75Zm-.86 13.028h1.36L4.323 2.145H2.865l8.875 11.633Z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.github.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.instagram.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.917 3.917 0 0 0-1.417.923A3.927 3.927 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.916 3.916 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.926 3.926 0 0 0-.923-1.417A3.911 3.911 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0h.003zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599.28.28.453.546.598.92.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.47 2.47 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.478 2.478 0 0 1-.92-.598 2.48 2.48 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233 0-2.136.008-2.388.046-3.231.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92.28-.28.546-.453.92-.598.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045v.002zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92zm-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217zm0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334z"></path>
			</svg>
		</a>
	</li>
</ul>

    </div>
    
  </div>

  <div class="my-8">
    <ul class="flex items-center space-x-4">
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/">Home</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="">About</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/tags/">Tags</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/tags/">Taxonomy</a></li>
      
    </ul>
  </div>

  <div class="border-t pt-4">

    <p class="text-sm">Copyright Â© 2025. All rights reserved. <a href="https://home.skyai.vn/" target="_blank" rel="noopener">SkyAI.vn</a></p>

  </div>
  
</div>
</footer>
      <script defer src="/js/insertoggle.474f9b0e08021c6519cff4e46df14ccf148285b2d3a23d6321d6e10f25c291fb.js" integrity="sha256-R0&#43;bDggCHGUZz/TkbfFMzxSChbLToj1jIdbhDyXCkfs=" crossorigin="anonymous"></script>
</body>
</html>