<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>C√°c th·ªß thu·∫≠t t·ª´ OpenAI gpt-oss m√† B·∫†N ü´µ c√≥ th·ªÉ s·ª≠ d·ª•ng v·ªõi transformers | AI Today - SkyAI</title>

<meta name="description" content="">
      <link rel="stylesheet" href="/css/main.min.584aed796ba9d245f84009d57da9662e9c02059611972780263728b442fc25ff.css" integrity="sha256-WErteWup0kX4QAnVfalmLpwCBZYRlyeAJjcotEL8Jf8=" crossorigin="anonymous">

<link rel="icon" type="image/svg+xml" href="https://aitoday.skyai.vn/favicon.svg"> 
<link rel="icon" type="image/x-icon" href="https://aitoday.skyai.vn/favicon.ico"> 
<link rel="icon" type="image/png" sizes="16x16" href="https://aitoday.skyai.vn/favicon.png"> 
<link rel="icon" type="image/png" sizes="32x32" href="https://aitoday.skyai.vn/favicon-32.png"> 
<link rel="icon" type="image/png" sizes="64x64" href="https://aitoday.skyai.vn/favicon-64.png"> 

</head>
<body>
  <header class="py-6 border-b"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8 flex flex-col">
	<div class="flex items-center">
		<div class="flex items-center">
			<button class="flex items-center space-x-2 rounded-full border py-1 pr-[5px] pl-3 group bg-zinc-100 hover:bg-zinc-200 toggle-button" data-target="menu-bar">
				<svg width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
					<path fill-rule="evenodd" d="M2.5 12a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5zm0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5zm0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5z"></path>
				</svg>
				<span class="bg-blue-500 fill-white rounded-full p-1.5">
					<svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="group-hover:animate-spin bi bi-egg-fried fill-white" viewBox="0 0 16 16">
						<path d="M8 11a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"></path>
						<path d="M13.997 5.17a5 5 0 0 0-8.101-4.09A5 5 0 0 0 1.28 9.342a5 5 0 0 0 8.336 5.109 3.5 3.5 0 0 0 5.201-4.065 3.001 3.001 0 0 0-.822-5.216zm-1-.034a1 1 0 0 0 .668.977 2.001 2.001 0 0 1 .547 3.478 1 1 0 0 0-.341 1.113 2.5 2.5 0 0 1-3.715 2.905 1 1 0 0 0-1.262.152 4 4 0 0 1-6.67-4.087 1 1 0 0 0-.2-1 4 4 0 0 1 3.693-6.61 1 1 0 0 0 .8-.2 4 4 0 0 1 6.48 3.273z"></path>
					</svg>
				</span>
			</button>
			<div class="relative rounded-full py-1.5 px-6 bg-zinc-100 hover:bg-zinc-200 text-xl font-bold uppercase mx-2">
				<h2><a class="before:content-[''] before:z-10 before:top-0 before:right-0 before:left-0 before:bottom-0 before:absolute before:pointer-events-auto" href="https://aitoday.skyai.vn/">AI Today - SkyAI</a></h2>
			</div>
		</div>
		<div class="flex items-center ml-auto">
			<button class="flex items-center rounded-full p-3 bg-zinc-100 hover:bg-zinc-200 toggle-button" data-target="search-bar">
				<svg width="18" height="18" fill="currentColor" viewBox="0 0 16 16">
					<path id="path1" class="transition-all ease-linear" d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path>
					<path id="path2" class="transition-all ease-linear hidden" d="M2.146 2.854a.5.5 0 1 1 .708-.708L8 7.293l5.146-5.147a.5.5 0 0 1 .708.708L8.707 8l5.147 5.146a.5.5 0 0 1-.708.708L8 8.707l-5.146 5.147a.5.5 0 0 1-.708-.708L7.293 8 2.146 2.854Z"></path>
				</svg>
			</button>
		</div>
	</div>
  <nav id="menu-bar" class="block mt-3 close">
    <ul class="flex items-center flex flex-nowrap whitespace-nowrap overflow-x-auto space-x-4">
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/">Home</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200">About</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/tags/">Tags</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/tags/">Taxonomy</a>
    </li>
    </ul>
  </nav>
<div id="search-bar" class="block mt-3 close">
	<form id="search" class="flex items-stretch">
		<input class="w-full block px-6 py-2 rounded-l-full focus:outline-none border border-zinc-200" type="text" placeholder="Search...">
		<button class="flex items-center px-7 py-2.5 rounded-r-full border border-zinc-200 ">
			<svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="group-hover:animate-pulse" viewBox="0 0 16 16">
				<path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path>
			</svg>
		</button>
	</form>
</div>
</div></header>
  <main class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8"><div id="breadcrumb" class="max-w-7xl mx-auto py-8">
	<ul class="flex space-x-4 text-sm text-zinc-500">
		<li class="after:content-['‚ùØ'] after:ml-4 after:opacity-30 last:after:content-none uppercase">
			<a href="https://aitoday.skyai.vn/">AI Today - SkyAI</a>
		</li>
		<li class="after:content-['‚ùØ'] after:ml-4 after:opacity-30 last:after:content-none uppercase">
			<a href="https://aitoday.skyai.vn/posts/">Posts</a>
		</li>
	</ul>
</div><div class="grid grid-cols-1 md:grid-cols-3 gap-6 mb-14">

  <article class="md:col-span-2 prose lg:prose-lg">

    <header class="not-prose">
      
      <h1 id="title" class="text-4xl font-bold leading-normal">C√°c th·ªß thu·∫≠t t·ª´ OpenAI gpt-oss m√† B·∫†N ü´µ c√≥ th·ªÉ s·ª≠ d·ª•ng v·ªõi transformers</h1>

      <div id="lead" class="my-6">

        <p class="font-bold"> </p>

      </div>
      
      <div id="writer" class="flex items-center space-x-4"><ul class="flex items-center space-x-4 flex-nowrap whitespace-nowrap overflow-x-auto">
          
          <li class="font-semibold my-2"></li>
          
          <li class="before:content-['‚Ä¢'] before:mr-2 before:opacity-50 my-2"><time datetime="2025-09-27T00:00:00&#43;00:00">September 27, 2025</time>
          </li>

          <li class="before:content-['‚Ä¢'] before:mr-2 before:opacity-50 my-2">
            17 min read
          </li>

        </ul>

      </div>
      
    </header>

    <figure id="featureimage" class="rounded-xl aspect-video">

          <img class="rounded-lg" src="https://huggingface.co/blog/assets/faster-transformers/thumbnail.png" alt="C√°c th·ªß thu·∫≠t t·ª´ OpenAI gpt-oss m√† B·∫†N ü´µ c√≥ th·ªÉ s·ª≠ d·ª•ng v·ªõi transformers">
        <figcaption class="text-center italic text-xs"></figcaption>

    </figure>

    <div id="content" class="mb-14">
      <h1 id="th·ªß-thu·∫≠t-t·ª´-openai-gpt-oss-m·ªçi-ng∆∞·ªùi--c√≥-th·ªÉ-s·ª≠-d·ª•ng-v·ªõi-transformers">Th·ªß thu·∫≠t t·ª´ OpenAI gpt-oss M·ªåI NG∆Ø·ªúI ü´µ c√≥ th·ªÉ s·ª≠ d·ª•ng v·ªõi transformers</h1>
<p>OpenAI g·∫ßn ƒë√¢y ƒë√£ ph√°t h√†nh <a href="https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4">b·ªô m√¥ h√¨nh GPT-OSS</a>. C√°c m√¥ h√¨nh n√†y c√≥ m·ªôt s·ªë k·ªπ thu·∫≠t m·ªõi nh∆∞ l∆∞·ª£ng t·ª≠ h√≥a MXFP4, kernel hi·ªáu qu·∫£, ƒë·ªãnh d·∫°ng chat ho√†n to√†n m·ªõi v√† h∆°n th·∫ø n·ªØa. ƒê·ªÉ cho ph√©p ph√°t h√†nh gpt-oss th√¥ng qua <code>transformers</code>, ch√∫ng t√¥i ƒë√£ n√¢ng c·∫•p ƒë√°ng k·ªÉ <a href="https://github.com/huggingface/transformers/">th∆∞ vi·ªán</a>. C√°c b·∫£n c·∫≠p nh·∫≠t gi√∫p vi·ªác <strong>t·∫£i</strong>, <strong>ch·∫°y</strong> v√† <strong>tinh ch·ªânh</strong> c√°c m√¥ h√¨nh tr·ªü n√™n r·∫•t hi·ªáu qu·∫£.</p>
<p>Trong b√†i ƒëƒÉng tr√™n blog n√†y, ch√∫ng ta s·∫Ω n√≥i v·ªÅ t·∫•t c·∫£ c√°c n√¢ng c·∫•p m·ªôt c√°ch chuy√™n s√¢u v√† c√°ch ch√∫ng tr·ªü th√†nh m·ªôt ph·∫ßn c·ªßa b·ªô c√¥ng c·ª• transformers ƒë·ªÉ c√°c m√¥ h√¨nh kh√°c (hi·ªán t·∫°i v√† t∆∞∆°ng lai) c√≥ th·ªÉ h∆∞·ªüng l·ª£i t·ª´ ch√∫ng. Cung c·∫•p c√°c tri·ªÉn khai r√µ r√†ng v·ªÅ c√°c ph∆∞∆°ng ph√°p m·ªõi trong transformers c≈©ng cho ph√©p c·ªông ƒë·ªìng nhanh ch√≥ng hi·ªÉu v√† √°p d·ª•ng ch√∫ng. C√°c framework nh∆∞ <a href="https://github.com/ml-explore/mlx-lm/pull/354"><code>MLX</code></a>, <a href="https://github.com/ggml-org/llama.cpp/discussions/15396"><code>llama.cpp</code></a> ho·∫∑c <a href="https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html"><code>vLLM</code></a> c√≥ th·ªÉ s·ª≠ d·ª•ng m√£ transformers l√†m t√†i li·ªáu tham kh·∫£o ƒë·ªÉ x√¢y d·ª±ng c√°c tri·ªÉn khai c·ªßa ri√™ng h·ªç.</p>
<p>ƒê·ªëi v·ªõi b·∫£n ph√°t h√†nh n√†y, ch√∫ng t√¥i ƒë√£ l√†m vi·ªác tr√™n:</p>
<ul>
<li><a href="#zero-build-kernels-downloadable-from-the-hub">Kernel kh√¥ng c·∫ßn build, c√≥ th·ªÉ t·∫£i xu·ªëng t·ª´ Hub</a></li>
<li><a href="#mxfp4-quantization">L∆∞·ª£ng t·ª≠ h√≥a MXFP4</a></li>
<li><a href="#tensor-parallelism">Song song h√≥a Tensor</a></li>
<li><a href="#expert-parallelism">Song song h√≥a Expert</a></li>
<li><a href="#dynamic-sliding-window-layer--cache">L·ªõp C·ª≠a s·ªï Tr∆∞·ª£t ƒê·ªông &amp; B·ªô nh·ªõ Cache</a></li>
<li><a href="#continuous-batching--paged-attention">Batching li√™n t·ª•c &amp; Ch√∫ √Ω theo Trang</a></li>
<li><a href="#load-larger-models-faster">T·∫£i c√°c m√¥ h√¨nh l·ªõn h∆°n nhanh h∆°n</a></li>
</ul>
<blockquote>
<p>Ph·∫ßn hay nh·∫•t: H·∫ßu h·∫øt c√°c t√≠nh nƒÉng n√†y s·∫Ω ho·∫°t ƒë·ªông tr√™n t·∫•t c·∫£ c√°c m√¥ h√¨nh ch√≠nh trong <code>transformers</code>!</p></blockquote>
<h2 id="kernel-kh√¥ng-c·∫ßn-build-c√≥-th·ªÉ-t·∫£i-xu·ªëng-t·ª´-hub">Kernel kh√¥ng c·∫ßn build, c√≥ th·ªÉ t·∫£i xu·ªëng t·ª´ Hub</h2>
<p>Kernel l√† m·ªôt ch∆∞∆°ng tr√¨nh <em>chuy√™n d·ª•ng</em>, nh·ªè g·ªçn ch·∫°y tr√™n c√°c b·ªô tƒÉng t·ªëc ƒë·ªÉ th·ª±c hi·ªán c√°c t√°c v·ª• nh∆∞ nh√¢n ma tr·∫≠n, k√≠ch ho·∫°t ho·∫∑c chu·∫©n h√≥a. Trong PyTorch eager, c√°c ho·∫°t ƒë·ªông k√≠ch ho·∫°t c√°c kernel ri√™ng l·∫ª m·ªôt c√°ch tu·∫ßn t·ª±, ƒëi·ªÅu n√†y r·∫•t ƒë∆°n gi·∫£n nh∆∞ng c√≥ th·ªÉ ph√°t sinh th√™m c√°c chuy·ªÉn b·ªô nh·ªõ v√† chi ph√≠ kh·ªüi ch·∫°y. <code>torch.compile</code> c·ªßa PyTorch 2.0 v·ªõi c√°c backend nh∆∞ <code>TorchInductor</code> gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ n√†y b·∫±ng c√°ch t·ª± ƒë·ªông h·ª£p nh·∫•t v√† t·ªëi ∆∞u h√≥a c√°c kernel, mang l·∫°i hi·ªáu su·∫•t tƒÉng t·ª´ <code>2‚Äì10√ó</code>.</p>
<p>Ngo√†i ra, c·ªông ƒë·ªìng ƒë√£ t·∫°o ra c√°c kernel t√πy ch·ªânh cho c√°c k·∫øt h·ª£p ho·∫°t ƒë·ªông th∆∞·ªùng xuy√™n, <em>kh√¥ng ch·ªâ c√°c op PyTorch ri√™ng l·∫ª nh∆∞ matmul</em>. V√≠ d·ª•: Flash Attention ƒë∆∞·ª£c t·∫°o ra ƒë·ªÉ t·ªëi ∆∞u h√≥a kh·ªëi attention quan tr·ªçng x√°c ƒë·ªãnh ki·∫øn tr√∫c transformers v√† c√≥ m·∫∑t trong nhi·ªÅu m√¥ h√¨nh, bao g·ªìm h·∫ßu h·∫øt LLM. B·∫±ng c√°ch k·∫øt h·ª£p c·∫©n th·∫≠n t·∫•t c·∫£ c√°c ho·∫°t ƒë·ªông attention b√™n trong m·ªôt kernel duy nh·∫•t, c√°c chuy·ªÉn b·ªô nh·ªõ ƒë∆∞·ª£c gi·∫£m thi·ªÉu, vi·ªác s·ª≠ d·ª•ng b·ªô nh·ªõ ƒë∆∞·ª£c gi·∫£m v√† c√≥ th·ªÉ ƒë·∫°t ƒë∆∞·ª£c tƒÉng t·ªëc.</p>
<p>V·∫•n ƒë·ªÅ l√† t·∫•t c·∫£ c√°c kernel kh√°c nhau n√†y ƒë·ªÅu c√≥ s·∫µn trong c√°c th∆∞ vi·ªán ri√™ng bi·ªát, ƒëi·ªÅu n√†y t·∫°o ra s·ª± ph√¨nh to v·ªÅ dependency n·∫øu ch√∫ng ƒë∆∞·ª£c th√™m v√†o th∆∞ vi·ªán transformers. H∆°n n·ªØa, c√°c kernel n√†y kh√¥ng ch·ªâ l√† m√£ Python, ch√∫ng bao g·ªìm m√£ cuda c·∫•p th·∫•p, ƒë∆∞·ª£c g·∫Øn l·∫°i v·ªõi C++ v√† ƒë∆∞·ª£c hi·ªÉn th·ªã th√¥ng qua m·ªôt l·ªõp Python. ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† ch√∫ng ph·∫£i ƒë∆∞·ª£c bi√™n d·ªãch trong h·ªá th·ªëng ƒë√≠ch, do ƒë√≥ y√™u c·∫ßu b·∫•t k·ª≥ h·ªá th·ªëng build n√†o ƒë∆∞·ª£c y√™u c·∫ßu b·ªüi m·ªói th∆∞ vi·ªán kernel.</p>
<p><a href="https://huggingface.co/blog/hello-hf-kernels">G√≥i kernels</a> gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ n√†y b·∫±ng c√°ch t·∫£i xu·ªëng c√°c binary ƒë∆∞·ª£c build s·∫µn c·ªßa c√°c kernel ƒë∆∞·ª£c h·ªó tr·ª£ t·ª´ Hub. B·∫°n ch·ªâ c·∫ßn ch·ªâ ra kernel b·∫°n mu·ªën s·ª≠ d·ª•ng v√† <code>kernels</code> s·∫Ω t√¨m ki·∫øm m·ªôt phi√™n b·∫£n t∆∞∆°ng th√≠ch v·ªõi h·ªá th·ªëng c·ªßa b·∫°n v√† t·∫£i xu·ªëng khi s·ª≠ d·ª•ng l·∫ßn ƒë·∫ßu.</p>
<h3 id="kernel-t√πy-ch·ªânh-cho-gpt-oss">Kernel t√πy ch·ªânh cho GPT-OSS</h3>
<p><a href="https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src/transformers/models/gpt_oss/modeling_gpt_oss.py">GPT-OSS</a>, m·ªôt m√¥ h√¨nh Mixture of Experts (MoE), l√† m·ªôt ng∆∞·ªùi d√πng l·ªõn c·ªßa Kernel t·ª´ Hub. N√≥ t·∫≠n d·ª•ng m·ªôt s·ªë kernel t√πy ch·ªânh:</p>
<ol>
<li>Liger RMSNorm, ƒë∆∞·ª£c s·ª≠ d·ª•ng l√†m <a href="https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src/transformers/models/gpt_oss/modeling_gpt_oss.py#L46"><code>@use_kernel_forward_from_hub(&quot;RMSNorm&quot;)</code></a></li>
<li>Kernel Megablocks MoE: <a href="https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src/transformers/models/gpt_oss/modular_gpt_oss.py#L160"><code>@use_kernel_forward_from_hub(&quot;MegaBlocksMoeMLP&quot;)</code></a></li>
<li>Flash Attention 3 v·ªõi <a href="https://huggingface.co/kernels-community/vllm-flash-attn3">h·ªó tr·ª£ cho attention sinks</a>.</li>
<li>Kernel triton MXFP4 (ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p <a href="#mxfp4-in-transformers">sau</a>)</li>
</ol>
<p>H√£y xem hai kernel ƒë·∫ßu ti√™n.</p>
<p>ƒê·∫±ng sau h·∫≠u tr∆∞·ªùng, c√°c decorator (1 v√† 2) ch·ªâ ƒë∆°n gi·∫£n l√† tr·ªè ƒë·∫øn c√°c kernel do c·ªông ƒë·ªìng ƒë√≥ng g√≥p. V√≠ d·ª•: <code>RMSNorm</code> ƒë·∫øn t·ª´ <a href="https://huggingface.co/kernels-community/liger_kernels"><code>liger_kernels</code></a>, trong khi kernel <code>MegaBlocksMoeMLP</code> ƒë·∫øn t·ª´ <a href="https://huggingface.co/kernels-community/megablocks"><code>megablocks</code></a>. T√πy thu·ªôc v√†o thi·∫øt b·ªã c·ªßa b·∫°n (CUDA ho·∫∑c ROCm) v√† b·∫°n ƒëang hu·∫•n luy·ªán hay ch·∫°y suy lu·∫≠n, kernel ph√π h·ª£p s·∫Ω ƒë∆∞·ª£c k√©o v√†o t·ª± ƒë·ªông.</p>
<p>Thi·∫øt k·∫ø n√†y v·ª´a <strong>c·ª• th·ªÉ v·ª´a t·ªïng qu√°t</strong>: c√°c kernel RMSNorm liger ƒë√£ ƒë∆∞·ª£c s·ª≠ d·ª•ng l·∫°i tr√™n nhi·ªÅu m√¥ h√¨nh v√† kernel MoE c≈©ng c√≥ th·ªÉ ƒë∆∞·ª£c √°p d·ª•ng cho MoE trong t∆∞∆°ng lai.</p>
<p>V√¨ <code>kernels</code> k√©o m√£ t·ª´ Hub, b·∫°n ph·∫£i ch·ªçn tham gia t√≠nh nƒÉng n√†y b·∫±ng c√°ch chuy·ªÉn <code>use_kernels=True</code> trong kh·ªüi t·∫°o m√¥ h√¨nh c·ªßa b·∫°n, nh∆∞ ƒë∆∞·ª£c hi·ªÉn th·ªã b√™n d∆∞·ªõi. Ch√∫ng t√¥i b·∫≠t ghi log <code>INFO</code> trong v√≠ d·ª• ƒë·ªÉ b·∫°n c√≥ th·ªÉ d·ªÖ d√†ng x√°c minh r·∫±ng c√°c kernel c√≥ th·ªÉ t·∫£i xu·ªëng ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng.</p>
<blockquote>
<p>C√°c kernel n√†y kh√¥ng t∆∞∆°ng th√≠ch v·ªõi <code>mxfp4</code>, v√¨ v·∫≠y suy lu·∫≠n s·∫Ω x·∫£y ra trong <code>bfloat16</code> n·∫øu b·∫°n s·ª≠ d·ª•ng ch√∫ng. Vui l√≤ng benchmark h·ªá th·ªëng c·ªßa b·∫°n ƒë·ªÉ c√≥ s·ª± k·∫øt h·ª£p t·ªët nh·∫•t v·ªÅ b·ªô nh·ªõ v√† th√¥ng l∆∞·ª£ng ph√π h·ª£p v·ªõi d·ª± √°n c·ªßa b·∫°n!</p></blockquote>
<p>python
from transformers import AutoTokenizer, AutoModelForCausalLM</p>
<p>import logging
logging.basicConfig(level=logging.INFO)</p>
<p>model_id = &ldquo;openai/gpt-oss-20b&rdquo;
tokenizer = AutoTokenizer.from_pretrained(model_id)</p>
<p>model = AutoModelForCausalLM.from_pretrained(
model_id,
dtype=&ldquo;auto&rdquo;,
device_map=&ldquo;auto&rdquo;,
use_kernels=True,
)</p>
<p>Ch·∫°y m·ªôt th·∫ø h·ªá nhanh ch√≥ng t·∫°o ra c√°c th√¥ng b√°o log nh∆∞</p>
<p>shell
INFO:root:Using layer <code>LigerRMSNorm</code> from repo <code>kernels-community/liger_kernels</code>
INFO:root:Using layer <code>MegaBlocksMoeMLP</code> from repo <code>kernels-community/megablocks</code></p>
<p><strong>H√¨nh 1</strong> cho th·∫•y r·∫±ng, trong h·ªá th·ªëng ch√∫ng t√¥i ƒë√£ th·ª≠ nghi·ªám, c√°c kernel n√†y ho·∫°t ƒë·ªông t·ªët nh·∫•t cho c√°c batch size l·ªõn h∆°n. Ch√∫ng t√¥i lu√¥n khuy√™n b·∫°n n√™n benchmark b·∫•t k·ª≥ thay ƒë·ªïi n√†o li√™n quan ƒë·∫øn hi·ªáu su·∫•t c√†ng g·∫ßn v·ªõi ƒëi·ªÅu ki·ªán s·∫£n xu·∫•t c·ªßa b·∫°n c√†ng t·ªët.</p>
<!-- raw HTML omitted -->
<blockquote>
<p>B·∫°n c√≥ th·ªÉ kh√°m ph√° v√† ch∆°i v·ªõi script benchmark <a href="https://huggingface.co/datasets/ariG23498/faster-transformers-scripts/blob/main/benchmark-kernels-with-without.py">t·∫°i ƒë√¢y</a></p></blockquote>
<h3 id="flash-attention-3">Flash Attention 3</h3>
<p>C√°c m√¥ h√¨nh OpenAI gpt-oss s·ª≠ d·ª•ng <em>attention sinks</em>, gi√∫p c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng v√† t·∫°o ƒëi·ªÅu ki·ªán thu·∫≠n l·ª£i cho vi·ªác s·ª≠ d·ª•ng c√°c ng·ªØ c·∫£nh d√†i h∆°n. Nh√≥m vLLM ƒë√£ th√™m t√≠nh nƒÉng n√†y v√†o phi√™n b·∫£n Flash Attention m·ªõi nh·∫•t (Flash Attention 3) v√† kernel t√πy ch·ªânh k·∫øt qu·∫£ c√≥ s·∫µn <a href="https://huggingface.co/kernels-community/vllm-flash-attn3">tr√™n Hub</a>. Hi·ªán t·∫°i, kernel n√†y t∆∞∆°ng th√≠ch v·ªõi ki·∫øn tr√∫c Hopper. N·∫øu b·∫°n c√≥ m·ªôt ki·∫øn tr√∫c, ƒë√¢y l√† c√°ch ƒë·ªÉ b·∫≠t n√≥:</p>
<p>diff
model = AutoModelForCausalLM.from_pretrained(
model_id,
dtype=&ldquo;auto&rdquo;,
device_map=&ldquo;auto&rdquo;,</p>
<ul>
<li>
<h1 id="flash-attention-with-sinks">Flash Attention with Sinks</h1>
</li>
<li>attn_implementation=&ldquo;kernels-community/vllm-flash-attn3&rdquo;,
)</li>
</ul>
<h2 id="l∆∞·ª£ng-t·ª≠-h√≥a-mxfp4">L∆∞·ª£ng t·ª≠ h√≥a MXFP4</h2>
<p>C√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn r·∫•t t·ªën b·ªô nh·ªõ. L∆∞·ª£ng t·ª≠ h√≥a l√†m gi·∫£m footprint b·ªô nh·ªõ b·∫±ng c√°ch l∆∞u tr·ªØ weights (v√† ƒë√¥i khi l√† activations) ·ªü c√°c ƒë·ªãnh d·∫°ng c√≥ ƒë·ªô ch√≠nh x√°c th·∫•p h∆°n. ƒê·ªÉ tham kh·∫£o, <code>FP32</code> s·ª≠ d·ª•ng 32 bit cho m·ªói s·ªë v√† <code>BF16</code> s·ª≠ d·ª•ng 16. B·∫±ng c√°ch gi·∫£m ƒë·ªô r·ªông bit, ch√∫ng ta ƒë√°nh ƒë·ªïi m·ªôt s·ªë ƒë·ªô ch√≠nh x√°c ƒë·ªÉ c√≥ c√°c m√¥ h√¨nh nh·ªè h∆°n v√† di chuy·ªÉn b·ªô nh·ªõ nhanh h∆°n.</p>
<p>N·∫øu b·∫°n mu·ªën c√≥ m·ªôt primer tr·ª±c quan v·ªÅ c√°c trade-off l∆∞·ª£ng t·ª≠ h√≥a, b√†i vi·∫øt c·ªßa <a href="https://huggingface.co/MaartenGr">Maarten Grootendorst</a> l√† tuy·ªát v·ªùi: <a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization"><em>H∆∞·ªõng d·∫´n tr·ª±c quan v·ªÅ l∆∞·ª£ng t·ª≠ h√≥a</em></a>.</p>
<h3 id="mxfp4-l√†-g√¨">MXFP4 l√† g√¨</h3>
<!-- raw HTML omitted -->
<p><code>MXFP4</code> l√† m·ªôt ƒë·ªãnh d·∫°ng d·∫•u ph·∫©y ƒë·ªông 4-bit v·ªõi b·ªë c·ª•c E2M1: 1 bit d·∫•u, 2 bit s·ªë m≈© v√† 1 bit ph·∫ßn ƒë·ªãnh tr·ªã, nh∆∞ ƒë∆∞·ª£c hi·ªÉn th·ªã trong <strong>H√¨nh 2</strong>. B·∫£n th√¢n E2M1 r·∫•t th√¥. MXFP4 b√π ƒë·∫Øp b·∫±ng <strong>scaling theo kh·ªëi</strong>:</p>
<ul>
<li>C√°c vector ƒë∆∞·ª£c nh√≥m th√†nh c√°c kh·ªëi g·ªìm 32 ph·∫ßn t·ª≠.</li>
<li>M·ªói kh·ªëi l∆∞u tr·ªØ m·ªôt scale ƒë∆∞·ª£c chia s·∫ª kh√¥i ph·ª•c ph·∫°m vi ƒë·ªông khi dequantizing.</li>
<li>B√™n trong m·ªói kh·ªëi, c√°c gi√° tr·ªã 4-bit ƒë·∫°i di·ªán cho c√°c s·ªë li√™n quan ƒë·∫øn scale ƒë√≥.</li>
</ul>
<p>L∆∞·ª£c ƒë·ªì theo kh·ªëi n√†y cho ph√©p <code>MXFP4</code> gi·ªØ ph·∫°m vi trong khi s·ª≠ d·ª•ng r·∫•t √≠t bit. Trong th·ª±c t·∫ø, GPT-OSS 20B ph√π h·ª£p v·ªõi kho·∫£ng <code>16 GB</code> VRAM v√† GPT-OSS 120B ph√π h·ª£p v·ªõi kho·∫£ng <code>80 GB</code> khi <code>MXFP4</code> ho·∫°t ƒë·ªông, ƒë√≥ l√† s·ª± kh√°c bi·ªát gi·ªØa &ldquo;kh√¥ng th·ªÉ t·∫£i&rdquo; v√† &ldquo;c√≥ th·ªÉ ch·∫°y tr√™n m·ªôt GPU duy nh·∫•t&rdquo;. V·∫•n ƒë·ªÅ l√† c√°c ph√©p nh√¢n ma tr·∫≠n hi·ªán ph·∫£i t√¥n tr·ªçng c√°c scale kh·ªëi. L√†m ƒëi·ªÅu n√†y m·ªôt c√°ch hi·ªáu qu·∫£ ·ªü quy m√¥ l·ªõn ƒë√≤i h·ªèi c√°c kernel chuy√™n d·ª•ng.</p>
<h3 id="mxfp4-trong-transformers">MXFP4 trong <code>transformers</code></h3>
<p><code>transformers</code> hi·ªán bao g·ªìm h·ªó tr·ª£ g·ªëc cho MXFP4, t·∫≠n d·ª•ng c√°c kernel <code>triton</code> (MXFP4) ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a ƒë·ªÉ tƒÉng c∆∞·ªùng hi·ªáu su·∫•t. ƒêi·ªÅu n√†y x√¢y d·ª±ng tr√™n ph√¢n ph·ªëi kernel do c·ªông ƒë·ªìng ƒëi·ªÅu khi·ªÉn <a href="#zero-build-kernels-downloadable-from-the-hub">ƒë∆∞·ª£c th·∫£o lu·∫≠n tr∆∞·ªõc ƒë√≥</a>, s·ª≠ d·ª•ng c√°c kernel ƒë∆∞·ª£c bi√™n d·ªãch tr∆∞·ªõc t·ª´ Hub ƒë·ªÉ ƒë∆°n gi·∫£n h√≥a vi·ªác tri·ªÉn khai.</p>
<p>Chi ti·∫øt tri·ªÉn khai ch√≠nh:</p>
<ul>
<li>Logic quantizer: ƒê∆∞·ª£c t√¨m th·∫•y trong <a href="https://github.com/huggingface/transformers/blob/0997c2f2ab08c32c8e2f90aaad06e29a7108535b/src/transformers/quantizers/quantizer_mxfp4.py">t·ªáp quantizer MXFP4</a>, ƒëi·ªÅu n√†y x·ª≠ l√Ω qu√° tr√¨nh l∆∞·ª£ng t·ª≠ h√≥a c·ªët l√µi cho MXFP4.</li>
<li>Integration hooks: <a href="https://github.com/huggingface/transformers/blob/0997c2f2ab08c32c8e2f90aaad06e29a7108535b/src/transformers/integrations/mxfp4.py">T·ªáp t√≠ch h·ª£p MXFP4</a> cho ph√©p s·ª≠ d·ª•ng MXFP4 li·ªÅn m·∫°ch trong framework transformers.</li>
</ul>
<p>ƒê·ªÉ ki·ªÉm tra xem m·ªôt m√¥ h√¨nh c√≥ h·ªó tr·ª£ <code>MXFP4</code> hay kh√¥ng, h√£y ki·ªÉm tra c·∫•u h√¨nh c·ªßa n√≥:</p>
<p>py
from transformers import GptOssConfig</p>
<p>model_id = &ldquo;openai/gpt-oss-120b&rdquo;
cfg = GptOssConfig.from_pretrained(model_id)
print(cfg.quantization_config)</p>
<h1 id="v√≠-d·ª•-ƒë·∫ßu-ra">V√≠ d·ª• ƒë·∫ßu ra:</h1>
<h1 id="heading">{</h1>
<h1 id="modules_to_not_convert-">&lsquo;modules_to_not_convert&rsquo;: [</h1>
<h1 id="modellayersself_attn">&lsquo;model.layers.*.self_attn&rsquo;,</h1>
<h1 id="modellayersmlprouter">&lsquo;model.layers.*.mlp.router&rsquo;,</h1>
<h1 id="modelembed_tokens">&lsquo;model.embed_tokens&rsquo;,</h1>
<h1 id="lm_head">&rsquo;lm_head&rsquo;</h1>
<h1 id="heading-1">],</h1>
<h1 id="quant_method-mxfp4">&lsquo;quant_method&rsquo;: &lsquo;mxfp4&rsquo;</h1>
<h1 id="heading-2">}</h1>
<p>N·∫øu c√≥ <code>'quant_method': 'mxfp4'</code>, m√¥ h√¨nh s·∫Ω t·ª± ƒë·ªông s·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n MXFP4 v·ªõi kernel Triton khi ƒë∆∞·ª£c h·ªó tr·ª£.</p>
<blockquote>
<p>Nh·ªù <a href="https://github.com/huggingface/transformers/pull/40176">pull request</a> n√†y, b·∫°n c√≥ th·ªÉ tinh ch·ªânh c√°c m√¥ h√¨nh gpt-oss v√† l∆∞u ch√∫ng tr·ª±c ti·∫øp v√†o Hub ·ªü ƒë·ªãnh d·∫°ng MXFP4, h·ª£p l√Ω h√≥a vi·ªác tri·ªÉn khai v·ªõi hi·ªáu su·∫•t ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a.</p></blockquote>
<h3 id="y√™u-c·∫ßu-v√†-d·ª±-ph√≤ng">Y√™u c·∫ßu v√† d·ª± ph√≤ng</h3>
<p>ƒê·ªÉ ch·∫°y <code>MXFP4</code> tr√™n GPU, b·∫°n c·∫ßn:</p>
<ol>
<li><code>accelerate</code>, <code>kernels</code> v√† <code>triton&gt;=3.4</code> ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t. L∆∞u √Ω r·∫±ng <code>Pytorch 2.8</code> ƒë√£ ƒëi k√®m v·ªõi <code>triton 3.4</code>, v√¨ v·∫≠y b·∫°n ch·ªâ c·∫ßn c√†i ƒë·∫∑t th·ªß c√¥ng triton n·∫øu s·ª≠ d·ª•ng <code>Pytorch 2.7</code>.</li>
<li>GPU NVIDIA v·ªõi compute capability <code>‚â• 7.5</code>. ƒêi·ªÅu n√†y quay tr·ªü l·∫°i Tesla, v√¨ v·∫≠y b·∫°n c√≥ th·ªÉ ch·∫°y <code>gpt-oss-20b</code> tr√™n c√°c t·∫ßng mi·ªÖn ph√≠ c·ªßa Google Colab v√† Kaggle, v√† tr√™n nhi·ªÅu GPU ti√™u d√πng.</li>
</ol>
<p>N·∫øu c√°c r√†ng bu·ªôc n√†y kh√¥ng ƒë∆∞·ª£c ƒë√°p ·ª©ng, <code>transformers</code> s·∫Ω quay tr·ªü l·∫°i ƒë∆∞·ªùng d·∫´n c√≥ ƒë·ªô ch√≠nh x√°c cao h∆°n (<code>bfloat16</code> ƒë∆∞·ª£c s·ª≠ d·ª•ng theo m·∫∑c ƒë·ªãnh), y√™u c·∫ßu kho·∫£ng 4 l·∫ßn b·ªô nh·ªõ c·ªßa MXFP4.</p>
<p><a href="https://huggingface.co/datasets/ariG23498/faster-transformers-scripts/blob/main/memory-requirements-quantized-vs-dequantized.py">Snippet</a> t·∫£i GPT-OSS hai l·∫ßn tr√™n CUDA: m·ªôt l·∫ßn v·ªõi <code>Mxfp4Config(dequantize=True)</code> (t·ªën nhi·ªÅu b·ªô nh·ªõ) v√† m·ªôt l·∫ßn trong ƒë∆∞·ªùng d·∫´n l∆∞·ª£ng t·ª≠ h√≥a m·∫∑c ƒë·ªãnh (hi·ªáu qu·∫£ b·ªô nh·ªõ). <strong>H√¨nh 3</strong> hi·ªÉn th·ªã l∆∞·ª£ng VRAM ƒë∆∞·ª£c s·ª≠ d·ª•ng sau m·ªói l·∫ßn t·∫£i ƒë·ªÉ b·∫°n c√≥ th·ªÉ h√¨nh dung c√°c kho·∫£n ti·∫øt ki·ªám.</p>
<!-- raw HTML omitted -->
<h3 id="kernel-cho-mxfp4">Kernel cho MXFP4</h3>
<p><code>MXFP4</code> hi·ªáu qu·∫£ y√™u c·∫ßu c√°c kernel hi·ªÉu c√°c kh·ªëi 32 ph·∫ßn t·ª≠ v√† scale c·ªßa ch√∫ng trong qu√° tr√¨nh GEMM v√† fused ops. ƒê√¢y l√† n∆°i <strong>Kernel t·ª´ Hub</strong> xu·∫•t hi·ªán tr·ªü l·∫°i. <code>transformers</code> t·ª± ƒë·ªông k√©o v√†o kernel Triton nh·∫≠n bi·∫øt <code>MXFP4</code> t·ª´ kho l∆∞u tr·ªØ c·ªông ƒë·ªìng khi b·∫°n t·∫£i m·ªôt m√¥ h√¨nh c·∫ßn ch√∫ng. Kho l∆∞u tr·ªØ s·∫Ω xu·∫•t hi·ªán trong b·ªô nh·ªõ cache c·ª•c b·ªô c·ªßa b·∫°n v√† s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng trong qu√° tr√¨nh chuy·ªÉn ti·∫øp. ƒê·ªëi v·ªõi c√°c kernel <code>MXFP4</code>, ng∆∞·ªùi ta kh√¥ng c·∫ßn s·ª≠ d·ª•ng tham s·ªë <code>use_kernels=True</code> nh∆∞ tr∆∞·ªõc ƒë√¢y, n√≥ ƒë∆∞·ª£c ƒë·∫∑t th√†nh m·∫∑c ƒë·ªãnh trong <code>transformers</code>.</p>
<p>Ki·ªÉm tra nhanh ch√≥ng v·ªõi CLI b·ªô nh·ªõ cache Hugging Face, sau khi ch·∫°y <code>gpt-oss-20b</code> tr√™n m·ªôt GPU t∆∞∆°ng th√≠ch v·ªõi kernel MXFP4 triton:</p>
<p>shell
hf cache scan</p>
<p>ƒê·∫ßu ra m·∫´u:</p>
<p>shell
REPO ID                          REPO TYPE SIZE ON DISK</p>
<hr>
<p>kernels-community/triton_kernels model           536.2K
openai/gpt-oss-20b               model            13.8G</p>
<p>ƒêi·ªÅu n√†y ch·ªâ ra r·∫±ng c√°c kernel MXFP4 ƒë√£ ƒë∆∞·ª£c t√¨m n·∫°p v√† c√≥ s·∫µn ƒë·ªÉ th·ª±c thi.</p>
<p>H√£y ch·∫°y m·ªôt s·ªë benchmark v√† xem kernel MXFP4 ho·∫°t ƒë·ªông t·ªët nh∆∞ th·∫ø n√†o. Trong <strong>H√¨nh 4</strong>, ch√∫ng ta th·∫•y r·∫±ng c√°c kernel <code>MXFP4</code> th·∫≠m ch√≠ c√≤n t·ªët h∆°n c√°c kernel MoE v√† RMSNorm t√πy ch·ªânh cho c√°c batch l·ªõn h∆°n.</p>
<!-- raw HTML omitted -->
<blockquote>
<p>B·∫°n c√≥ th·ªÉ kh√°m ph√° v√† ch∆°i v·ªõi script benchmark <a href="https://huggingface.co/datasets/ariG23498/faster-transformers-scripts/blob/main/benchmark-mxfp4-kernels.py">t·∫°i ƒë√¢y</a></p></blockquote>
<h2 id="song-song-h√≥a-tensor">Song song h√≥a Tensor</h2>
<!-- raw HTML omitted -->
<p>Song song h√≥a Tensor (TP) chia <strong>c√°c tensor b√™n trong m·ªôt l·ªõp</strong> tr√™n nhi·ªÅu GPU (nh∆∞ ƒë∆∞·ª£c hi·ªÉn th·ªã trong <strong>H√¨nh 5</strong>). M·ªói GPU nh√¢n shard c·ªßa n√≥ song song, v√† sau ƒë√≥ c√°c k·∫øt qu·∫£ m·ªôt ph·∫ßn ƒë∆∞·ª£c thu th·∫≠p b·∫±ng c√°ch s·ª≠ d·ª•ng c√°c ho·∫°t ƒë·ªông all-gather ho·∫∑c all-reduce.
ƒêi·ªÅu n√†y l√†m gi·∫£m b·ªô nh·ªõ tr√™n m·ªói GPU v√† gi·ªØ cho t·∫•t c·∫£ c√°c GPU ho·∫°t ƒë·ªông tr√™n <strong>c√πng m·ªôt l·ªõp</strong>, ƒëi·ªÅu n√†y c·∫£i thi·ªán th√¥ng l∆∞·ª£ng khi ƒë·ªô d√†i chu·ªói ho·∫∑c batch size tƒÉng l√™n. TP t·ªën nhi·ªÅu giao ti·∫øp v√† th∆∞·ªùng ho·∫°t ƒë·ªông t·ªët nh·∫•t tr√™n <strong>m·ªôt m√°y duy nh·∫•t v·ªõi c√°c li√™n k·∫øt intra-node nhanh</strong>.</p>
<h3 id="ƒëi·ªÅu-n√†y-cho-ph√©p-ƒëi·ªÅu-g√¨-trong-transformers">ƒêi·ªÅu n√†y cho ph√©p ƒëi·ªÅu g√¨ trong <code>transformers</code></h3>
<p><code>transformers</code> tri·ªÉn khai TP tr·ª±c ti·∫øp trong <code>from_pretrained</code>. B·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu v·ªõi plan ƒë∆∞·ª£c x√°c ƒë·ªãnh tr∆∞·ªõc:</p>
<p>python</p>
<h1 id="ch·∫°y-v·ªõi-torchrun-nproc-per-node-4-tp_gpt_osspy">ch·∫°y v·ªõi: torchrun &ndash;nproc-per-node 4 tp_gpt_oss.py</h1>
<p>import torch
from transformers import PreTrainedTokenizerFast, GptOssForCausalLM
from transformers.distributed import DistributedConfig</p>
<p>model_id = &ldquo;openai/gpt-oss-120b&rdquo;
tokenizer = PreTrainedTokenizerFast.from_pretrained(model_id)
model = GptOssForCausalLM.from_pretrained(
model_id,
tp_plan=&ldquo;auto&rdquo;, # built in TP support
dtype=&ldquo;auto&rdquo;,
).eval()</p>
<p>messages = [
{&ldquo;role&rdquo;: &ldquo;system&rdquo;, &ldquo;content&rdquo;: &ldquo;Be concise.&rdquo;},
{&ldquo;role&rdquo;: &ldquo;user&rdquo;, &ldquo;content&rdquo;: &ldquo;Explain KV caching briefly.&rdquo;},
]
inputs = tokenizer.apply_chat_template(
messages,
add_generation_prompt=True,
return_tensors=&ldquo;pt&rdquo;,
return_dict=True,
reasoning_effort=&ldquo;low&rdquo;,
).to(model.device)</p>
<p>with torch.inference_mode():
generations = model.generate(**inputs, max_new_tokens=128)</p>
<p>print(tokenizer.decode(generations[0][inputs[&ldquo;input_ids&rdquo;].shape[-1]:]))</p>
<p>N·∫øu b·∫°n kh√¥ng c√≥ c∆° s·ªü h·∫° t·∫ßng ƒë·ªÉ ch·∫°y ƒëo·∫°n m√£ tr√™n, b·∫°n ch·ªâ c·∫ßn t·∫°o m·ªôt process tr√™n GPU c·ªßa ch√∫ng t√¥i b·∫±ng c√°ch s·ª≠ d·ª•ng <a href="https://huggingface.co/docs/huggingface_hub/en/guides/jobs">Hugging Face Jobs</a>!</p>
<p>bash
hf jobs run &ndash;detach &ndash;flavor l4x4 ghcr.io/astral-sh/uv:debian /bin/bash -c <br>
&ldquo;uv venv .venv &ndash;python 3.12 &amp;&amp; <br>
source .venv/bin/activate &amp;&amp; <br>
uv pip install &ndash;upgrade torch numpy transformers accelerate triton kernels &amp;&amp; <br>
wget <a href="https://huggingface.co/datasets/ariG23498/distributed/raw/main/tp_gpt_oss.py">https://huggingface.co/datasets/ariG23498/distributed/raw/main/tp_gpt_oss.py</a> &amp;&amp; <br>
torchrun &ndash;nproc-per-node=4 tp_gpt_oss.py&rdquo;</p>
<blockquote>
<p><a href="https://huggingface.co/docs/huggingface_hub/guides/jobs"><code>hf jobs</code></a> c√≥ s·∫µn cho t·∫•t c·∫£ ng∆∞·ªùi d√πng Hugging Face PRO &amp; Enterprise.</p></blockquote>
<p>B√™n d∆∞·ªõi, <code>tp_plan=&quot;auto&quot;</code> ch·ªçn m·ªôt c√¥ng th·ª©c sharding ƒë∆∞·ª£c x√°c ƒë·ªãnh tr∆∞·ªõc cho m·ªói l·ªõp v√† k·∫øt n·ªëi c√°c <a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=a0:_parallel_programming_crash_course">t·∫≠p h·ª£p</a> c·∫ßn thi·∫øt. B·∫°n c√≥ th·ªÉ ki·ªÉm tra plan ƒëang ho·∫°t ƒë·ªông b·∫±ng <code>print(model._tp_plan)</code> n·∫øu b·∫°n mu·ªën x√°c minh nh·ªØng g√¨ ƒëang ƒë∆∞·ª£c sharded.</p>
<h3 id="khi-n√†o-n√™n-s·ª≠-d·ª•ng-tp">Khi n√†o n√™n s·ª≠ d·ª•ng TP</h3>
<p>S·ª≠ d·ª•ng TP khi m√¥ h√¨nh qu√° l·ªõn ƒë·ªëi v·ªõi m·ªôt GPU v√† b·∫°n mu·ªën <strong>t√≠nh to√°n song song</strong>, kh√¥ng ch·ªâ v·ªã tr√≠ b·ªô nh·ªõ. TP c√≥ xu h∆∞·ªõng scale th√¥ng l∆∞·ª£ng v·ªõi nhi·ªÅu GPU h∆°n, ƒë·∫∑c bi·ªát ƒë·ªëi v·ªõi c√°c chu·ªói d√†i ho·∫∑c c√°c batch l·ªõn h∆°n.</p>
<blockquote>
<p>N·∫øu b·∫°n t√≤ m√≤ v·ªÅ c√°ch TP kh√°c v·ªõi <code>device_map=&quot;auto&quot;</code> (v·ªã tr√≠ b·ªô nh·ªõ), <a href="https://stackoverflow.com/questions/78852192/choose-available-gpu-devices-with-device-map">c√¢u tr·∫£ l·ªùi Stack Overflow</a> ng·∫Øn g·ªçn n√†y gi·∫£i th√≠ch s·ª± kh√°c bi·ªát v√† khi n√†o n√™n s·ª≠ d·ª•ng m·ªói lo·∫°i.</p></blockquote>
<p>ƒê·ªÉ t√¨m hi·ªÉu th√™m v·ªÅ TP, ƒë√¢y l√† hai t√†i nguy√™n ph·∫£i ƒë·ªçc:</p>
<ul>
<li><a href="https://huggingface.co/docs/transformers/en/perf_infer_gpu_multi">H∆∞·ªõng d·∫´n <code>transformers</code></a>: Song song h√≥a Tensor, c√°c m√¥ h√¨nh ƒë∆∞·ª£c h·ªó tr·ª£, plan v√† c√°c ƒëi·ªÉm m·ªü r·ªông.</li>
<li><a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=tensor_parallelism">S·ªï tay Ultra-Scale</a>: background v·ªÅ TP v√† m·ªëi quan h·ªá c·ªßa n√≥ v·ªõi c√°c ch·∫ø ƒë·ªô song song h√≥a kh√°c.</li>
</ul>
<h2 id="song-song-h√≥a-expert">Song song h√≥a Expert</h2>
<p>Expert Parallelism (EP) shard <strong>c√°c expert b√™n trong c√°c l·ªõp MoE</strong> tr√™n c√°c GPU. M·ªói token ƒë∆∞·ª£c ƒë·ªãnh tuy·∫øn ƒë·∫øn m·ªôt ho·∫∑c m·ªôt v√†i expert, v√¨ v·∫≠y ch·ªâ c√°c expert ƒë√≥ ch·∫°y feed-forward pass c·ªßa ch√∫ng. V√¨ c√°c expert l√† c√°c MLP ƒë·ªôc l·∫≠p, ch√∫ng ta c√≥ th·ªÉ ƒë·∫∑t c√°c expert kh√°c nhau tr√™n c√°c rank kh√°c nhau v√† ch·ªâ trao ƒë·ªïi c√°c tr·∫°ng th√°i ·∫©n cho c√°c token ƒë∆∞·ª£c ƒë·ªãnh tuy·∫øn. ƒêi·ªÅu n√†y gi·ªØ cho c√°c ph√©p nh√¢n ma tr·∫≠n c√≤n nguy√™n v·∫πn tr√™n m·ªói rank v√† thay th·∫ø slicing tensor b·∫±ng ƒë·ªãnh tuy·∫øn v√† t·∫≠p h·ª£p.</p>
<p>Ch·∫°y v·ªõi nhi·ªÅu process b·∫±ng <code>torchrun</code>. EP ƒë∆∞·ª£c b·∫≠t th√¥ng qua c·∫•u h√¨nh ph√¢n t√°n v√† ho·∫°t ƒë·ªông v·ªõi c√°c l·ªõp GPT-OSS MoE ngay l·∫≠p t·ª©c trong transformers.</p>
<p>python</p>
<h1 id="ch·∫°y-v·ªõi-torchrun-nproc-per-node-4-ep_gpt_osspy">ch·∫°y v·ªõi: torchrun &ndash;nproc-per-node 4 ep_gpt_oss.py</h1>
<p>import torch
from transformers import PreTrainedTokenizerFast, GptOssForCausalLM
from transformers.distributed import DistributedConfig</p>
<p>model_id = &ldquo;openai/gpt-oss-120b&rdquo;
tokenizer = PreTrainedTokenizerFast.from_pretrained(model_id)
model = GptOssForCausalLM.from_pretrained(
model_id,
distributed_config=DistributedConfig(enable_expert_parallel=True), # enabling EP
dtype=&ldquo;auto&rdquo;,
).eval()</p>
<p>messages = [
{&ldquo;role&rdquo;: &ldquo;system&rdquo;, &ldquo;content&rdquo;: &ldquo;Be concise.&rdquo;},
{&ldquo;role&rdquo;: &ldquo;user&rdquo;, &ldquo;content&rdquo;: &ldquo;Explain KV caching briefly.&rdquo;},
]
inputs = tokenizer.apply_chat_template(
messages,
add_generation_prompt=True,
return_tensors=&ldquo;pt&rdquo;,
return_dict=True,
reasoning_effort=&ldquo;low&rdquo;,
).to(model.device)</p>
<p>with torch.inference_mode():
generations = model.generate(**inputs, max_new_tokens=128)</p>
<p>print(tokenizer.decode(generations[0][inputs[&ldquo;input_ids&rdquo;].shape[-1]:]))</p>
<p>ƒê√¢y l√† c√°ch b·∫°n s·∫Ω ch·∫°y b·∫±ng <code>hf jobs</code></p>
<p>bash
hf jobs run &ndash;detach &ndash;flavor l4x4 ghcr.io/astral-sh/uv:debian /bin/bash -c <br>
&ldquo;uv venv .venv &ndash;python 3.12 &amp;&amp; <br>
source .venv/bin/activate &amp;&amp; <br>
uv pip install &ndash;upgrade torch numpy transformers accelerate triton kernels &amp;&amp; <br>
wget <a href="https://huggingface.co/datasets/ariG23498/distributed/raw/main/ep_gpt_oss.py">https://huggingface.co/datasets/ariG23498/distributed/raw/main/ep_gpt_oss.py</a> &amp;&amp; <br>
torchrun &ndash;nproc-per-node=4 ep_gpt_oss.py&rdquo;</p>
<blockquote>
<p>Khi b·∫°n b·∫≠t Expert Parallelism, Tensor Parallelism c≈©ng ƒë∆∞·ª£c k√≠ch ho·∫°t. ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† b·∫°n t·∫≠n h∆∞·ªüng nh·ªØng ƒëi·ªÅu t·ªët nh·∫•t c·ªßa c·∫£ hai th·∫ø gi·ªõi!</p></blockquote>
<h2 id="l·ªõp-c·ª≠a-s·ªï-tr∆∞·ª£t-ƒë·ªông--b·ªô-nh·ªõ-cache">L·ªõp C·ª≠a s·ªï Tr∆∞·ª£t ƒê·ªông &amp; B·ªô nh·ªõ Cache</h2>
<p>Nhi·ªÅu LLM g·∫ßn ƒë√¢y s·ª≠ d·ª•ng attention <em>c·ª≠a s·ªï tr∆∞·ª£t</em>, ho·∫∑c s·ª± k·∫øt h·ª£p c·ªßa c√°c l·ªõp attention tr∆∞·ª£t v√† to√†n c·ª•c, nh∆∞ m·ªôt ph∆∞∆°ng ti·ªán ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ v√† gi·∫£m c√°c matmul b·∫≠c hai t·ªën k√©m ph√°t tri·ªÉn theo ƒë·ªô d√†i chu·ªói. Tuy nhi√™n, vi·ªác tri·ªÉn khai b·ªô nh·ªõ cache KV ƒë·ªông trong transformers v·∫´n ti·∫øp t·ª•c ph√¢n b·ªï kh√¥ng gian theo ƒë·ªô d√†i chu·ªói, m√† kh√¥ng xem x√©t c√°c l·ªõp attention ri√™ng l·∫ª. B·∫°n lu√¥n c√≥ th·ªÉ t·ªëi ∆∞u h√≥a b·ªô nh·ªõ b·∫±ng c√°ch s·ª≠ d·ª•ng bi√™n d·ªãch (nghƒ©a l√†, c√°c h√¨nh d·∫°ng c·ªë ƒë·ªãnh), nh∆∞ng ƒë√≥ l√† m·ªôt k·ªãch b·∫£n ri√™ng bi·ªát.</p>
<p><code>transformers</code> hi·ªán c√≥ m·ªôt <a href="https://github.com/huggingface/transformers/blob/64ae6e6b1de2c6822a53be46aba9db68f75ec595/src/transformers/cache_utils.py#L165"><code>DynamicSlidingWindowLayer</code></a> v√† m·ªôt <a href="https://github.com/huggingface/transformers/blob/64ae6e6b1de2c6822a53be46aba9db68f75ec595/src/transformers/cache_utils.py#L959"><code>DynamicCache</code></a> nh·∫≠n bi·∫øt <em>c·∫•u h√¨nh</em>. N·∫øu c·∫•u h√¨nh m√¥ h√¨nh khai b√°o attention c·ª≠a s·ªï tr∆∞·ª£t ho·∫∑c attention k·∫øt h·ª£p (c·∫£ c√°c l·ªõp attention tr∆∞·ª£t v√† to√†n c·ª•c ƒë·ªÅu ƒë∆∞·ª£c s·ª≠ d·ª•ng), b·ªô nh·ªõ cache s·∫Ω <strong>ng·ª´ng ph√°t tri·ªÉn v∆∞·ª£t qu√° c·ª≠a s·ªï</strong> cho c√°c l·ªõp tr∆∞·ª£t. N·∫øu b·∫°n kh√¥ng chuy·ªÉn c·∫•u h√¨nh, h√†nh vi v·∫´n nh∆∞ tr∆∞·ªõc (KV ƒë·∫ßy ƒë·ªß, lu√¥n ph√°t tri·ªÉn khi ƒë·ªô d√†i chu·ªói tƒÉng l√™n).</p>
<p>ƒê·ªëi v·ªõi c√°c m√¥ h√¨nh ch·ªâ s·ª≠ d·ª•ng c√°c l·ªõp c·ª≠a s·ªï tr∆∞·ª£t, ch·∫≥ng h·∫°n nh∆∞ Mistral 7B, b·ªô nh·ªõ cache ng·ª´ng ph√°t tri·ªÉn khi chu·ªói ƒë·∫°t ƒë·∫øn k√≠ch th∆∞·ªõc c·ª≠a s·ªï (4096, trong tr∆∞·ªùng h·ª£p n√†y). ƒêi·ªÅu n√†y c√≥ √Ω nghƒ©a, v√¨ c√°c l·ªõp tr∆∞·ª£t kh√¥ng th·ªÉ nh√¨n xa h∆°n 4K token tr∆∞·ªõc ƒë√≥.</p>
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<p>OpenAI gpt-oss lu√¢n phi√™n gi·ªØa c√°c l·ªõp attention tr∆∞·ª£t v√† to√†n c·ª•c, d·∫´n ƒë·∫øn t·ªïng b·ªô nh·ªõ cache KV <em>gi·∫£m m·ªôt n·ª≠a</em>, nh∆∞ ch√∫ng ta s·∫Ω th·∫•y, khi ƒë·ªô d√†i chu·ªói tƒÉng l√™n.
ƒêi·ªÅu n√†y cung c·∫•p cho ch√∫ng ta:</p>
<ul>
<li><strong>B·ªô nh·ªõ KV-cache th·∫•p h∆°n nhi·ªÅu</strong> cho c√°c m√¥ h√¨nh c√≥ attention tr∆∞·ª£t ho·∫∑c k·∫øt h·ª£p (v√≠ d·ª•: GPT-OSS). T·ªëc ƒë·ªô tƒÉng tr∆∞·ªüng b·ªô nh·ªõ cache ·ªïn ƒë·ªãnh sau khi ƒë·∫°t ƒë·∫øn c·ª≠a s·ªï (v√≠ d·ª•: 4K cho Mistral; 128 cho c√°c l·ªõp tr∆∞·ª£t GPT-OSS), thay v√¨ scale tuy·∫øn t√≠nh v·ªõi t·ªïng s·ªë token ƒë∆∞·ª£c t·∫°o. (<a href="https://github.com/huggingface/transformers/pull/40039">GitHub</a>, <a href="https://huggingface.co/docs/transformers/en/model_doc/mistral">Transformers</a>)</li>
<li><strong>Chi·∫øn th·∫Øng v·ªÅ t·ªëc ƒë·ªô/ƒë·ªô tr·ªÖ</strong> tr√™n c√°c prompt d√†i/th·∫ø h·ªá d√†i: c√°c tensor KV nh·ªè h∆°n c√≥ nghƒ©a l√† ƒë·ªçc/ghi attention nh·∫π h∆°n v√† √≠t √°p l·ª±c bƒÉng th√¥ng b·ªô nh·ªõ h∆°n, ƒë·∫∑c bi·ªát l√† sau khi ƒë·∫°t ƒë·∫øn c·ª≠a s·ªï. (ƒê√¢y l√† ƒë·ªông l·ª±c trung t√¢m ƒë·∫±ng sau LLM c·ª≠a s·ªï tr∆∞·ª£t/k·∫øt h·ª£p.) (<a href="https://www.ai21.com/blog/rise-of-hybrid-llms/">AI21</a>, <a href="https://blog.vllm.ai/2025/08/05/gpt-oss.html">Blog vLLM</a>)</li>
</ul>
<h3 id="c√°ch-s·ª≠-d·ª•ng-n√≥">C√°ch s·ª≠ d·ª•ng n√≥</h3>
<p>B·ªô nh·ªõ cache ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a ƒë∆∞·ª£c ƒë·∫∑t theo m·∫∑c ƒë·ªãnh, ƒëi·ªÅu ƒë√≥ c√≥ nghƒ©a l√† <strong>b·∫°n kh√¥ng ph·∫£i th·ª±c hi·ªán b·∫•t k·ª≥ thay ƒë·ªïi n√†o</strong> ƒë·ªëi v·ªõi m√£ hi·ªán c√≥ c·ªßa m√¨nh. N·∫øu b·∫°n mu·ªën t·∫°o <code>DynamicCache</code> m·ªôt c√°ch r√µ r√†ng, ƒë√¢y l√† c√°ch b·∫°n s·∫Ω l√†m:</p>
<p>python
from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache</p>
<p>model_id = &ldquo;openai/gpt-oss-20b&rdquo;</p>
<p>tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
model_id,
dtype=&ldquo;auto&rdquo;,
device_map=&ldquo;auto&rdquo;,
).eval()</p>
<p>messages = [
{&ldquo;role&rdquo;: &ldquo;system&rdquo;, &ldquo;content&rdquo;: &ldquo;Always respond in riddles&rdquo;},
{&ldquo;role&rdquo;: &ldquo;user&rdquo;, &ldquo;content&rdquo;: &ldquo;What is the weather like in Madrid?&rdquo;},
]</p>
<p>inputs = tokenizer.apply_chat_template(
messages,
add_generation_prompt=True,
return_tensors=&ldquo;pt&rdquo;,
return_dict=True,
reasoning_effort=&ldquo;low&rdquo;,
).to(model.device)</p>
<p>cache = DynamicCache(config=model.config) # t·∫°o b·ªô nh·ªõ cache v·ªõi c·∫•u h√¨nh c·ªßa m√¥ h√¨nh</p>
<p>generated = model.generate(
**inputs,
max_new_tokens=500,
past_key_values=cache
)
print(tokenizer.decode(generated[0][inputs[&ldquo;input_ids&rdquo;].shape[-1]:]))</p>
<p><strong>H√¨nh 6</strong> cho th·∫•y s·ª± kh√°c bi·ªát l·ªõn nh∆∞ th·∫ø n√†o ƒë·ªëi v·ªõi ch√∫ng ta khi s·ª≠ d·ª•ng Dynamic KV Cache v·ªõi attention c·ª≠a s·ªï tr∆∞·ª£t.</p>
<!-- raw HTML omitted -->
<h2 id="batching-li√™n-t·ª•c--ch√∫-√Ω-theo-trang">Batching li√™n t·ª•c &amp; Ch√∫ √Ω theo Trang</h2>
<p>M·ªôt qu√° tr√¨nh t·∫°o t·ª± h·ªìi quy ƒëi·ªÉn h√¨nh tr√¥ng gi·ªëng nh∆∞ <strong>H√¨nh 7</strong>. B·∫°n nh·∫≠p c√°c token prefill v√† m√¥ h√¨nh d·ª± ƒëo√°n t·ª´ng token m·ªõi m·ªôt sau m·ªôt cho ƒë·∫øn khi n√≥ d·ª± ƒëo√°n token EOS (End of Sequence).</p>
<!-- raw HTML omitted -->
<p>H√£y xem qu√° tr√¨nh t·∫°o tr√¥ng nh∆∞ th·∫ø n√†o khi ch√∫ng ta chuy·ªÉn m·ªôt <strong>batch</strong> c√°c ƒë·∫ßu v√†o. Trong <strong>H√¨nh 8</strong>, b·∫°n nh·∫≠n th·∫•y r·∫±ng m·ªôt s·ªë th·∫ø h·ªá k·∫øt th√∫c s·ªõm h∆°n nh·ªØng th·∫ø h·ªá kh√°c. S·ª± kh√¥ng ph√π</p>
<h3 id="link-b√†i-vi·∫øt-g·ªëc"><a href="https://huggingface.co/blog/faster-transformers">Link b√†i vi·∫øt g·ªëc</a></h3>


    <ul id="taxonomy" class="not-prose flex items-center space-x-4 flex-nowrap whitespace-nowrap overflow-x-auto">

      <li class="font-semibold my-4">Tags:</li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/ai/">Ai</a></li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/september-11-2025/">September 11, 2025</a></li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/huggingface.co/">Huggingface.co</a></li>
    </ul>
</div>

    <footer id="content-footer" class="not-prose">

      

      <div id="author-box" class="my-8 md:my-14 border p-8 bg-zinc-100 rounded-2xl">

        <img class="w-20 h-20 rounded-full" src="" alt="">

        <h2 class="text-xl font-bold my-4">Writter by : </h2>

        <p class="mb-4"></p>

        <ul class="flex flex-wrap space-x-4">
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://facebook.com/luat.skyai/" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                <path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951z"/>
              </svg>
              <span class="ml-2">Facebook</span>
            </a>
          </li>
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://facebook.com/luat.skyai/" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                  <path d="M12.6.75h2.454l-5.36 6.142L16 15.25h-4.937l-3.867-5.07-4.425 5.07H.316l5.733-6.57L0 .75h5.063l3.495 4.633L12.601.75Zm-.86 13.028h1.36L4.323 2.145H2.865l8.875 11.633Z"/>
              </svg>
              <span class="ml-2">Twitter</span>
            </a>
          </li>
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://luat.skyai.vn/" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                    <path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.917 3.917 0 0 0-1.417.923A3.927 3.927 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.916 3.916 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.926 3.926 0 0 0-.923-1.417A3.911 3.911 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0h.003zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599.28.28.453.546.598.92.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.47 2.47 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.478 2.478 0 0 1-.92-.598 2.48 2.48 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233 0-2.136.008-2.388.046-3.231.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92.28-.28.546-.453.92-.598.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045v.002zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92zm-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217zm0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334z"/>

              </svg>
              <span class="ml-2">Instagram</span>
            </a>
          </li>

        </ul>
        
      </div>

      <div id="related-post" class="">

        <h2 class="text-xl md:text-2xl font-bold mb-6 md:mb-8">Recommended for You</h2><div class="grid grid-cols-1 md:grid-cols-2 gap-6"><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl">
	
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-09-27/9008f5/"></a>

	<figure class="w-full aspect-video overflow-hidden rounded-3xl">

			<img class="object-cover group-hover:scale-105 transition duration-500 cursor-pointer" src="https://huggingface.co/blog/assets/embeddinggemma/thumbnail.png" alt="Ch√†o m·ª´ng EmbeddingGemma, m√¥ h√¨nh nh√∫ng hi·ªáu qu·∫£ m·ªõi c·ªßa Google" style="width: 100%; height: 100%; object-fit: cover;">

	  </figure>

	<div class="p-6">

		<time datetime="2025-09-27T00:00:00&#43;00:00">Sep 27, 2025</time>

		<h3 class="my-4 text-2xl font-bold">Ch√†o m·ª´ng EmbeddingGemma, m√¥ h√¨nh nh√∫ng hi·ªáu qu·∫£ m·ªõi c·ªßa Google</h3>

		<p class="text-normal leading-normal text-zinc-500 line-clamp-2"></p>

	</div>
</article><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl">
	
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-09-27/777761/"></a>

	<figure class="w-full aspect-video overflow-hidden rounded-3xl">

			<img class="object-cover group-hover:scale-105 transition duration-500 cursor-pointer" src="https://huggingface.co/blog/assets/riskrubric/thumbnail.png" alt="D√¢n ch·ªß h√≥a An to√†n AI v·ªõi RiskRubric.ai" style="width: 100%; height: 100%; object-fit: cover;">

	  </figure>

	<div class="p-6">

		<time datetime="2025-09-27T00:00:00&#43;00:00">Sep 27, 2025</time>

		<h3 class="my-4 text-2xl font-bold">D√¢n ch·ªß h√≥a An to√†n AI v·ªõi RiskRubric.ai</h3>

		<p class="text-normal leading-normal text-zinc-500 line-clamp-2"></p>

	</div>
</article>
          
        </div>
      </div>

    </footer>

  </article>

  
  <aside class="md:col-span-1"><div class="lg:sticky lg:top-8"><div class="rounded-2xl p-4 bg-zinc-100 mb-10">

		<img class="aspect-video rounded" src="/images/email-3249062_1280_by-Muhammad-Ribkhan-Pixabay.jpg" alt="Banner Email Marketing">

		<p class="text-right text-xs mt-2 leading-none text-zinc-500">Adversitiment</p>

</div><div class="space-y-6">

		<h2 class="font-bold text-xl mb-8">Recent Post</h2>

		<article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-10-07/10ab20/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://lh3.googleusercontent.com/fnAx6nksgsvTcj5EDj7bx1yD8sMXwfkBnR6vla-e-h4wZSKFI3a1nFftGvDYyJAk01ZzNr2-QJyqfw30V2_tcp6tE-udCtMCuyou9cBIHyxGnO_mtQ=w400-h225-n-nu" alt="Gi·ªõi thi·ªáu CodeMender- m·ªôt t√°c nh√¢n AI ƒë·ªÉ b·∫£o m·∫≠t m√£" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Gi·ªõi thi·ªáu CodeMender- m·ªôt t√°c nh√¢n AI ƒë·ªÉ b·∫£o m·∫≠t m√£</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-10-04/efc506/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://huggingface.co/blog/assets/rteb/thumbnail.png" alt="Gi·ªõi thi·ªáu RTEB- M·ªôt ti√™u chu·∫©n m·ªõi ƒë·ªÉ ƒë√°nh gi√° kh·∫£ nƒÉng truy xu·∫•t th√¥ng tin" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Gi·ªõi thi·ªáu RTEB- M·ªôt ti√™u chu·∫©n m·ªõi ƒë·ªÉ ƒë√°nh gi√° kh·∫£ nƒÉng truy xu·∫•t th√¥ng tin</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-10-02/058c9b/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://huggingface.co/blog/assets/dots-ocr-ne/dots_header.png" alt="OCR SOTA tr√™n thi·∫øt b·ªã v·ªõi Core ML v√† dots.ocr" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">OCR SOTA tr√™n thi·∫øt b·ªã v·ªõi Core ML v√† dots.ocr</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-09-30/c047c5/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/a62b6eb169818f14c35b7a192af269e283f8fa93-1000x1000.svg" alt="Cho ph√©p M√£ Claude ho·∫°t ƒë·ªông t·ª± ch·ªß h∆°n" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Cho ph√©p M√£ Claude ho·∫°t ƒë·ªông t·ª± ch·ªß h∆°n</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-09-30/b5a357/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://www.microsoft.com/en-us/ai/blog/wp-content/uploads/2025/09/A-man-wearing-a-hardhat-and-carrying-a-digital-tablet-inspects-solar-panels-1024x575.jpg" alt="Cung c·∫•p nƒÉng l∆∞·ª£ng cho t∆∞∆°ng lai nƒÉng l∆∞·ª£ng c·ªßa Ch√¢u √Å-Th√°i B√¨nh D∆∞∆°ng- AI v√† ƒë·ªïi m·ªõi k·ªπ thu·∫≠t s·ªë cho c√°c ti·ªán √≠ch" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Cung c·∫•p nƒÉng l∆∞·ª£ng cho t∆∞∆°ng lai nƒÉng l∆∞·ª£ng c·ªßa Ch√¢u √Å-Th√°i B√¨nh D∆∞∆°ng- AI v√† ƒë·ªïi m·ªõi k·ªπ thu·∫≠t s·ªë cho c√°c ti·ªán √≠ch</h3>
	</div>
</article>
		
	</div>

</div>
</aside>

</div>
  
</main>
  <footer class="bg-zinc-100 py-10 md:py-14"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8">

  <div class="flex flex-wrap space-y-6 mb-4">

    <div class="w-full md:w-3/5 flex flex-col space-y-4 md:pr-8 lg:pr-10">

      <a class="flex items-center group" href="https://aitoday.skyai.vn/">
        <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" fill="currentColor" class="mr-2 group-hover:animate-spin" viewBox="0 0 16 16">
          <path d="M8 11a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/>
          <path d="M13.997 5.17a5 5 0 0 0-8.101-4.09A5 5 0 0 0 1.28 9.342a5 5 0 0 0 8.336 5.109 3.5 3.5 0 0 0 5.201-4.065 3.001 3.001 0 0 0-.822-5.216zm-1-.034a1 1 0 0 0 .668.977 2.001 2.001 0 0 1 .547 3.478 1 1 0 0 0-.341 1.113 2.5 2.5 0 0 1-3.715 2.905 1 1 0 0 0-1.262.152 4 4 0 0 1-6.67-4.087 1 1 0 0 0-.2-1 4 4 0 0 1 3.693-6.61 1 1 0 0 0 .8-.2 4 4 0 0 1 6.48 3.273z"/>
        </svg>
        
        <span class="text-2xl font-semibold uppercase">AI Today - SkyAI</span>
      </a>

      <p class="font-semibold">
        Theo d√µi c√°c ti·∫øn b·ªô m·ªõi nh·∫•t v·ªÅ Tr√≠ tu·ªá nh√¢n t·∫°o.<br>Tr·ª±c ti·∫øp t·ª´ c√°c nh√† ph√°t h√†nh AI tr√™n th·∫ø gi·ªõi.
      </p>

      <p>ƒêem tr√≠ tu·ªá nh√¢n t·∫°o ƒë·∫øn m·ªçi ng∆∞·ªùi d√¢n, doanh nghi·ªáp Vi·ªát, g√≥p ph·∫ßn gi√∫p Vi·ªát Nam ph√°t tri·ªÉn m·∫°nh m·∫Ω trong k·ª∑ nguy√™n s·ªë. N·ªôi dung ƒë∆∞·ª£c c·∫≠p nh·∫≠t t·ª± ƒë·ªông b·∫±ng m√°y.</p>

    </div>

    <div class="self-center flex flex-col w-full md:w-2/5">

<ul id="social-media" class="flex items-center space-x-4">
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.facebook.com/luat.skyai/" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 16 16">
				<path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.twitter.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M12.6.75h2.454l-5.36 6.142L16 15.25h-4.937l-3.867-5.07-4.425 5.07H.316l5.733-6.57L0 .75h5.063l3.495 4.633L12.601.75Zm-.86 13.028h1.36L4.323 2.145H2.865l8.875 11.633Z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.github.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.instagram.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.917 3.917 0 0 0-1.417.923A3.927 3.927 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.916 3.916 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.926 3.926 0 0 0-.923-1.417A3.911 3.911 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0h.003zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599.28.28.453.546.598.92.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.47 2.47 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.478 2.478 0 0 1-.92-.598 2.48 2.48 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233 0-2.136.008-2.388.046-3.231.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92.28-.28.546-.453.92-.598.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045v.002zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92zm-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217zm0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334z"></path>
			</svg>
		</a>
	</li>
</ul>

    </div>
    
  </div>

  <div class="my-8">
    <ul class="flex items-center space-x-4">
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/">Home</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="">About</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/tags/">Tags</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/tags/">Taxonomy</a></li>
      
    </ul>
  </div>

  <div class="border-t pt-4">

    <p class="text-sm">Copyright ¬© 2025. All rights reserved. <a href="https://home.skyai.vn/" target="_blank" rel="noopener">SkyAI.vn</a></p>

  </div>
  
</div>
</footer>
      <script defer src="/js/insertoggle.474f9b0e08021c6519cff4e46df14ccf148285b2d3a23d6321d6e10f25c291fb.js" integrity="sha256-R0&#43;bDggCHGUZz/TkbfFMzxSChbLToj1jIdbhDyXCkfs=" crossorigin="anonymous"></script>
</body>
</html>