<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>CƒÉn ch·ªânh Ng√¢n s√°ch- L√†m cho M√¥ h√¨nh Suy lu·∫≠n b·∫±ng Ng√¥n ng·ªØ c·ªßa Ng∆∞·ªùi d√πng | AI Today - SkyAI</title>

<meta name="description" content="CƒÉn ch·ªânh Ng√¢n s√°ch- L√†m cho M√¥ h√¨nh Suy lu·∫≠n b·∫±ng Ng√¥n ng·ªØ c·ªßa Ng∆∞·ªùi d√πng">
      <link rel="stylesheet" href="/css/main.min.584aed796ba9d245f84009d57da9662e9c02059611972780263728b442fc25ff.css" integrity="sha256-WErteWup0kX4QAnVfalmLpwCBZYRlyeAJjcotEL8Jf8=" crossorigin="anonymous">

<link rel="icon" type="image/svg+xml" href="https://aitoday.skyai.vn/favicon.svg"> 
<link rel="icon" type="image/x-icon" href="https://aitoday.skyai.vn/favicon.ico"> 
<link rel="icon" type="image/png" sizes="16x16" href="https://aitoday.skyai.vn/favicon.png"> 
<link rel="icon" type="image/png" sizes="32x32" href="https://aitoday.skyai.vn/favicon-32.png"> 
<link rel="icon" type="image/png" sizes="64x64" href="https://aitoday.skyai.vn/favicon-64.png"> 

</head>
<body>
  <header class="py-6 border-b"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8 flex flex-col">
	<div class="flex items-center">
		<div class="flex items-center">
			<button class="flex items-center space-x-2 rounded-full border py-1 pr-[5px] pl-3 group bg-zinc-100 hover:bg-zinc-200 toggle-button" data-target="menu-bar">
				<svg width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
					<path fill-rule="evenodd" d="M2.5 12a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5zm0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5zm0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5z"></path>
				</svg>
				<span class="bg-blue-500 fill-white rounded-full p-1.5">
					<svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="group-hover:animate-spin bi bi-egg-fried fill-white" viewBox="0 0 16 16">
						<path d="M8 11a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"></path>
						<path d="M13.997 5.17a5 5 0 0 0-8.101-4.09A5 5 0 0 0 1.28 9.342a5 5 0 0 0 8.336 5.109 3.5 3.5 0 0 0 5.201-4.065 3.001 3.001 0 0 0-.822-5.216zm-1-.034a1 1 0 0 0 .668.977 2.001 2.001 0 0 1 .547 3.478 1 1 0 0 0-.341 1.113 2.5 2.5 0 0 1-3.715 2.905 1 1 0 0 0-1.262.152 4 4 0 0 1-6.67-4.087 1 1 0 0 0-.2-1 4 4 0 0 1 3.693-6.61 1 1 0 0 0 .8-.2 4 4 0 0 1 6.48 3.273z"></path>
					</svg>
				</span>
			</button>
			<div class="relative rounded-full py-1.5 px-6 bg-zinc-100 hover:bg-zinc-200 text-xl font-bold uppercase mx-2">
				<h2><a class="before:content-[''] before:z-10 before:top-0 before:right-0 before:left-0 before:bottom-0 before:absolute before:pointer-events-auto" href="https://aitoday.skyai.vn/">AI Today - SkyAI</a></h2>
			</div>
		</div>
		<div class="flex items-center ml-auto">
			<button class="flex items-center rounded-full p-3 bg-zinc-100 hover:bg-zinc-200 toggle-button" data-target="search-bar">
				<svg width="18" height="18" fill="currentColor" viewBox="0 0 16 16">
					<path id="path1" class="transition-all ease-linear" d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path>
					<path id="path2" class="transition-all ease-linear hidden" d="M2.146 2.854a.5.5 0 1 1 .708-.708L8 7.293l5.146-5.147a.5.5 0 0 1 .708.708L8.707 8l5.147 5.146a.5.5 0 0 1-.708.708L8 8.707l-5.146 5.147a.5.5 0 0 1-.708-.708L7.293 8 2.146 2.854Z"></path>
				</svg>
			</button>
		</div>
	</div>
  <nav id="menu-bar" class="block mt-3 close">
    <ul class="flex items-center flex flex-nowrap whitespace-nowrap overflow-x-auto space-x-4">
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/">Home</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200">About</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/tags/">Tags</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/tags/">Taxonomy</a>
    </li>
    </ul>
  </nav>
<div id="search-bar" class="block mt-3 close">
	<form id="search" class="flex items-stretch">
		<input class="w-full block px-6 py-2 rounded-l-full focus:outline-none border border-zinc-200" type="text" placeholder="Search...">
		<button class="flex items-center px-7 py-2.5 rounded-r-full border border-zinc-200 ">
			<svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="group-hover:animate-pulse" viewBox="0 0 16 16">
				<path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path>
			</svg>
		</button>
	</form>
</div>
</div></header>
  <main class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8"><div id="breadcrumb" class="max-w-7xl mx-auto py-8">
	<ul class="flex space-x-4 text-sm text-zinc-500">
		<li class="after:content-['‚ùØ'] after:ml-4 after:opacity-30 last:after:content-none uppercase">
			<a href="https://aitoday.skyai.vn/">AI Today - SkyAI</a>
		</li>
		<li class="after:content-['‚ùØ'] after:ml-4 after:opacity-30 last:after:content-none uppercase">
			<a href="https://aitoday.skyai.vn/posts/">Posts</a>
		</li>
	</ul>
</div><div class="grid grid-cols-1 md:grid-cols-3 gap-6 mb-14">

  <article class="md:col-span-2 prose lg:prose-lg">

    <header class="not-prose">
      
      <h1 id="title" class="text-4xl font-bold leading-normal">CƒÉn ch·ªânh Ng√¢n s√°ch- L√†m cho M√¥ h√¨nh Suy lu·∫≠n b·∫±ng Ng√¥n ng·ªØ c·ªßa Ng∆∞·ªùi d√πng</h1>

      <div id="lead" class="my-6">

        <p class="font-bold">CƒÉn ch·ªânh Ng√¢n s√°ch- L√†m cho M√¥ h√¨nh Suy lu·∫≠n b·∫±ng Ng√¥n ng·ªØ c·ªßa Ng∆∞·ªùi d√πng </p>

      </div>
      
      <div id="writer" class="flex items-center space-x-4"><ul class="flex items-center space-x-4 flex-nowrap whitespace-nowrap overflow-x-auto">
          
          <li class="font-semibold my-2"></li>
          
          <li class="before:content-['‚Ä¢'] before:mr-2 before:opacity-50 my-2"><time datetime="2025-11-10T00:00:00&#43;00:00">November 10, 2025</time>
          </li>

          <li class="before:content-['‚Ä¢'] before:mr-2 before:opacity-50 my-2">
            14 min read
          </li>

        </ul>

      </div>
      
    </header>

    <figure id="featureimage" class="rounded-xl aspect-video">

          <img class="rounded-lg" src="https://cdn-avatars.huggingface.co/v1/production/uploads/63600b93d9e4214b9a67807c/x_VE45ZPoaUFiryYl_5J5.png" alt="CƒÉn ch·ªânh Ng√¢n s√°ch- L√†m cho M√¥ h√¨nh Suy lu·∫≠n b·∫±ng Ng√¥n ng·ªØ c·ªßa Ng∆∞·ªùi d√πng">
        <figcaption class="text-center italic text-xs">CƒÉn ch·ªânh Ng√¢n s√°ch- L√†m cho M√¥ h√¨nh Suy lu·∫≠n b·∫±ng Ng√¥n ng·ªØ c·ªßa Ng∆∞·ªùi d√πng</figcaption>

    </figure>

    <div id="content" class="mb-14">
      <h1 id="budget-alignment-making-models-reason-in-the-users-language">Budget Alignment: Making Models Reason in the User‚Äôs Language</h1>
<p><strong>Authors</strong>: Shan Chen*, Jirui Qi*, Zidi Xiong, Timothy Miller, Arianna Bisazza, Raquel Fern√°ndez, and Danielle Bitterman</p>
<p><strong>Institutions</strong>: Mass General Brigham | Harvard University | University of Groningen | University of Amsterdam</p>
<p><em>Please read this as a late stage work in progress where we are colleagues sharing this in a lab meeting to help/motivate potential parallel research.</em></p>
<h2 id="introduction">Introduction</h2>
<p>When you ask a large language model (LLM) a math question in Japanese, it responds politely in Japanese‚Äîbut internally, it‚Äôs reasoning in English/Chinese. Variables, steps, and mathematical lemmas often silently switch languages during the reasoning process. This behavior, where models default to English for chain-of-thought (CoT) reasoning, is more than just a quirk. It interferes with instruction following, confuses human reviewers, and undermines the goal of multilingual evaluation.</p>
<p>The objective is straightforward: we want models to reason about a question in the same language it was asked, not just answer in that language. However, this proves to be more challenging than it seems. Forcing models to reason in languages other than English typically leads to a decrease in accuracy. Previous research indicates that instructing models to reason only in the prompt language through prompting or steering improves coherence and alignment with grading <!-- raw HTML omitted -->1<!-- raw HTML omitted -->, but this often comes at a significant cost to accuracy. Even a small amount of multilingual fine-tuning helps, but doesn‚Äôt eliminate this trade-off <!-- raw HTML omitted -->2<!-- raw HTML omitted -->. Furthermore, models not only prefer to reason in English, but they also reason <em>more effectively</em> in English. When researchers enforce strict reasoning within the prompt language (e.g., in Swahili or Thai), models often lose accuracy compared to when they are allowed to reason in English. For higher-resource languages like French or German, this trade-off is less pronounced‚Äîmodels can reason in-language almost as well as in English. For low-resource languages, enforcing strict in-language reasoning harms performance more significantly.</p>
<p>Why do models switch to English in the first place? A large part of the reason lies in their training. Most reasoning data is in English. Fine-tuning even strong multilingual models on English CoT data often causes them to adopt English as their ‚Äúinternal language of logic.‚Äù Yong et al. (2025) observed a ‚Äúquote-and-think‚Äù behavior <!-- raw HTML omitted -->3<!-- raw HTML omitted -->, where models copy phrases from the prompt language but then explain everything in English <!-- raw HTML omitted -->4<!-- raw HTML omitted -->. The model understands the question in the non-English language‚Äîit just prefers to reason in English.</p>
<p>Our technical goal is simple: <strong>stop the switching without sacrificing accuracy</strong>‚Äîideally, push the Pareto frontier of <em>(Accuracy, Language-consistency)</em>.
And we want to share this as a practical guide with lessons we learned along the way.</p>
<p>Our code and data are available at:</p>
<p>üîó <a href="https://github.com/Betswish/mCoT-XReasoning/tree/main/training">https://github.com/Betswish/mCoT-XReasoning/tree/main/training</a></p>
<p>ü§î <a href="https://github.com/Betswish/mCoT-pass-K">https://github.com/Betswish/mCoT-pass-K</a></p>
<p>ü§ó <a href="https://huggingface.co/collections/shanchen/xreasoning">https://huggingface.co/collections/shanchen/xreasoning</a></p>
<hr>
<h2 id="what-we-try-method-in-two-steps">What we try (Method in two steps)</h2>
<p>üîß <strong>Base model.</strong> <code>deepseek-ai/DeepSeek-R1-Distill-Qwen-7B</code>, a large reasoning model distilled from R1 through supervised fine-tuning on its reasoning traces, exhibiting an English/Chinese-dominant prior.</p>
<p><strong>Step 1 ‚Äî Small SFT to teach in-language reasoning.</strong>
We fine-tune on <strong>817 curated multilingual reasoning chains</strong> (from LiMO <!-- raw HTML omitted -->5<!-- raw HTML omitted -->). This supervision data contains high-quality reasoning data matching R1 long-form reasoning <em>style</em>. No Reinforcement Learning (RL) here‚Äîjust teach the policy to keep reasoning in the user‚Äôs query language.</p>
<p><strong>Step 2 ‚Äî Math-only GRPO to push accuracy while retaining reasoning language.</strong>
We run an RLVR-style GRPO with no KL, higher clip of 0.28 vs -0.2 (DAPO-like <!-- raw HTML omitted -->6<!-- raw HTML omitted -->), rollout 24, LoRA r=8, LR = 1e-5, <strong>only on a Math-500 set translated to each language</strong>.
Intuition: let RL optimize hard cases and verification behaviors, while the high clip reduces catastrophic reasoning style collapse back to English.</p>
<p>We set the verifiable rewards as <strong>1.0 for accuracy, 0.2 for language consistency of reasoning traces, and 0.2 for answer format</strong> <!-- raw HTML omitted -->7<!-- raw HTML omitted -->.</p>
<p>üìä <strong>Evaluation.</strong></p>
<p>We tried our approach on three different languages: <strong>Japanese (JA) / French (FR) / Spanish (ES)</strong></p>
<p>And tested on multiple datasets: <strong>MMLU College Math (MMLU Math), AIME25, GPQA, MMLU Pro Medicine (MMLU Med)</strong></p>
<p>The first two are in-domain: MMLU-Math is similar to the training data in terms of hardness, while AIME25 is harder.
The other two are out-of-domain: GPQA covers hard science questions, and MMLU Pro Medicine is made up of hard questions in the medical domain.</p>
<p><strong>Regimes tested:</strong></p>
<ul>
<li>Base -&gt; <code>deepseek-ai/DeepSeek-R1-Distill-Qwen-7B</code></li>
<li>SFT on top of base</li>
<li>GRPO-from-Base</li>
<li>GRPO-from-SFT</li>
</ul>
<p><strong>Metrics:</strong></p>
<ul>
<li><code>pass@k(1,5,10) where n = 32</code> for accuracy <!-- raw HTML omitted -->8<!-- raw HTML omitted --></li>
<li><code>Language-consistency %</code> (both reasoning traces <strong>and</strong> final answers must be in the requested language; script-aware checks)</li>
</ul>
<p><strong>How we score language consistency:</strong>
We check the entire CoT span and the final boxed answer.
A sample counts as <code>Following = 1</code> only if both passages are in the requested language (script tokens, numerals, and markers allowed); otherwise <code>0</code>.
We report the % across the set.</p>
<hr>
<h2 id="-key-contributions">üîë Key contributions</h2>
<ol>
<li>
<p><strong>Small SFT reprograms inner monologue.</strong>
With only <strong>817 chains</strong>, language consistency rises near the ceiling in French/Spanish across datasets and substantially in Japanese (Fig. RQ0).</p>
</li>
<li>
<p><strong>Two-step recipe Pareto-improves.</strong>
SFT secures language consistency; <strong>GRPO-SFT recovers/boosts accuracy on tough sets</strong> (AIME/GPQA) without reverting to English (Figs. RQ1‚ÄìRQ4).</p>
</li>
<li>
<p><strong>Diagnose regressions and actionable fixes.</strong>
Regressions stem from:</p>
<ul>
<li>Japanese tokenization/numeric friction,</li>
<li>Spanish cue misalignment,</li>
<li>medicine reward/style mismatch.</li>
<li>Tokenizer-aware normalization, small Japanese/Spanish SFT top-ups, and multi-objective GRPO (with optional model merging) could recover accuracy without sacrificing in-language reasoning.</li>
</ul>
</li>
<li>
<p><strong>TL; DR. You can briefly see our main results from the two figures below:</strong>
Starting from an EN/ZH-dominant reasoning prior, small multilingual SFT is the most cost-effective way to ‚Äústeer‚Äù in-language chains of reasoning. Adding math-only GRPO then recovers or improves accuracy on hard sets like AIME and GPQA while mostly preserving SFT‚Äôs language consistency discipline ‚Äî pushing the Accuracy √ó Following frontier in many language-dataset pairs. The two pain points, Japanese (tokenization/numeric friction) and medicine (reward/style mismatch), are expected from the base prior and training signal, and both have potential straightforward fixes with light domain augmentation. And surprisingly, model merging can be very useful and effective.</p>
</li>
</ol>
<p>Figure 1.a) Performance comparison overall across methods</p>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/63600b93d9e4214b9a67807c/YE1iHvwv8Oa3FFP96AGlz.png" alt="Figure 1.a"></p>
<p>Figure 1.b) Overall language consistency rate comparison across methods</p>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/63600b93d9e4214b9a67807c/hTwh5zKc-TbV0s3-tMxvZ.png" alt="Figure 1.b"></p>
<hr>
<h2 id="rq0--can-small-sft-reprogram-a-reasoning-models-reasoning-tone">RQ0 ‚Äî Can small SFT reprogram a reasoning model‚Äôs &ldquo;reasoning tone&rdquo;?</h2>
<p>Models often output the final answer in the same language as the user query. We want the <strong>reasoning process</strong> to match the prompt (user) language, too.</p>
<p><strong>Results.</strong>
SFT drives the language consistency rate close to the ceiling (<strong>~99‚Äì100%</strong>) in French/Spanish and raises Japanese substantially (<strong>high-80s/90s</strong>).
The language consistency rates averaged across all datasets are shown in Fig. RQ0: bars labeled Japanese/French/Spanish.</p>
<p><strong>Interpretation.</strong>
A few hundred <strong>high-quality chains</strong> are enough to overwrite the English/Chinese inner-monologue priority to other languages. Japanese remains stubborn‚Äîsee RQ5.</p>
<!-- raw HTML omitted -->
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/63600b93d9e4214b9a67807c/JT_L8xYl-aX_UE34JiEq5.png" alt="RQ0 Results"></p>
<hr>
<h2 id="rq1--does-sft-help-accuracy-or-only-language-reasoning-style">RQ1 ‚Äî Does SFT help accuracy, or only language reasoning <em>style</em>?</h2>
<p>We have shown that <strong>SFT significantly improves language consistency rates</strong>, but how about the accuracy?</p>
<p><strong>Design.</strong>
Compare the accuracy <strong>Base vs SFT</strong> on <code>pass@k</code> per dataset-language
(Fig. RQ1: Œî pass@10 = SFT ‚Äì Base).</p>
<p><strong>Findings.</strong></p>
<ul>
<li>
<p><strong>MMLU-Math:</strong> substantial improvements when train and test are in the same domain</p>
<ul>
<li><em>French:</em> ~76 ‚Üí <strong>98</strong></li>
<li><em>Spanish:</em> ~80 ‚Üí <strong>99</strong></li>
<li><em>Japanese:</em> ~68 ‚Üí <strong>88</strong></li>
</ul>
</li>
<li>
<p><strong>AIME:</strong> mixed. Although AIME contains math problems, it is way more difficult than LiMo, making it less likely to be considered as in-domain. As a result, SFT trades accuracy for strict language consistency when reasoning in ES.</p>
</li>
<li>
<p><strong>GPQA / MMLU Pro Medicine:</strong> Accuracy drops in most cases, but language consistency rises after SFT, indicating that it&rsquo;s not trivial to generalize the capability of generating the correct answer from the training domain to others.</p>
</li>
</ul>
<p><strong>Takeaway.</strong>
SFT reliably improves language consistency <strong>and often increases accuracy on in-domain tasks (Math).</strong>
On OOD, SFT can over-narrate or change prior most probable token paths since the models are undertrained to reason in lower resource languages‚Äîaccuracy may dip unless taking further actions (e.g., reinforced by RL, shown in RQ2 and RQ3).</p>
<p><strong>Practical guidance.</strong>
If your target is <strong>language consistency/reasoning style + some accuracy</strong>, SFT alone is cost-effective in-domain.
If you also need robustness on hard and/or OOD sets, doing an <strong>RL top-up could be helpful.</strong></p>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/63600b93d9e4214b9a67807c/A4FR-0zXXPfHQ3QI3gWfi.png" alt="RQ1 Results"></p>
<hr>
<h2 id="rq2--when-rl-comes-how-does-grpo-help-with-accuracy">RQ2 ‚Äî When RL comes, how does GRPO help with accuracy?</h2>
<p><strong>Design.</strong>
Train GRPO only on Math-500; evaluate deltas (<strong>GRPO-SFT ‚àí SFT</strong>) across
MMLU-Math / AIME / GPQA / MMLU-Med (Fig. RQ2).</p>
<p><strong>In-domain.</strong>
SFT helps accuracy, but not always; GRPO brings a boost on top of the base SFT while maintaining language consistency of reasoning traces.</p>
<ul>
<li><strong>MMLU-Math-FR</strong> pass@10: <strong>76.0 ‚Üí 97.8 ‚Üí 98.0</strong> (Base ‚Üí SFT ‚Üí GRPO-SFT)</li>
<li><strong>MMLU-Math-ES</strong> pass@10: <strong>80.5 ‚Üí 98.6 ‚Üí 99.1</strong> (Base ‚Üí SFT ‚Üí GRPO-SFT)</li>
<li><strong>MMLU-Math-JA</strong> pass@10: <strong>68.1 ‚Üí 88.0 ‚Üí 91.5</strong> (Base ‚Üí SFT ‚Üí GRPO-SFT)</li>
</ul>
<p>The improvement in accuracy is consistent but slight due to the fact that MMLU-Math is relatively easy:
The model almost achieves 90‚Äì100% accuracy after SFT, leaving no room for GRPO. Thus, the OOD sets are more informative.</p>
<p><strong>Out-of-domain.</strong></p>
<p>Positive transfers on <strong>AIME JA/FR/ES and GPQA JA/FR</strong>.
For instance:</p>
<ul>
<li><strong>GPQA-ES</strong> pass@10: <strong>68.7 ‚Üí 85.2 ‚Üí 85.7</strong> (Base ‚Üí SFT ‚Üí GRPO-SFT)</li>
<li><strong>AIME-JA</strong> pass@10: <strong>22.6 ‚Üí 28.5 ‚Üí 34.4</strong> (Base ‚Üí SFT ‚Üí GRPO-SFT; GRPO adds a large JA gain)</li>
</ul>
<p>More results are shown in the figure below.
Although improvements on AIME-FR/ES and GPQA-ES are marginal, they still indicate a successful transfer of knowledge on the OOD setup after GRPO.</p>
<p><strong>Negative transfers on Pro-Medicine.</strong></p>
<ul>
<li>Accuracy improves on Pro-Medicine-JA but decreases on French and Spanish.</li>
</ul>
<p><strong>Interpretation.</strong>
GRPO learns verification/search habits that generalize: language consistency, math reasoning styles, re-checking numeric steps, and tighter answer boxing.
Those help <strong>GPQA and AIME</strong>.
But medicine needs domain lexicon, evidence phrasing, and calibrated claims‚Äî<strong>absent in math RL</strong>.
Previous works have shown reasoning-only post-training harms performance on downstream instruction-following and knowledge recall tasks <!-- raw HTML omitted -->9<!-- raw HTML omitted -->.</p>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/63600b93d9e4214b9a67807c/wuOq2AE686qUpuCCWgRow.png" alt="RQ2 Results"></p>
<hr>
<h2 id="rq3--where-should-rl-start-from-base-or-sft">RQ3 ‚Äî Where should RL start from: Base or SFT?</h2>
<p><strong>Design.</strong>
Compare <strong>GRPO-from-Base vs GRPO-from-SFT</strong> (Fig. RQ3).</p>
<p><strong>Patterns.</strong></p>
<ul>
<li><strong>GRPO-from-SFT is a steadier path.</strong>
On MMLU-Math FR, for example, GRPO-SFT sits around <strong>~98 pass@10</strong> while GRPO-Base is closer to <strong>~70</strong>,
i.e., <strong>starting from SFT provides language consistency and still improves accuracy.</strong></li>
<li><strong>SFT ‚Üí RL keeps the multilingual policy.</strong>
Because SFT already forced the model to reason in Japanese/French/Spanish,
RL on top of that mostly optimizes correctness <strong>without switching back to EN/ZH reasoning</strong> (Fig. 1b).</li>
</ul>
<p><strong>Interpretation.</strong>
<strong>SFT establishes the multilingual ‚Äúreasoning policy.‚Äù</strong>
Starting RL from the SFT model lets GRPO optimize correctness <em>while preserving language consistency</em>.
RL from Base sometimes pushes the model back toward its original reasoning style while still producing answers in the target language.
That can make a few out-of-domain slices look better, but it also increases variance and <strong>style regression</strong> compared to starting from SFT.</p>
<p><strong>Practical rule.</strong>
If you care about following <em>(see figure 1b)</em> <strong>and</strong> better in-domain accuracy, <strong>do GRPO after SFT.</strong></p>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/63600b93d9e4214b9a67807c/cgrzgLM0Qb-MgLplropUB.png" alt="RQ3 Results"></p>
<hr>
<h2 id="rq4--can-we-push-the-pareto-frontier-instead-of-trading-accuracy-for-language-consistency">RQ4 ‚Äî Can we push the Pareto frontier instead of trading accuracy for language consistency?</h2>
<p><strong>Design.</strong>
Plot Accuracy (x-axis) vs Following (y-axis) for each regime (4-panel Pareto figure).
Then, inspect bar/line panels per dataset and language.</p>
<h3 id="what-we-see">What we see.</h3>
<ul>
<li><strong>SFT shifts points up</strong> (Following‚Üë).
On some hard sets, accuracy dips slightly.</li>
<li><strong>GRPO-SFT shifts rightward</strong> (Accuracy‚Üë) with at most a small upward loss, compared with SFT-only ‚Äî
<strong>creating new frontiers on:</strong>
<ul>
<li><strong>MMLU-Math (JA/FR/ES):</strong> both metrics are high.</li>
<li><strong>GPQA-ES:</strong> strong frontier point.</li>
</ul>
</li>
<li><strong>Non-frontier holdouts:</strong> Pro-Med FR/JA and AIME-ES, where domain/reward mismatch persists.</li>
</ul>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/63600b93d9e4214b9a67807c/DEIPHxXq7dR_1qgq40QAh.png" alt="RQ4 Results"></p>
<p><strong>Bottom line.</strong>
Read each plot within the same language marker (Japanese ‚ñ≤, French ‚ñ†, Spanish ‚óè) and compare colors:</p>
<ul>
<li><strong>yellow vs. blue</strong> = GRPO-from-SFT vs. Base</li>
<li><strong>green vs. blue</strong> = SFT vs. Base</li>
</ul>
<p>Under this pairing:</p>
<!-- raw HTML omitted -->
<p>SFT (green) vs. Base (blue) generally shifts points up/right, and <strong>GRPO-from-SFT most often traces the upper-right envelope</strong> when strict dominance does occur.</p>
<hr>
<h2 id="rq5--does-model-merging-help">RQ5 ‚Äî Does model merging help?</h2>
<p><strong>Motivation.</strong>
GRPO+SFT often peaks on math but can regress on knowledge-heavy sets (e.g., Pro Medicine),
and SFT alone doesn‚Äôt consistently stabilize accuracy across Japanese/French/Spanish.</p>
<p>Ideally, we want a solution that smooths these trade-offs while <strong>keeping language-consistency strong</strong>.
Previous studies have shown that model merging is a promising approach to combine models‚Äô abilities,
albeit with some performance degradation <!-- raw HTML omitted -->11<!-- raw HTML omitted -->.</p>
<p>Here, we merged the base model with the other three SFT models using <code>merge-kit</code> (<a href="https://github.com/arcee-ai/mergekit">https://github.com/arcee-ai/mergekit</a>) with equal linear merge.</p>
<!-- raw HTML omitted -->
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/63600b93d9e4214b9a67807c/a63yTSTSWhGOFZiCZ8Qnq.png" alt="RQ5 Results - Figure 5.b"></p>
<h3 id="result-avg-pattern-across-datasets">Result (avg pattern across datasets)</h3>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/63600b93d9e4214b9a67807c/EUu76drzJYO2_ZxW99zSB.png" alt="RQ5 Results - Figure 5.a"></p>
<p><strong>MERGE consistently shrinks worst-case losses and raises floor performance</strong>, especially where SFT/GRPO dip.
On Pro Medicine, MERGE recovers large chunks of accuracy for Japanese/French
(e.g., JA pass@10 climbs from SFT/GRPO‚Äôs ~47‚Äì58% to ~70%; FR from ~47‚Äì70% to ~76%),
while staying competitive on AIME/GPQA and within a few points of GRPO+SFT on MMLU-Math.</p>
<p>In Spanish, where SFT already leads on Medicine, MERGE lands in the middle of Base vs SFT/GRPO+SFT
rather than decreasing performance to base.</p>
<p>Overall, it trades a small slice of peak scores for <strong>lower variance across languages and tasks.</strong></p>
<h3 id="interpretation">Interpretation</h3>
<p>Parameter-space interpolation acts like an ensemble/regularizer:</p>
<ul>
<li>MERGE <strong>blends GRPO‚Äôs strong multi-step heuristics</strong> with <strong>SFT‚Äôs alignment priors</strong></li>
<li>Dampens overfitting to any single regime</li>
<li><strong>Stabilizes cross-lingual behavior</strong></li>
</ul>
<p>Practically, it expresses a steering effect:</p>
<!-- raw HTML omitted -->
<ul>
<li>the <strong>highest leaderboard peak</strong>, pick <strong>GRPO+SFT</strong></li>
<li><strong>reliable, in-language reasoning across JA/FR/ES</strong>, especially on domain-heavy sets, pick <strong>MERGE</strong></li>
</ul>
<!-- raw HTML omitted -->
<hr>
<h2 id="discussion-where-performance-regresses-and-potential-solutions">Discussion: Where performance regresses, and potential solutions</h2>
<p><strong>Empirical signal.</strong>
After SFT followed by GRPO, Japanese language consistency improves markedly, but accuracy lags French (e.g., AIME-JA pass@1 <strong>4.4 ‚Üí 17.9</strong>, pass@10 <strong>22.6 ‚Üí 34.4</strong>;
AIME-FR pass@1 <strong>22.2 ‚Üí 27.3</strong>, pass@10 <strong>46.3 ‚Üí 48.2</strong>), indicating Japanese-specific friction even with its high increase.</p>
<p>Spanish on AIME shows the opposite tension: the <strong>Base</strong> model scores well because it always reasons in English despite Spanish prompts, while <strong>SFT+GRPO enforces Spanish chains and accuracy drops</strong>.</p>
<p>In Pro-Medicine, <strong>math-only GRPO from SFT causes regression</strong> (e.g.,
FR pass@10 <strong>70.1 ‚Üí 46.6</strong>, ES <strong>86.6 ‚Üí 76.6</strong>, JA <strong>75.9 ‚Üí 58.3</strong>),
whereas GRPO started from Base hurts less.</p>
<h3 id="mechanisms">Mechanisms</h3>
<ol>
<li><strong>Language-prior competition.</strong>
The model‚Äôs strongest <em>reasoning prior</em> is in EN/ZH.
Under difficulty, chains drift toward those priors.
SFT+GRPO strengthens language consistency, which <strong>reduces access to English-anchored reasoning traces</strong> that previously helped (e.g., AIME-ES).
‚Üí evidenced by the huge language consistency bump.</li>
<li><strong>Tokenizer &amp; formatting tax (Japanese &gt; French / Spanish).</strong>
Mixed scripts, half/full-width digits, unit variants, and thousand separators inflate perplexity on numeric steps‚Äîprecisely where accuracy is most sensitive.</li>
<li><strong>Cue misalignment in Spanish math.</strong>
AIME leans on algebra/number-theory ‚Äúrecipes‚Äù the model learned primarily in English
(phrases like ‚Äúlet x be,‚Äù ‚Äúgcd,‚Äù ‚Äúmod‚Äù).
Spanish equivalents (‚Äúsea x,‚Äù ‚Äúmcd,‚Äù ‚Äúm√≥dulo‚Äù) are rarer, longer, more accented ‚Üí
model drifts into slower or incorrect approaches mid-solution.</li>
<li><strong>Reward misspecification in medicine.</strong>
Math-only RL optimizes numeric correctness, <strong>not</strong> biomedical recall, calibration, or evidence style.
The policy over-indexes math heuristics and becomes <strong>over-assertive</strong> on clinical QA.</li>
<li><strong>Starting-point effect.</strong>
RL from SFT pushes the policy toward SFT‚Äôs language/style anchors and away from neutral reasoning.
On medicine, this causes bigger drops.
RL from Base is more neutral; regressions are smaller.</li>
</ol>
<h3 id="lightweight-fixes-that-may-work-across-cases">Lightweight fixes that may work across cases</h3>
<ol>
<li><strong>Prompt-level normalization (before more training).</strong>
<ul>
<li><em>Japanese:</em> unify to half-width digits/decimals/exp notation; no thousand separators;
explicit math chain template in Japanese.
Example: <code>Êï∞Â≠ó„ÅØÂçäËßí‚Ä¶ SI „Çí‰ΩøÁî®„Åó‚Ä¶</code>.</li>
<li><em>Spanish:</em> prefer <code>gcd / lcm / mod</code>, exponent notation, half-width digits;
terse step headers (<code>Definimos / Sustituimos / Comprobaci√≥n / Respuesta</code>).</li>
</ul>
</li>
<li><strong>Tokenizer-aware formatting.</strong>
Consistent spacing around numerals/operators; avoid formatting that fragments tokens.</li>
<li><strong>Targeted SFT top-ups.</strong>
Small, math-dense Japanese/Spanish datasets using normalized templates to reinforce per-language priors.</li>
<li><strong>Reward shaping for GRPO.</strong>
<ul>
<li>For <strong>AIME-ES:</strong> up-weight <em>correctness</em> and make <strong>‚ÄúSpanish-only chain‚Äù</strong> a secondary objective.
‚Üí nudges reasoning into Spanish <strong>without punishing English-anchored correct answers</strong>.</li>
<li>For <strong>Medicine:</strong> add a <strong>tiny medical reward head</strong>
(terminology fidelity, claim calibration, evidence cues),
plus a <strong>KL / behavior-cloning regularizer</strong> toward medical SFT to preserve discourse style.</li>
<li>Use <strong>mixed-objective batches</strong> (math + clinical QA),
and replay OOD medical exemplars during RL to avoid domain forgetting.</li>
</ul>
</li>
</ol>
<h3 id="takeaway">Takeaway</h3>
<p>The regressions likely stem from one cause:</p>
<!-- raw HTML omitted -->
<hr>
<h2 id="blog-summary--practical-takeaways">Blog Summary ‚Äî Practical takeaways</h2>
<ol>
<li><strong>If you can only afford one step, do SFT (a few hundred high-quality SFT data).</strong>
You‚Äôll almost certainly fix language-consistency without compromising accuracy;
you might also get accuracy improvements on in-domain tasks.</li>
<li><strong>If you can afford two steps, do SFT ‚Üí GRPO-SFT.</strong>
Use <strong>high clip / no KL</strong>; keep rollouts moderate; verify you haven‚Äôt regressed following.</li>
<li>Another really practical yet computationally efficient approach is <strong>model merging among your SFT models</strong>.</li>
<li><strong>For medicine or other narrative-dense domains, add a tiny domain reward with in-domain data or a dozens-scale domain SFT.</strong></li>
<li><strong>For Japanese (or any non-latin script), include numeric/style templates</strong>
and optionally patch tokenization via formatting.</li>
<li><strong>Track Pareto, not single metrics.</strong>
Always plot <em>(Accuracy, Following)</em> together; real wins move you <strong>up-and-right</strong>.</li>
</ol>
<hr>
<h2 id="limitations--threats-to-validity">Limitations &amp; threats to validity</h2>
<ul>
<li><strong>Dataset scope.</strong>
We use four well-known benchmarks; real-world prompts are noisier.</li>
<li><strong>Reward misspecification.</strong>
Math-only RL can hurt non-math; our suggested fixes mitigate but don‚Äôt prove generality across all medical subspecialities.</li>
<li><strong>Model prior.</strong>
EN/ZH dominance shapes outcomes. A different base prior (e.g., EU-centric) could change which languages are hardest.</li>
<li><strong>Language consistency metric.</strong>
Strong, script-aware, but still an automatic proxy; human raters may be stricter.</li>
</ul>
<hr>
<h3 id="our-works-and-resources-related-to-this-blog">Our works and resources related to this blog:</h3>
<p><a href="https://huggingface.co/collections/shanchen/xreasoning">https://huggingface.co/collections/shanchen/xreasoning</a></p>
<p>bibtex
@inproceedings{qi-etal-2025-models,
title = &ldquo;When Models Reason in Your Language: Controlling Thinking Language Comes at the Cost of Accuracy&rdquo;,
author = &ldquo;Qi, Jirui  and
Chen, Shan  and
Xiong, Zidi  and
Fern{'a}ndez, Raquel  and
Bitterman, Danielle  and
Bisazza, Arianna&rdquo;,
editor = &ldquo;Christodoulopoulos, Christos  and
Chakraborty, Tanmoy  and
Rose, Carolyn  and
Peng, Violet&rdquo;,
booktitle = &ldquo;Findings of the Association for Computational Linguistics: EMNLP 2025&rdquo;,
month = nov,
year = &ldquo;2025&rdquo;,
address = &ldquo;Suzhou, China&rdquo;,
publisher = &ldquo;Association for Computational Linguistics&rdquo;,
url = &ldquo;<a href="https://aclanthology.org/2025.findings-emnlp.1103/%22">https://aclanthology.org/2025.findings-emnlp.1103/&quot;</a>,
pages = &ldquo;20279&ndash;20296&rdquo;,
ISBN = &ldquo;979-8-89176-335-7&rdquo;,
}</p>
<p>@misc{chen2025modelsreasonlanguagecontrollingtraining,
title={Budget Alignment: Making Models Reason in the User‚Äôs Language},
author={Shan Chen and Jirui Qi and Zidi Xiong and and Timothy Miller and Arianna Bisazza and Raquel Fern√°ndez and Danielle S. Bitterman},
year={2025},
archivePrefix={github},
url={https://github.com/Betswish/mCoT-XReasoning/tree/main/training},
}</p>
<h3 id="link-b√†i-vi·∫øt-g·ªëc"><a href="https://huggingface.co/blog/shanchen/mcot-rl">Link b√†i vi·∫øt g·ªëc</a></h3>


    <ul id="taxonomy" class="not-prose flex items-center space-x-4 flex-nowrap whitespace-nowrap overflow-x-auto">

      <li class="font-semibold my-4">Tags:</li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/ai/">Ai</a></li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/6-ng%C3%A0y-tr%C6%B0%E1%BB%9Bc/">6 Ng√†y Tr∆∞·ªõc</a></li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/huggingface.co/">Huggingface.co</a></li>
    </ul>
</div>

    <footer id="content-footer" class="not-prose">

      

      <div id="author-box" class="my-8 md:my-14 border p-8 bg-zinc-100 rounded-2xl">

        <img class="w-20 h-20 rounded-full" src="" alt="">

        <h2 class="text-xl font-bold my-4">Writter by : </h2>

        <p class="mb-4"></p>

        <ul class="flex flex-wrap space-x-4">
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://facebook.com/luat.skyai/" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                <path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951z"/>
              </svg>
              <span class="ml-2">Facebook</span>
            </a>
          </li>
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://facebook.com/luat.skyai/" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                  <path d="M12.6.75h2.454l-5.36 6.142L16 15.25h-4.937l-3.867-5.07-4.425 5.07H.316l5.733-6.57L0 .75h5.063l3.495 4.633L12.601.75Zm-.86 13.028h1.36L4.323 2.145H2.865l8.875 11.633Z"/>
              </svg>
              <span class="ml-2">Twitter</span>
            </a>
          </li>
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://luat.skyai.vn/" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                    <path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.917 3.917 0 0 0-1.417.923A3.927 3.927 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.916 3.916 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.926 3.926 0 0 0-.923-1.417A3.911 3.911 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0h.003zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599.28.28.453.546.598.92.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.47 2.47 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.478 2.478 0 0 1-.92-.598 2.48 2.48 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233 0-2.136.008-2.388.046-3.231.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92.28-.28.546-.453.92-.598.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045v.002zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92zm-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217zm0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334z"/>

              </svg>
              <span class="ml-2">Instagram</span>
            </a>
          </li>

        </ul>
        
      </div>

      <div id="related-post" class="">

        <h2 class="text-xl md:text-2xl font-bold mb-6 md:mb-8">Recommended for You</h2><div class="grid grid-cols-1 md:grid-cols-2 gap-6"><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl">
	
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-11-10/d6e9ef/"></a>

	<figure class="w-full aspect-video overflow-hidden rounded-3xl">

			<img class="object-cover group-hover:scale-105 transition duration-500 cursor-pointer" src="https://cdn-avatars.huggingface.co/v1/production/uploads/667d6f513a685cd30f542fef/ipB8pJRbVIK2FJOOEGUjC.png" alt="B·∫£ng x·∫øp h·∫°ng so s√°nh- AI - T·ª´ l∆∞·ª£t b√¨nh ch·ªçn c·ªßa ng∆∞·ªùi d√πng ƒë·∫øn b·∫£ng x·∫øp h·∫°ng m√¥ h√¨nh d·ª±a tr√™n s·ª± tham gia" style="width: 100%; height: 100%; object-fit: cover;">

	  </figure>

	<div class="p-6">

		<time datetime="2025-11-10T00:00:00&#43;00:00">Nov 10, 2025</time>

		<h3 class="my-4 text-2xl font-bold">B·∫£ng x·∫øp h·∫°ng so s√°nh- AI - T·ª´ l∆∞·ª£t b√¨nh ch·ªçn c·ªßa ng∆∞·ªùi d√πng ƒë·∫øn b·∫£ng x·∫øp h·∫°ng m√¥ h√¨nh d·ª±a tr√™n s·ª± tham gia</h3>

		<p class="text-normal leading-normal text-zinc-500 line-clamp-2">B·∫£ng x·∫øp h·∫°ng so s√°nh- AI - T·ª´ l∆∞·ª£t b√¨nh ch·ªçn c·ªßa ng∆∞·ªùi d√πng ƒë·∫øn b·∫£ng x·∫øp h·∫°ng m√¥ h√¨nh d·ª±a tr√™n s·ª± tham gia</p>

	</div>
</article><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl">
	
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-11-10/51f15f/"></a>

	<figure class="w-full aspect-video overflow-hidden rounded-3xl">

			<img class="object-cover group-hover:scale-105 transition duration-500 cursor-pointer" src="https://cdn-avatars.huggingface.co/v1/production/uploads/660fefd0e219d7297f3f85b1/DxJqJ90SsZ5dHmvq0pVyQ.jpeg" alt="Ch·∫°y c√°c M√¥ h√¨nh Transformer L·ªõn tr√™n Thi·∫øt b·ªã Di ƒë·ªông v√† Bi√™n" style="width: 100%; height: 100%; object-fit: cover;">

	  </figure>

	<div class="p-6">

		<time datetime="2025-11-10T00:00:00&#43;00:00">Nov 10, 2025</time>

		<h3 class="my-4 text-2xl font-bold">Ch·∫°y c√°c M√¥ h√¨nh Transformer L·ªõn tr√™n Thi·∫øt b·ªã Di ƒë·ªông v√† Bi√™n</h3>

		<p class="text-normal leading-normal text-zinc-500 line-clamp-2">Ch·∫°y c√°c M√¥ h√¨nh Transformer L·ªõn tr√™n Thi·∫øt b·ªã Di ƒë·ªông v√† Bi√™n</p>

	</div>
</article>
          
        </div>
      </div>

    </footer>

  </article>

  
  <aside class="md:col-span-1"><div class="lg:sticky lg:top-8"><div class="rounded-2xl p-4 bg-zinc-100 mb-10">

		<img class="aspect-video rounded" src="/images/email-3249062_1280_by-Muhammad-Ribkhan-Pixabay.jpg" alt="Banner Email Marketing">

		<p class="text-right text-xs mt-2 leading-none text-zinc-500">Adversitiment</p>

</div><div class="space-y-6">

		<h2 class="font-bold text-xl mb-8">Recent Post</h2>

		<article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2026-01-31/3f2e00/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1583856921041-5dd96eb166059660ed1ee413.png" alt="C√°c L·ª±a ch·ªçn Ki·∫øn tr√∫c trong H·ªá sinh th√°i AI Ngu·ªìn m·ªü c·ªßa Trung Qu·ªëc- X√¢y d·ª±ng V∆∞·ª£t ra ngo√†i DeepSeek" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">C√°c L·ª±a ch·ªçn Ki·∫øn tr√∫c trong H·ªá sinh th√°i AI Ngu·ªìn m·ªü c·ªßa Trung Qu·ªëc- X√¢y d·ª±ng V∆∞·ª£t ra ngo√†i DeepSeek</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2026-01-31/3fbf92/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="http://lh3.googleusercontent.com/RVgTAOz2sApbCGYBZlO4WDgBPJ4Fi-_CtyCIw01txeqw7vpd9VPMn_qWu2sKfA5-eSD3nQYtHNjiP9TAFFEo22lmZ5Azi5V22KgwBANKiI6unMa2Xg=w352-h352-n-nu" alt="C√°ch ch√∫ng t√¥i mang x√°c minh h√¨nh ·∫£nh AI ƒë·∫øn ·ª©ng d·ª•ng Gemini" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">C√°ch ch√∫ng t√¥i mang x√°c minh h√¨nh ·∫£nh AI ƒë·∫øn ·ª©ng d·ª•ng Gemini</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2026-01-31/889d73/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="http://lh3.googleusercontent.com/bDhAGgYcQMPjS1lEQZaupLkx9A4eIfwnXDmaJntvF_VmThk3Rrwo2-xndepttQGtlC_ng4C0zGEmuy67rz4c9xT6ROFIN0s0-qaD1SP96AIXnbHK0Gk=w352-h352-n-nu" alt="C·∫£i thi·ªán c√°c m√¥ h√¨nh √¢m thanh Gemini cho tr·∫£i nghi·ªám gi·ªçng n√≥i m·∫°nh m·∫Ω" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">C·∫£i thi·ªán c√°c m√¥ h√¨nh √¢m thanh Gemini cho tr·∫£i nghi·ªám gi·ªçng n√≥i m·∫°nh m·∫Ω</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2026-01-31/2f80ef/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://cdn-avatars.huggingface.co/v1/production/uploads/62d648291fa3e4e7ae3fa6e8/oatOwf8Xqe5eDbCSuYqCd.png" alt="Ch√∫ng t√¥i ƒë√£ khi·∫øn Claude x√¢y d·ª±ng c√°c Kernel CUDA v√† d·∫°y c√°c m√¥ h√¨nh m·ªü!" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Ch√∫ng t√¥i ƒë√£ khi·∫øn Claude x√¢y d·ª±ng c√°c Kernel CUDA v√† d·∫°y c√°c m√¥ h√¨nh m·ªü!</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2026-01-31/908108/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://deepmind.google" alt="D·ª± √°n Genie- Th·ª≠ nghi·ªám v·ªõi th·∫ø gi·ªõi t∆∞∆°ng t√°c v√¥ h·∫°n" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">D·ª± √°n Genie- Th·ª≠ nghi·ªám v·ªõi th·∫ø gi·ªõi t∆∞∆°ng t√°c v√¥ h·∫°n</h3>
	</div>
</article>
		
	</div>

</div>
</aside>

</div>
  
</main>
  <footer class="bg-zinc-100 py-10 md:py-14"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8">

  <div class="flex flex-wrap space-y-6 mb-4">

    <div class="w-full md:w-3/5 flex flex-col space-y-4 md:pr-8 lg:pr-10">

      <a class="flex items-center group" href="https://aitoday.skyai.vn/">
        <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" fill="currentColor" class="mr-2 group-hover:animate-spin" viewBox="0 0 16 16">
          <path d="M8 11a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/>
          <path d="M13.997 5.17a5 5 0 0 0-8.101-4.09A5 5 0 0 0 1.28 9.342a5 5 0 0 0 8.336 5.109 3.5 3.5 0 0 0 5.201-4.065 3.001 3.001 0 0 0-.822-5.216zm-1-.034a1 1 0 0 0 .668.977 2.001 2.001 0 0 1 .547 3.478 1 1 0 0 0-.341 1.113 2.5 2.5 0 0 1-3.715 2.905 1 1 0 0 0-1.262.152 4 4 0 0 1-6.67-4.087 1 1 0 0 0-.2-1 4 4 0 0 1 3.693-6.61 1 1 0 0 0 .8-.2 4 4 0 0 1 6.48 3.273z"/>
        </svg>
        
        <span class="text-2xl font-semibold uppercase">AI Today - SkyAI</span>
      </a>

      <p class="font-semibold">
        Theo d√µi c√°c ti·∫øn b·ªô m·ªõi nh·∫•t v·ªÅ Tr√≠ tu·ªá nh√¢n t·∫°o.<br>Tr·ª±c ti·∫øp t·ª´ c√°c nh√† ph√°t h√†nh AI tr√™n th·∫ø gi·ªõi.
      </p>

      <p>ƒêem tr√≠ tu·ªá nh√¢n t·∫°o ƒë·∫øn m·ªçi ng∆∞·ªùi d√¢n, doanh nghi·ªáp Vi·ªát, g√≥p ph·∫ßn gi√∫p Vi·ªát Nam ph√°t tri·ªÉn m·∫°nh m·∫Ω trong k·ª∑ nguy√™n s·ªë. N·ªôi dung ƒë∆∞·ª£c c·∫≠p nh·∫≠t t·ª± ƒë·ªông b·∫±ng m√°y.</p>

    </div>

    <div class="self-center flex flex-col w-full md:w-2/5">

<ul id="social-media" class="flex items-center space-x-4">
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.facebook.com/luat.skyai/" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 16 16">
				<path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.twitter.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M12.6.75h2.454l-5.36 6.142L16 15.25h-4.937l-3.867-5.07-4.425 5.07H.316l5.733-6.57L0 .75h5.063l3.495 4.633L12.601.75Zm-.86 13.028h1.36L4.323 2.145H2.865l8.875 11.633Z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.github.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.instagram.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.917 3.917 0 0 0-1.417.923A3.927 3.927 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.916 3.916 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.926 3.926 0 0 0-.923-1.417A3.911 3.911 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0h.003zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599.28.28.453.546.598.92.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.47 2.47 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.478 2.478 0 0 1-.92-.598 2.48 2.48 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233 0-2.136.008-2.388.046-3.231.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92.28-.28.546-.453.92-.598.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045v.002zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92zm-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217zm0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334z"></path>
			</svg>
		</a>
	</li>
</ul>

    </div>
    
  </div>

  <div class="my-8">
    <ul class="flex items-center space-x-4">
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/">Home</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="">About</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/tags/">Tags</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/tags/">Taxonomy</a></li>
      
    </ul>
  </div>

  <div class="border-t pt-4">

    <p class="text-sm">Copyright ¬© 2026. All rights reserved. <a href="https://home.skyai.vn/" target="_blank" rel="noopener">SkyAI.vn</a></p>

  </div>
  
</div>
</footer>
      <script defer src="/js/insertoggle.474f9b0e08021c6519cff4e46df14ccf148285b2d3a23d6321d6e10f25c291fb.js" integrity="sha256-R0&#43;bDggCHGUZz/TkbfFMzxSChbLToj1jIdbhDyXCkfs=" crossorigin="anonymous"></script>
</body>
</html>