<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Chạy các Mô hình Transformer Lớn trên Thiết bị Di động và Biên | AI Today - SkyAI</title>

<meta name="description" content="Chạy các Mô hình Transformer Lớn trên Thiết bị Di động và Biên">
      <link rel="stylesheet" href="/css/main.min.584aed796ba9d245f84009d57da9662e9c02059611972780263728b442fc25ff.css" integrity="sha256-WErteWup0kX4QAnVfalmLpwCBZYRlyeAJjcotEL8Jf8=" crossorigin="anonymous">

<link rel="icon" type="image/svg+xml" href="https://aitoday.skyai.vn/favicon.svg"> 
<link rel="icon" type="image/x-icon" href="https://aitoday.skyai.vn/favicon.ico"> 
<link rel="icon" type="image/png" sizes="16x16" href="https://aitoday.skyai.vn/favicon.png"> 
<link rel="icon" type="image/png" sizes="32x32" href="https://aitoday.skyai.vn/favicon-32.png"> 
<link rel="icon" type="image/png" sizes="64x64" href="https://aitoday.skyai.vn/favicon-64.png"> 

</head>
<body>
  <header class="py-6 border-b"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8 flex flex-col">
	<div class="flex items-center">
		<div class="flex items-center">
			<button class="flex items-center space-x-2 rounded-full border py-1 pr-[5px] pl-3 group bg-zinc-100 hover:bg-zinc-200 toggle-button" data-target="menu-bar">
				<svg width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
					<path fill-rule="evenodd" d="M2.5 12a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5zm0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5zm0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5z"></path>
				</svg>
				<span class="bg-blue-500 fill-white rounded-full p-1.5">
					<svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="group-hover:animate-spin bi bi-egg-fried fill-white" viewBox="0 0 16 16">
						<path d="M8 11a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"></path>
						<path d="M13.997 5.17a5 5 0 0 0-8.101-4.09A5 5 0 0 0 1.28 9.342a5 5 0 0 0 8.336 5.109 3.5 3.5 0 0 0 5.201-4.065 3.001 3.001 0 0 0-.822-5.216zm-1-.034a1 1 0 0 0 .668.977 2.001 2.001 0 0 1 .547 3.478 1 1 0 0 0-.341 1.113 2.5 2.5 0 0 1-3.715 2.905 1 1 0 0 0-1.262.152 4 4 0 0 1-6.67-4.087 1 1 0 0 0-.2-1 4 4 0 0 1 3.693-6.61 1 1 0 0 0 .8-.2 4 4 0 0 1 6.48 3.273z"></path>
					</svg>
				</span>
			</button>
			<div class="relative rounded-full py-1.5 px-6 bg-zinc-100 hover:bg-zinc-200 text-xl font-bold uppercase mx-2">
				<h2><a class="before:content-[''] before:z-10 before:top-0 before:right-0 before:left-0 before:bottom-0 before:absolute before:pointer-events-auto" href="https://aitoday.skyai.vn/">AI Today - SkyAI</a></h2>
			</div>
		</div>
		<div class="flex items-center ml-auto">
			<button class="flex items-center rounded-full p-3 bg-zinc-100 hover:bg-zinc-200 toggle-button" data-target="search-bar">
				<svg width="18" height="18" fill="currentColor" viewBox="0 0 16 16">
					<path id="path1" class="transition-all ease-linear" d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path>
					<path id="path2" class="transition-all ease-linear hidden" d="M2.146 2.854a.5.5 0 1 1 .708-.708L8 7.293l5.146-5.147a.5.5 0 0 1 .708.708L8.707 8l5.147 5.146a.5.5 0 0 1-.708.708L8 8.707l-5.146 5.147a.5.5 0 0 1-.708-.708L7.293 8 2.146 2.854Z"></path>
				</svg>
			</button>
		</div>
	</div>
  <nav id="menu-bar" class="block mt-3 close">
    <ul class="flex items-center flex flex-nowrap whitespace-nowrap overflow-x-auto space-x-4">
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/">Home</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200">About</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/tags/">Tags</a>
    </li>
    <li class="my-2">
      <a class="rounded-full border px-6 py-2 bg-zinc-100 hover:bg-zinc-200" href="/tags/">Taxonomy</a>
    </li>
    </ul>
  </nav>
<div id="search-bar" class="block mt-3 close">
	<form id="search" class="flex items-stretch">
		<input class="w-full block px-6 py-2 rounded-l-full focus:outline-none border border-zinc-200" type="text" placeholder="Search...">
		<button class="flex items-center px-7 py-2.5 rounded-r-full border border-zinc-200 ">
			<svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="group-hover:animate-pulse" viewBox="0 0 16 16">
				<path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path>
			</svg>
		</button>
	</form>
</div>
</div></header>
  <main class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8"><div id="breadcrumb" class="max-w-7xl mx-auto py-8">
	<ul class="flex space-x-4 text-sm text-zinc-500">
		<li class="after:content-['❯'] after:ml-4 after:opacity-30 last:after:content-none uppercase">
			<a href="https://aitoday.skyai.vn/">AI Today - SkyAI</a>
		</li>
		<li class="after:content-['❯'] after:ml-4 after:opacity-30 last:after:content-none uppercase">
			<a href="https://aitoday.skyai.vn/posts/">Posts</a>
		</li>
	</ul>
</div><div class="grid grid-cols-1 md:grid-cols-3 gap-6 mb-14">

  <article class="md:col-span-2 prose lg:prose-lg">

    <header class="not-prose">
      
      <h1 id="title" class="text-4xl font-bold leading-normal">Chạy các Mô hình Transformer Lớn trên Thiết bị Di động và Biên</h1>

      <div id="lead" class="my-6">

        <p class="font-bold">Chạy các Mô hình Transformer Lớn trên Thiết bị Di động và Biên </p>

      </div>
      
      <div id="writer" class="flex items-center space-x-4"><ul class="flex items-center space-x-4 flex-nowrap whitespace-nowrap overflow-x-auto">
          
          <li class="font-semibold my-2"></li>
          
          <li class="before:content-['•'] before:mr-2 before:opacity-50 my-2"><time datetime="2025-11-10T00:00:00&#43;00:00">November 10, 2025</time>
          </li>

          <li class="before:content-['•'] before:mr-2 before:opacity-50 my-2">
            40 min read
          </li>

        </ul>

      </div>
      
    </header>

    <figure id="featureimage" class="rounded-xl aspect-video">

          <img class="rounded-lg" src="https://cdn-avatars.huggingface.co/v1/production/uploads/660fefd0e219d7297f3f85b1/DxJqJ90SsZ5dHmvq0pVyQ.jpeg" alt="Chạy các Mô hình Transformer Lớn trên Thiết bị Di động và Biên">
        <figcaption class="text-center italic text-xs">Chạy các Mô hình Transformer Lớn trên Thiết bị Di động và Biên</figcaption>

    </figure>

    <div id="content" class="mb-14">
      <h1 id="chạy-các-mô-hình-transformer-lớn-trên-thiết-bị-di-động-và-thiết-bị-biên">Chạy các Mô hình Transformer Lớn trên Thiết bị Di động và Thiết bị Biên</h1>
<p>Chạy các mô hình học máy trên thiết bị di động và môi trường biên đã trở thành một chủ đề quan trọng ngày càng tăng trong những năm gần đây. Đặc biệt, việc chạy các mô hình ngôn ngữ lớn dựa trên Transformer trực tiếp trên thiết bị thay vì trên đám mây mang lại những lợi thế đáng kể về <strong>quyền riêng tư</strong> và <strong>độ trễ thấp</strong>. Quyền riêng tư của người dùng được bảo vệ vì dữ liệu không bao giờ rời khỏi thiết bị và khả năng nhận phản hồi theo thời gian thực xuất hiện khi độ trễ mạng được loại bỏ. Tuy nhiên, hàng triệu tham số và yêu cầu tính toán cao của các mô hình lớn này làm cho việc chạy chúng một cách hiệu quả dưới <strong>các ràng buộc về bộ xử lý và bộ nhớ</strong> của thiết bị di động trở nên thách thức.</p>
<p>Trong bài viết này, chúng ta sẽ đề cập đến các <!-- raw HTML omitted -->kỹ thuật tối ưu hóa mô hình<!-- raw HTML omitted --> và <!-- raw HTML omitted -->công cụ<!-- raw HTML omitted --> cần thiết để chạy các mô hình Transformer lớn trên thiết bị di động và thiết bị biên. Trong hệ sinh thái <strong>Hugging Face</strong>, chúng ta sẽ xem xét các cách làm cho mô hình phù hợp với thiết bị di động (như lượng tử hóa, chưng cất kiến thức và cắt tỉa mô hình) và chi tiết các phương pháp triển khai bằng cách sử dụng các công nghệ và công cụ như <strong>ONNX</strong>, <strong>Core ML</strong> và <strong>Hugging Face Optimum</strong>. Chúng ta sẽ bao gồm các giải thích kỹ thuật và các đoạn mã để hướng dẫn các nhà phát triển di động và kỹ sư AI, cùng với các ví dụ thực tế.</p>
<h2 id="tại-sao-lại-chạy-mô-hình-trên-thiết-bị">Tại sao lại Chạy Mô hình Trên Thiết bị?</h2>
<p>Chạy mô hình trực tiếp trên thiết bị thay vì trên cơ sở hạ tầng đám mây có một số ưu điểm:</p>
<ul>
<li><strong>Quyền riêng tư:</strong> Dữ liệu người dùng không rời khỏi thiết bị, cho phép xử lý thông tin nhạy cảm mà không cần gửi lên đám mây. Ví dụ, Apple đã nhấn mạnh rằng việc chạy các mô hình ngôn ngữ lớn trên thiết bị bảo vệ quyền riêng tư của người dùng. Cách tiếp cận này rất quan trọng để đảm bảo quyền riêng tư dữ liệu trong các lĩnh vực như y tế hoặc tài chính.</li>
<li><strong>Độ trễ thấp:</strong> Đầu ra của mô hình có thể được nhận ngay lập tức mà không cần kết nối internet. Các ứng dụng có thể phản hồi theo thời gian thực vì không có lưu lượng mạng hoặc độ trễ máy chủ. Điều này đặc biệt quan trọng đối với các ứng dụng <!-- raw HTML omitted -->quan trọng về thời gian<!-- raw HTML omitted --> như thực tế tăng cường, trợ lý giọng nói hoặc thiết bị tự hành.</li>
<li><strong>Sử dụng ngoại tuyến:</strong> Các ứng dụng di động có thể tiếp tục hoạt động ngay cả khi không có kết nối internet. Ví dụ, một ứng dụng dịch thuật có thể thực hiện dịch ngoại tuyến bằng cách lưu trữ một mô hình Transformer đã được huấn luyện trước trên thiết bị.</li>
<li><strong>Chi phí và Khả năng mở rộng:</strong> Việc liên tục chạy mô hình trên máy chủ đám mây có thể tốn kém. Đối với các ứng dụng có hàng triệu người dùng, việc chạy mô hình trên thiết bị của người dùng thay vì thực hiện các yêu cầu đám mây riêng biệt cho từng khách hàng sẽ giảm chi phí máy chủ. Hơn nữa, các bộ xử lý mạnh mẽ trên thiết bị (như Apple Neural Engine, Qualcomm Hexagon DSP) thường có dung lượng rảnh rỗi – AI trên thiết bị <!-- raw HTML omitted -->sử dụng hiệu quả phần cứng hiện có này<!-- raw HTML omitted -->.</li>
</ul>
<p>Tất nhiên, chạy các mô hình lớn trên thiết bị có một số <!-- raw HTML omitted -->thách thức<!-- raw HTML omitted -->. Các trở ngại bao gồm yêu cầu bộ nhớ cao, thời lượng pin hạn chế, sức mạnh tính toán hạn chế của bộ xử lý di động và các ràng buộc về nhiệt. Dưới đây, chúng ta sẽ thảo luận về các <!-- raw HTML omitted -->kỹ thuật nén và tăng tốc mô hình<!-- raw HTML omitted --> thường được sử dụng để vượt qua những thách thức này.</p>
<h2 id="các-phương-pháp-để-thu-nhỏ-và-tăng-tốc-các-mô-hình-lớn">Các Phương pháp để Thu nhỏ và Tăng tốc các Mô hình Lớn</h2>
<p>Để làm cho một mô hình học máy lớn hoạt động hiệu quả trong môi trường di động, cần phải giảm kích thước và tải tính toán của nó. Trong số các phương pháp phổ biến nhất là <!-- raw HTML omitted -->lượng tử hóa<!-- raw HTML omitted -->, <!-- raw HTML omitted -->chưng cất kiến thức<!-- raw HTML omitted --> và <!-- raw HTML omitted -->cắt tỉa mô hình<!-- raw HTML omitted -->. Các kỹ thuật này thường có thể được sử dụng cùng nhau để đạt được việc giảm mô hình đáng kể. Dưới đây, chúng tôi sẽ trình bày chi tiết từng phương pháp dưới các tiêu đề riêng biệt.</p>
<h3 id="lượng-tử-hóa-giảm-độ-chính-xác-số-học">Lượng tử hóa (Giảm Độ chính xác Số học)</h3>
<p>Lượng tử hóa là quá trình <!-- raw HTML omitted -->giảm biểu diễn số học của trọng số và kích hoạt mô hình<!-- raw HTML omitted -->. Ví dụ, các cách tiếp cận phổ biến bao gồm sử dụng dấu phẩy động 16 bit (FP16) hoặc số nguyên 8 bit (INT8) thay vì dấu phẩy động 32 bit (FP32). Điều này làm giảm đáng kể kích thước mô hình và việc sử dụng bộ nhớ (chuyển từ FP32 sang INT8 mang lại mức giảm kích thước khoảng 4 lần). Đồng thời, các phép tính có số bit thấp có thể <!-- raw HTML omitted -->chạy nhanh hơn<!-- raw HTML omitted --> với sự hỗ trợ phần cứng thích hợp (ví dụ: các phép toán vector, bộ tăng tốc GPU hoặc NPU).</p>
<p>Lượng tử hóa có thể là <!-- raw HTML omitted -->tĩnh<!-- raw HTML omitted --> hoặc <!-- raw HTML omitted -->động<!-- raw HTML omitted -->. <!-- raw HTML omitted -->Lượng tử hóa động<!-- raw HTML omitted --> chuyển đổi trọng số sang định dạng số nguyên tại thời điểm chạy và thường chỉ lượng tử hóa các trọng số (kích hoạt vẫn có thể là FP32). Cách tiếp cận này không yêu cầu huấn luyện bổ sung và được sử dụng để tăng tốc các lớp cụ thể trong các mô hình Transformer (ví dụ: phép nhân ma trận). Trong <!-- raw HTML omitted -->lượng tử hóa tĩnh<!-- raw HTML omitted --> (với huấn luyện nhận biết lượng tử hóa hoặc hiệu chuẩn sau huấn luyện), các phạm vi kích hoạt của mô hình được đo bằng một tập dữ liệu hiệu chuẩn, và cả trọng số lẫn kích hoạt đều được <!-- raw HTML omitted -->chuyển đổi sang biểu diễn bit thấp trước<!-- raw HTML omitted -->. Lượng tử hóa tĩnh thường mang lại kết quả tốt hơn về việc bảo tồn độ chính xác nhưng phức tạp hơn một chút (nó yêu cầu hiệu chuẩn mô hình).</p>
<p>Trong môi trường Hugging Face, thư viện <strong>Optimum</strong> cung cấp các công cụ lượng tử hóa dễ sử dụng với hỗ trợ ONNX Runtime. Ví dụ, xuất một mô hình Transformers sang định dạng ONNX và áp dụng lượng tử hóa động 8 bit có thể được thực hiện chỉ với vài dòng mã. Mã sau đây minh họa việc sử dụng <strong>Hugging Face Optimum</strong> để chuyển đổi mô hình <strong>DistilBERT</strong> sang định dạng ONNX và lượng tử hóa động nó cho tập lệnh AVX512-VNNI:</p>
<p>python
from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification
from optimum.onnxruntime.configuration import AutoQuantizationConfig</p>
<h1 id="tải-một-mô-hình-pytorch-transformers-từ-hf-hub-dưới-dạng-onnx">Tải một mô hình PyTorch Transformers từ HF Hub dưới dạng ONNX</h1>
<p>onnx_model = ORTModelForSequenceClassification.from_pretrained(
&ldquo;distilbert-base-uncased-finetuned-sst-2-english&rdquo;, export=True
)</p>
<h1 id="tạo-đối-tượng-lượng-tử-hóa">Tạo đối tượng lượng tử hóa</h1>
<p>quantizer = ORTQuantizer.from_pretrained(onnx_model)</p>
<h1 id="xác-định-cấu-hình-lượng-tử-hóa-ví-dụ-cho-avx512_vnni-động-và-không-theo-kênh">Xác định cấu hình lượng tử hóa (ví dụ: cho AVX512_VNNI, động và không theo kênh)</h1>
<p>qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)</p>
<h1 id="lượng-tử-hóa-mô-hình-và-lưu-kết-quả">Lượng tử hóa mô hình và lưu kết quả</h1>
<p>quantizer.quantize(save_dir=&ldquo;quantized-model/&rdquo;, quantization_config=qconfig)</p>
<p>Kết quả của quy trình trên là tất cả các lớp được hỗ trợ của mô hình được biểu diễn dưới dạng số nguyên 8 bit trong ONNX. Thư viện <strong>Optimum</strong> trừu tượng hóa các công cụ lượng tử hóa ONNX Runtime, đơn giản hóa quy trình này. Cũng có thể làm tương tự với lệnh một dòng:</p>
<p>bash
optimum-cli onnxruntime quantize &ndash;onnx_model onnx_model_folder/ &ndash;arm64 -o onnx_quantized/</p>
<p>Cờ <!-- raw HTML omitted --><!-- raw HTML omitted -->&ndash;arm64<!-- raw HTML omitted --><!-- raw HTML omitted --> trong lệnh chọn chế độ lượng tử hóa phù hợp cho các bộ xử lý di động dựa trên ARM (ví dụ: dựa trên số nguyên 8 bit thông qua SIMD). Optimum CLI có thể tự động xác định <!-- raw HTML omitted -->cài đặt lượng tử hóa tốt nhất<!-- raw HTML omitted --> cho các mục tiêu phần cứng khác nhau như <strong>ARM64, AVX2 và AVX512</strong>. Mô hình được lượng tử hóa sẽ nhỏ hơn nhiều so với bản gốc và mang lại khả năng tăng tốc trên phần cứng được hỗ trợ. Ví dụ, lượng tử hóa 8 bit của một mô hình BERT làm giảm tệp mô hình xuống còn một phần tư kích thước, mang lại lợi thế cả về lưu trữ và bộ nhớ.</p>
<p><!-- raw HTML omitted -->Hiệu suất:<!-- raw HTML omitted --> Việc tăng hiệu suất từ lượng tử hóa phụ thuộc vào phần cứng được sử dụng. Nhiều CPU di động có các đơn vị vector được tối ưu hóa cho phép nhân ma trận số nguyên 8 bit. Apple Neural Engine hoặc DSP/NPU được hỗ trợ bởi Android NNAPI cũng mang lại sự tăng tốc đáng kể cho các phép toán 8 bit. ONNX Runtime có thể tích hợp với <strong>NNAPI</strong> hoặc <strong>XNNPACK</strong> trên Android và <strong>Core ML</strong> trên iOS, cho phép các mô hình được lượng tử hóa hưởng lợi từ phần cứng này. Nếu phần cứng không hỗ trợ trực tiếp các phép toán 8 bit, mô hình được lượng tử hóa vẫn sẽ chạy, nhưng lợi ích có thể bị hạn chế. Ngoài ra, độ sâu bit thấp có thể dẫn đến một số <!-- raw HTML omitted -->mất mát độ chính xác<!-- raw HTML omitted -->, đặc biệt là trong các mô hình rất sâu. Do đó, đối với các ứng dụng quan trọng, các giải pháp <!-- raw HTML omitted -->đa độ chính xác<!-- raw HTML omitted --> (ví dụ: một số lớp là 8 bit, lớp khác là 16 bit) hoặc cải thiện độ chính xác thông qua <!-- raw HTML omitted -->tinh chỉnh sau lượng tử hóa<!-- raw HTML omitted --> được sử dụng.</p>
<h3 id="chưng-cất-kiến-thức-knowledge-distillation">Chưng cất Kiến thức (Knowledge Distillation)</h3>
<p>Chưng cất kiến thức là một kỹ thuật nhằm mục đích chuyển giao kiến thức từ một mô hình <!-- raw HTML omitted -->giáo viên<!-- raw HTML omitted --> lớn, có khả năng cao cho một mô hình <!-- raw HTML omitted -->học sinh<!-- raw HTML omitted --> nhỏ hơn, nhanh hơn. Trong phương pháp này, mô hình học sinh được huấn luyện để bắt chước đầu ra hoặc các biểu diễn trung gian của mô hình giáo viên. Kết quả là, mô hình học sinh, mặc dù có ít tham số hơn và độ sâu ít hơn, có thể đạt được hiệu suất gần với mô hình giáo viên.</p>
<p>Chưng cất đã được áp dụng rất thành công, đặc biệt là cho các mô hình Transformer. Ví dụ, <strong>DistilBERT</strong> là một mô hình do nhóm Hugging Face tạo ra, nhỏ hơn và nhanh hơn 40% so với mô hình BERT gốc, đạt được điều này thông qua chưng cất. DistilBERT là phiên bản có khoảng 66 triệu tham số của mô hình BERT-Base và mang lại độ chính xác gần với BERT gốc. Được huấn luyện với ba hàm mất mát (mất mát ngôn ngữ, mất mát chưng cất và mất mát tương tự cosine), DistilBERT chứa 6 lớp Transformer thay vì 12 lớp của BERT, do đó giảm một nửa cả bộ nhớ và tải xử lý.</p>
<p>Tương tự, các mô hình như <strong>TinyBERT</strong> và <strong>MobileBERT</strong> cũng được chưng cất từ mô hình giáo viên BERT. <strong>MobileBERT</strong> là một mô hình được tối ưu hóa cho thiết bị di động, được Google tạo ra bằng cách sửa đổi cấu trúc của mô hình BERT lớn (thêm các lớp nút cổ chai và phép biến đổi tuyến tính). MobileBERT có khoảng 25 triệu tham số và đã được chứng minh là chạy nhanh hơn nhiều lần so với BERT-Base trên điện thoại Pixel. Một ví dụ khác, <strong>SqueezeBERT</strong>, giảm kích thước BERT bằng cách kết hợp cơ chế chú ý với cấu trúc khác và lấy cảm hứng từ các lớp tích chập; nó được báo cáo là chạy nhanh hơn 4,3 lần so với BERT-base trên điện thoại Pixel 3.</p>
<p>Ý tưởng cơ bản trong chưng cất là <!-- raw HTML omitted -->logit mềm<!-- raw HTML omitted --> hoặc <!-- raw HTML omitted -->đầu ra lớp trung gian<!-- raw HTML omitted --> của mô hình giáo viên sẽ hướng dẫn mô hình học sinh. Mô hình học sinh cố gắng học các phân phối xác suất do mô hình giáo viên dự đoán và, nếu có sẵn, các bản đồ chú ý của nó. Điều này cho phép nó học <!-- raw HTML omitted -->cách mô hình giáo viên phân tán kiến thức của mình<!-- raw HTML omitted -->, thay vì chỉ học câu trả lời đúng. Cuối cùng, mô hình học sinh đạt được độ chính xác cao hơn so với một mô hình tương đương được huấn luyện từ đầu trên cùng một dữ liệu. Ví dụ, nếu một mô hình BERT-Base đạt 90% độ chính xác, một mô hình nhỏ được huấn luyện trên cùng dữ liệu có thể chỉ đạt 85%; nhưng với chưng cất, mô hình nhỏ này có thể đạt gần 88-89% độ chính xác.</p>
<p><!-- raw HTML omitted -->Ưu điểm:<!-- raw HTML omitted --> Các mô hình được áp dụng chưng cất kiến thức thường <!-- raw HTML omitted -->nhỏ gọn hơn<!-- raw HTML omitted --> và <!-- raw HTML omitted -->nhanh hơn<!-- raw HTML omitted -->. Điều này cực kỳ quan trọng đối với các ứng dụng thời gian thực trên thiết bị di động. Một lợi ích khác của chưng cất là nếu mô hình giáo viên đã được huấn luyện trước trên một lượng lớn dữ liệu, mô hình học sinh có thể đạt được sự tổng quát hợp lý với ít dữ liệu hơn – giống như nó được hưởng lợi từ kinh nghiệm của giáo viên. Thật vậy, một giáo viên có thể được chuyển giao cho nhiều học sinh, do đó tạo ra các họ mô hình có kích thước khác nhau. Các nhà nghiên cứu của NVIDIA báo cáo rằng khi họ giảm mô hình Llama 3.1 (8 tỷ tham số) xuống các mô hình <strong>Minitron</strong> có 4 tỷ và 2,7 tỷ tham số bằng cách sử dụng chưng cất và cắt tỉa, các mô hình nhỏ hơn này hoạt động gần với bản gốc nhưng yêu cầu ít tính toán hơn nhiều. Họ cũng tuyên bố rằng chưng cất đã giảm 40 lần lượng dữ liệu cần thiết để huấn luyện mô hình 8 tỷ tham số.</p>
<p><!-- raw HTML omitted -->Những điều cần lưu ý:<!-- raw HTML omitted --> Khi áp dụng chưng cất, <!-- raw HTML omitted -->kiến trúc mô hình học sinh<!-- raw HTML omitted --> thường được xác định trước (ví dụ: một nửa số lớp, kích thước ẩn hẹp hơn, v.v.). Nếu mô hình học sinh được chọn quá nhỏ, có nguy cơ nó không thể hấp thụ hoàn toàn kiến thức của mô hình giáo viên – đây được gọi là <!-- raw HTML omitted -->khoảng cách năng lực<!-- raw HTML omitted --> trong tài liệu (nếu năng lực của học sinh không đủ để đại diện cho giáo viên, hiệu suất sẽ giảm). Do đó, trong thực tế, kích thước của mô hình học sinh phải được cân bằng với các yêu cầu của tác vụ mục tiêu. Ngoài ra, quy trình huấn luyện chưng cất có thể phức tạp hơn huấn luyện tiêu chuẩn, vì nó liên quan đến việc tối ưu hóa nhiều thuật ngữ mất mát đồng thời. Tuy nhiên, các framework hiện đại (ví dụ: hỗ trợ chưng cất trong API <strong>Trainer</strong> của Hugging Face hoặc các script huấn luyện do Tencent cung cấp cho TinyBERT) đang làm cho quy trình này dễ dàng hơn. Tóm lại, các mô hình được chưng cất mang đến giải pháp cho các ứng dụng di động <!-- raw HTML omitted -->gần như tốt như mô hình giáo viên, nhưng hiệu quả hơn nhiều<!-- raw HTML omitted -->.</p>
<h3 id="cắt-tỉa-mô-hình-model-pruning">Cắt tỉa Mô hình (Model Pruning)</h3>
<p>Cắt tỉa mô hình là quá trình làm cho mô hình nhỏ hơn và nhanh hơn bằng cách loại bỏ (thiết lập về 0) một số phần của nó. Quy trình cắt tỉa có thể là <!-- raw HTML omitted -->không có cấu trúc<!-- raw HTML omitted --> hoặc <!-- raw HTML omitted -->có cấu trúc<!-- raw HTML omitted -->.</p>
<ul>
<li><!-- raw HTML omitted -->Cắt tỉa không có cấu trúc:<!-- raw HTML omitted --> Điều này có nghĩa là <!-- raw HTML omitted -->thiết lập các trọng số có tầm quan trọng nhỏ nhất<!-- raw HTML omitted --> trong một ma trận trọng số thành 0. Ví dụ, thiết lập 20% trọng số trong một lớp thành 0 làm cho các tham số mô hình trở nên thưa thớt. Cách tiếp cận này không trực tiếp giảm kích thước tệp mô hình (số 0 vẫn chiếm không gian), nhưng nó có thể mang lại sự tăng tốc nếu các runtime đặc biệt có thể bỏ qua phép tính cho các số 0 này. Thật không may, nhiều trình biên dịch tiêu chuẩn không hiệu quả trong việc bỏ qua số 0, vì vậy việc tăng tốc từ cắt tỉa không có cấu trúc thuần túy trên thiết bị di động có thể bị hạn chế. Tuy nhiên, cắt tỉa không có cấu trúc có thể giảm tải băng thông bộ nhớ và được sử dụng như một bước tiền tối ưu hóa.</li>
<li><!-- raw HTML omitted -->Cắt tỉa có cấu trúc:<!-- raw HTML omitted --> Điều này liên quan đến việc <!-- raw HTML omitted -->loại bỏ hoàn toàn<!-- raw HTML omitted --> các phần tử nhất định khỏi cấu trúc mô hình. Ví dụ, cắt tỉa hoàn toàn một số <!-- raw HTML omitted -->đầu chú ý<!-- raw HTML omitted --> (attention heads) trong một mô hình Transformer hoặc loại bỏ một số lớp là cắt tỉa có cấu trúc. Bằng cách này, các phần bị cắt tỉa không được tính toán chút nào, và kích thước mô hình thực sự bị thu nhỏ. Ví dụ, có thể thu được một mô hình có ít đầu chú ý hơn bằng cách cắt tỉa các đầu chú ý có ý nghĩa thấp nhất từ mỗi lớp trong mô hình BERT. Cắt tỉa có cấu trúc là một tối ưu hóa nâng cao hơn và thường yêu cầu mô hình phải được <!-- raw HTML omitted -->huấn luyện lại (tinh chỉnh)<!-- raw HTML omitted --> sau khi cắt tỉa, vì việc xóa đột ngột một số neuron có thể gây ra sự sụt giảm hiệu suất đáng kể. Huấn luyện lại cho phép các phần còn lại của mô hình phục hồi và hiệu suất tăng lên.</li>
</ul>
<p>Nghiên cứu hiện tại đã chỉ ra rằng việc kết hợp cắt tỉa với chưng cất là rất hiệu quả. Trong nghiên cứu Minitron của NVIDIA đã đề cập ở trên, họ đầu tiên cắt tỉa mô hình Llama 8B dựa trên lớp và chiều rộng xuống còn 4 tỷ tham số, sau đó lấy lại hiệu suất của nó bằng cách chưng cất. Bằng cách này, mô hình đã cắt tỉa đạt được <!-- raw HTML omitted -->điểm kiểm tra tổng thể tốt hơn 16% (MMLU) so với huấn luyện ban đầu<!-- raw HTML omitted -->. Hugging Face cũng đã đạt được sự tăng tốc tương tự bằng cách sử dụng cắt tỉa khối trên mô hình BLOOM, chẳng hạn (các nghiên cứu cắt tỉa có cấu trúc GLU tồn tại). Một ứng dụng khác của cắt tỉa là cung cấp tính linh hoạt cho <!-- raw HTML omitted -->tiêu thụ tài nguyên hạn chế<!-- raw HTML omitted --> trong một mô hình: Ví dụ, bạn có thể tạo ra các phiên bản kích thước khác nhau của một mô hình (ví dụ: phiên bản 100 triệu, 50 triệu, 10 triệu tham số) từ cùng một mô hình cơ sở thông qua cắt tỉa và chọn phiên bản phù hợp dựa trên năng lực của thiết bị.</p>
<p><!-- raw HTML omitted -->Tích hợp với Optimum-Intel:<!-- raw HTML omitted --> Thư viện <!-- raw HTML omitted -->Hugging Face Optimum-Intel<!-- raw HTML omitted --> cung cấp <!-- raw HTML omitted -->cắt tỉa và lượng tử hóa tích hợp<!-- raw HTML omitted --> với các công cụ như Intel Neural Compressor (INC) và Neural Network Compression Framework (NNCF). Ví dụ, cấu trúc <strong>INCTrainer</strong> trong <strong>Optimum</strong> có thể tự động cắt tỉa trọng số mô hình tại
các bước nhất định trong quá trình huấn luyện. Một ví dụ về cấu hình được hiển thị bên dưới:</p>
<p>python
from optimum.intel.neural_compressor import INCTrainer
from neural_compressor import WeightPruningConfig</p>
<p>pruning_config = WeightPruningConfig(
pruning_type=&ldquo;magnitude&rdquo;,
target_sparsity=0.2,
pruning_scope=&ldquo;local&rdquo;,
# &hellip;các bước bắt đầu/kết thúc, v.v.
)</p>
<p>trainer = INCTrainer(
model=model,
args=TrainingArguments(&hellip;),
train_dataset=&hellip;,
pruning_config=pruning_config,
# &hellip; các tham số Trainer khác
)
trainer.train()
trainer.save_model(&ldquo;pruned_model&rdquo;)</p>
<p>Cấu hình như trên sẽ đặt các trọng số nhỏ nhất thành 0 với một tỷ lệ nhất định trong mỗi bước huấn luyện. Tuy nhiên, <!-- raw HTML omitted -->cắt tỉa dựa trên độ lớn<!-- raw HTML omitted --> (magnitude-based pruning) này, như đã thấy trong cấu hình, đặt các trọng số thành 0 theo cách &ldquo;cục bộ&rdquo; và theo tỷ lệ. Điều này không làm giảm kích thước mô hình; nó chỉ có nghĩa là 20% trọng số trở thành 0. Để kích thước mô hình thực sự thu nhỏ, các trọng số này cần được <!-- raw HTML omitted -->loại bỏ (xóa hàng/cột) sau khi đủ số lượng đã được đặt về 0<!-- raw HTML omitted -->. Có thể thực hiện <!-- raw HTML omitted -->cắt tỉa có cấu trúc<!-- raw HTML omitted --> bằng Optimum-Intel với NNCF, và phương pháp này có thể thực sự tạo ra các tệp mô hình nhỏ hơn.</p>
<p><!-- raw HTML omitted -->Điểm chính:<!-- raw HTML omitted --> Khi áp dụng cắt tỉa, người ta phải cẩn thận rằng việc cắt tỉa quá mạnh có thể làm giảm năng lực mô hình và dẫn đến giảm độ chính xác. Thông thường, tỷ lệ cắt tỉa từ 10-30% (độ thưa mục tiêu) mang lại sự tăng tốc với sự suy giảm hiệu suất nhỏ. Tỷ lệ cắt tỉa cao (ví dụ: 80-90%) chỉ khả thi nếu được bù đắp bằng chưng cất hoặc các kỹ thuật huấn luyện lại đặc biệt. Hơn nữa, lợi ích thực tế đạt được nếu cơ sở hạ tầng được sử dụng để chạy các mô hình được cắt tỉa có thể tăng tốc các phép toán ma trận thưa thớt (ví dụ: NVIDIA cuSparse hoặc <!-- raw HTML omitted -->thư viện thưa thớt<!-- raw HTML omitted --> trên CPU). Các thư viện suy luận phổ biến trên thiết bị di động chưa thực sự hiệu quả với phép nhân ma trận thưa thớt, vì vậy trong các tình huống di động thực tế, <!-- raw HTML omitted -->cắt tỉa có cấu trúc<!-- raw HTML omitted --> nên được ưu tiên. Ví dụ, <!-- raw HTML omitted -->cắt tỉa toàn bộ đầu chú ý<!-- raw HTML omitted --> hoặc <!-- raw HTML omitted -->loại bỏ toàn bộ một lớp<!-- raw HTML omitted --> đều giảm kích thước mô hình và trực tiếp có nghĩa là ít phép tính hơn trên thiết bị di động.</p>
<p>Tóm lại, bằng cách sử dụng các kỹ thuật lượng tử hóa, chưng cất và cắt tỉa cùng nhau, kích thước và thời gian chạy của một mô hình Transformer ban đầu có thể được giảm đi rất nhiều. Ví dụ, hãy lấy mô hình BERT-base: 110 triệu tham số, sử dụng ~440 MB bộ nhớ với trọng số FP32, và có lẽ nó có thể xử lý vài câu mỗi giây trên CPU di động. Giả sử chúng ta giảm mô hình xuống còn 66 triệu tham số bằng cách chưng cất (DistilBERT), chuyển sang trọng số 8 bit bằng cách lượng tử hóa (110 MB), và cắt tỉa một số đầu chú ý, để lại 60 triệu tham số hiệu quả. Mô hình kết quả có thể có kích thước dưới ~100 MB và, với thư viện di động được tối ưu hóa, có thể cung cấp phản hồi gần như thời gian thực trên thiết bị một cách thoải mái. Thật vậy, các mô hình như <strong>TinyBERT</strong> và <strong>MiniLM</strong> trong thư viện <strong>Hugging Face Transformers</strong> đã được huấn luyện với các kỹ thuật như vậy và đã trở nên phổ biến cho việc sử dụng trên thiết bị di động.</p>
<h2 id="chuyển-đổi-mô-hình-và-phương-pháp-runtime-cho-thiết-bị-di-động">Chuyển đổi Mô hình và Phương pháp Runtime cho Thiết bị Di động</h2>
<p>Sau khi thu nhỏ mô hình của chúng ta bằng các kỹ thuật tối ưu hóa hoặc chọn một mô hình nhỏ ngay từ đầu, vấn đề <!-- raw HTML omitted -->chạy (suy luận)<!-- raw HTML omitted --> nó trên nền tảng di động sẽ phát sinh. Có các định dạng mô hình và tùy chọn runtime khác nhau cho các nền tảng di động khác nhau (Android, iOS, thiết bị IoT, v.v.). Trong phần này, chúng ta sẽ mô tả các phương pháp phổ biến như <strong>ONNX</strong>, <strong>Core ML</strong> và <strong>TensorFlow Lite</strong>, và các cách chuyển đổi sang các định dạng này bằng chuỗi công cụ Hugging Face. Chúng ta cũng sẽ thảo luận về khả năng tương thích phần cứng và đặc điểm hiệu suất của các phương pháp này.</p>
<h3 id="triển-khai-độc-lập-nền-tảng-với-onnx-và-onnx-runtime">Triển khai Độc lập Nền tảng với ONNX và ONNX Runtime</h3>
<p>ONNX (Open Neural Network Exchange) là một định dạng trung gian được hỗ trợ rộng rãi cho phép trao đổi mô hình giữa các framework học máy khác nhau. Ngay cả khi các mô hình Transformer được huấn luyện trong PyTorch hoặc TensorFlow, chúng có thể được <!-- raw HTML omitted -->xuất<!-- raw HTML omitted --> sang định dạng ONNX và chạy trong nhiều môi trường khác nhau, bao gồm cả thiết bị di động. Ưu điểm của ONNX là nó có các định nghĩa chuẩn hóa cho từng phép toán. Điều này có nghĩa là khi bạn chuyển đổi mô hình của mình sang ONNX, bất kỳ cơ sở hạ tầng nào hỗ trợ nó đều có thể diễn giải mô hình đó. Ví dụ, có thể chuyển đổi một mô hình được huấn luyện bằng PyTorch sang ONNX và sử dụng nó với TensorFlow.</p>
<p>Thư viện Hugging Face Transformers có một công cụ tích hợp để chuyển đổi mô hình sang ONNX. Bạn có thể xuất một mô hình phổ biến bằng một lệnh đơn giản. Ví dụ:</p>
<p>bash
pip install transformers[onnx]  # Các phụ thuộc cần thiết cho mô-đun xuất ONNX
python -m transformers.onnx &ndash;model=distilbert-base-uncased &ndash;feature=sequence-classification onnx/</p>
<p>Lệnh trên sẽ lưu mô hình phân loại văn bản <strong>DistilBERT</strong> ở định dạng ONNX vào tệp <!-- raw HTML omitted -->onnx/model.onnx<!-- raw HTML omitted -->. Trong quá trình xuất, thư viện Transformers sử dụng các lớp <!-- raw HTML omitted -->config<!-- raw HTML omitted --> được hỗ trợ bởi mô hình tương ứng cho ONNX và cũng xác minh rằng các đầu ra được tạo ra chính xác. Nhiều kiến trúc được hỗ trợ (BERT, RoBERTa, GPT-2, T5, MobileBERT, SqueezeBERT, v.v., có các cấu hình ONNX sẵn sàng).</p>
<p>Tùy chọn phổ biến nhất để <!-- raw HTML omitted -->chạy<!-- raw HTML omitted --> một mô hình đã được chuyển đổi sang định dạng ONNX là thư viện <strong>ONNX Runtime (ORT)</strong>. Được phát triển bởi Microsoft, ORT được sử dụng để chạy hiệu quả các mô hình ONNX trong cả môi trường máy chủ và di động. ORT cung cấp một cấu trúc mô-đun được gọi là <!-- raw HTML omitted -->Execution Providers<!-- raw HTML omitted --> để sử dụng các bộ tăng tốc khác nhau trên các thiết bị khác nhau. Các bộ tăng tốc quan trọng đối với thiết bị di động là:</p>
<ul>
<li><!-- raw HTML omitted -->CPU (Mặc định):<!-- raw HTML omitted --> Chế độ chạy trên hầu hết mọi thiết bị, sử dụng các thư viện được tối ưu hóa trên CPU thuần túy (ví dụ: XNNPACK). Đây là lựa chọn đáng tin cậy cho các mô hình nhỏ và trong trường hợp có vấn đề về nhiệt.</li>
<li><!-- raw HTML omitted -->NNAPI (Android Neural Networks API):<!-- raw HTML omitted --> Giao diện do Android cung cấp để sử dụng phần cứng <!-- raw HTML omitted -->DSP, GPU hoặc NPU<!-- raw HTML omitted --> trên thiết bị Android. ONNX Runtime có thể kích hoạt chế độ NNAPI để chạy các phép toán được hỗ trợ trên các bộ tăng tốc đặc biệt của điện thoại. Điều này có thể mang lại lợi ích tăng tốc rất lớn, đặc biệt với các mô hình được lượng tử hóa (ví dụ: Qualcomm Hexagon DSP thực hiện các phép toán ma trận int8 nhanh hơn nhiều so với CPU).</li>
<li><!-- raw HTML omitted -->Core ML (iOS):<!-- raw HTML omitted --> Đối với các thiết bị iPhone/iPad, ONNX Runtime có thể sử dụng trực tiếp cơ sở hạ tầng Core ML. Điều này cho phép các đơn vị như Apple Neural Engine được sử dụng.</li>
<li><!-- raw HTML omitted -->XNNPACK:<!-- raw HTML omitted --> Đây là một thư viện tính toán CPU được tối ưu hóa có sẵn trên cả Android và iOS. Nó đặc biệt nhanh cho các phép toán <!-- raw HTML omitted -->float32 và float16<!-- raw HTML omitted --> trên CPU di động. ORT có thể tự động sử dụng XNNPACK cho các mô hình không được lượng tử hóa.</li>
</ul>
<p>Việc tích hợp mô hình ONNX vào ứng dụng di động yêu cầu thêm gói ORT tương ứng của nền tảng vào dự án và thực hiện một vài lệnh gọi API. ORT cung cấp các thư viện sẵn sàng cho Java, C++ và Swift/Obj-C. Ví dụ, trên Android, sau khi thêm gói <!-- raw HTML omitted -->.aar<!-- raw HTML omitted -->, bạn có thể tạo <!-- raw HTML omitted -->OrtEnvironment<!-- raw HTML omitted --> với giao diện Java, tải mô hình thông qua <!-- raw HTML omitted -->InferenceSession<!-- raw HTML omitted --> và chạy nó. Mã liên quan tương tự như sử dụng PyTorch hoặc TensorFlow, chỉ khác là framework. Cũng có thể lấy mô hình ONNX do Hugging Face cung cấp trực tiếp từ <strong>HF Hub</strong> vào ứng dụng; tuy nhiên, để kiểm soát kích thước ứng dụng, việc phân phối tệp mô hình cùng với ứng dụng thường được ưu tiên.</p>
<p><!-- raw HTML omitted -->Hiệu suất và Kích thước:<!-- raw HTML omitted --> Phân phối di động của ONNX Runtime khá toàn diện theo mặc định và chiếm khoảng 10-15 MB không gian (vì nó bao gồm nhiều toán tử khác nhau). Tuy nhiên, kích thước này có thể được giảm bằng cách loại bỏ các phần không cần thiết (bản dựng tối thiểu). Ví dụ, có thể giảm thư viện ORT xuống còn 4-5 MB bằng cách chỉ biên dịch các toán tử được sử dụng bởi một mô hình cụ thể. Kích thước tệp mô hình ONNX tương tự như kích thước tệp trọng số PyTorch/TensorFlow gốc; nó sẽ nhỏ hơn nếu bạn đã lượng tử hóa nó. Kết hợp các kỹ thuật tối ưu hóa chúng ta đã đề cập ở trên với ONNX rất hiệu quả: bạn có thể đạt được hiệu quả tối đa cho cả mô hình và phần cứng bằng cách chạy mô hình được chưng cất và lượng tử hóa với ONNX Runtime + NNAPI.</p>
<p>Hãy đưa ra một ví dụ kịch bản: Giả sử người dùng sẽ phân loại một văn bản ngắn trong ứng dụng. Bạn đã chuyển đổi mô hình <strong>DistilBERT</strong> sang ONNX và lượng tử hóa nó thành 8 bit. Ứng dụng Android của bạn tải mô hình ONNX này bằng ORT. Khi ứng dụng nhận đầu vào văn bản, nó trước tiên sẽ mã hóa câu bằng <!-- raw HTML omitted -->Hugging Face Tokenizer<!-- raw HTML omitted --> (ví dụ: <!-- raw HTML omitted -->AutoTokenizer<!-- raw HTML omitted -->) và chuyển đổi nó thành các mảng numpy, sau đó đưa nó cho ORT. ORT, ở chế độ nền, sử dụng NNAPI nếu có để tận dụng các bộ tăng tốc khác ngoài CPU. Kết quả là, nó nhận được kết quả phân loại trong mili giây và hiển thị cho người dùng. Quy trình này nhanh hơn và riêng tư hơn nhiều so với việc gửi lên đám mây. Không có sự khác biệt lớn so với việc sử dụng PyTorch về mặt mã, ngoại trừ việc sử dụng ORT. Thật vậy, tài liệu của Hugging Face nêu rõ: <!-- raw HTML omitted -->&ldquo;Sau khi mô hình được xuất sang định dạng ONNX, việc chạy nó trên nhiều bộ tăng tốc là rất dễ dàng – ví dụ, chúng ta có thể tải và chạy nó với ONNX Runtime&rdquo;<!-- raw HTML omitted -->. Nói cách khác, chuyển sang định dạng ONNX là chìa khóa để linh hoạt và tốc độ trên thiết bị di động.</p>
<h3 id="mô-hình-transformer-trên-thiết-bị-ios-với-core-ml">Mô hình Transformer trên Thiết bị iOS với Core ML</h3>
<p>Core ML là framework được sử dụng để chạy các mô hình học máy trong hệ sinh thái Apple (iOS, macOS, watchOS, tvOS). Nó cung cấp khả năng chạy mô hình trên chip Apple (chip dòng A, chip dòng M Mac) với hiệu suất cực kỳ cao và tiêu thụ điện năng thấp. Giống như ONNX, Core ML sử dụng các định dạng mô hình khác nhau bằng cách chuyển đổi chúng sang định dạng riêng của nó được gọi là <!-- raw HTML omitted -->.mlmodel<!-- raw HTML omitted -->. Apple tuyên bố rằng các mô hình chạy với Core ML <!-- raw HTML omitted -->tự động và hiệu quả nhất được cân bằng giữa Apple Neural Engine, GPU và CPU<!-- raw HTML omitted --> – vì vậy nhà phát triển không cần lo lắng về chi tiết phần cứng cấp thấp.</p>
<p>Để chuyển đổi mô hình PyTorch hoặc TensorFlow sang định dạng Core ML, gói Python <strong>Core ML Tools (coremltools)</strong> của Apple được sử dụng. Việc chuyển đổi thường là quy trình hai bước:</p>
<ol>
<li><!-- raw HTML omitted -->Chuyển đổi mô hình sang định dạng TorchScript (cho PyTorch):<!-- raw HTML omitted --> Các mô hình PyTorch trước tiên được chuyển đổi sang TorchScript (tệp <!-- raw HTML omitted -->.pt<!-- raw HTML omitted -->). TorchScript là một biểu diễn có thể tuần tự hóa của mô hình, độc lập với mã Python. Nó có thể được thực hiện theo hai cách: <!-- raw HTML omitted -->Truy vết<!-- raw HTML omitted --> (truy xuất đồ thị tính toán từ một ví dụ đầu vào cụ thể) hoặc <!-- raw HTML omitted -->Scripting<!-- raw HTML omitted --> (biên dịch mã mô hình ở cấp độ AST Python). Truy vết tạo ra mã nhanh và được tối ưu hóa, nhưng scripting có thể cần thiết cho các mô hình có luồng điều khiển động. Các mô hình như Transformers, có một bộ lớp cụ thể, thường có thể được chuyển đổi thành công bằng cách truy vết.</li>
<li><!-- raw HTML omitted -->Chuyển đổi mô hình TorchScript sang định dạng Core ML:<!-- raw HTML omitted --> Gói <!-- raw HTML omitted -->coremltools<!-- raw HTML omitted --> chuyển đổi mô hình TorchScript (hoặc SavedModel, ONNX, Keras) sang tệp <!-- raw HTML omitted -->.mlmodel<!-- raw HTML omitted --> bằng hàm <!-- raw HTML omitted -->ct.convert()<!-- raw HTML omitted -->. Trong bước này, các định dạng đầu vào/đầu ra của mô hình và, lý tưởng nhất, phiên bản iOS mục tiêu được chỉ định. Ví dụ:</li>
</ol>
<p>python
import coremltools as ct
traced_model = torch.jit.trace(my_model, example_input)
mlmodel = ct.convert(
traced_model,
inputs=[ct.TensorType(shape=input_shape)]
)
mlmodel.save(&ldquo;model.mlmodel&rdquo;)</p>
<p>Mã trên truy vết mô hình PyTorch và lưu mô hình Core ML. Ở đây, chúng ta đang cố định chiều của tensor đầu vào với <!-- raw HTML omitted -->input_shape<!-- raw HTML omitted -->; Core ML nói chung hoạt động với <!-- raw HTML omitted -->tensor có kích thước cố định<!-- raw HTML omitted -->. Kích thước linh hoạt có thể được chỉ định nếu cần, nhưng cố định chúng được khuyến nghị để đạt hiệu suất.</p>
<p>Có một vài điểm cần lưu ý khi chuyển đổi các mô hình dựa trên Transformer lớn sang định dạng Core ML:</p>
<ul>
<li><!-- raw HTML omitted -->Kích thước bộ nhớ:<!-- raw HTML omitted --> Core ML Tools lưu trọng số mô hình dưới dạng FP16 (Float16) theo mặc định. Điều này làm giảm một nửa kích thước mô hình (16 bit thay vì 32 bit). Ví dụ, trong bản demo của Apple, họ nói rằng khi họ chuyển đổi mô hình Llama2 8B sang Core ML, nó chiếm ~16 GB (8 tỷ tham số * 2 byte). Điều này có vẻ khá lớn; do đó, trong thực tế, các lượng tử hóa mạnh mẽ hơn như 8 bit hoặc 4 bit (ví dụ: lượng tử hóa thành 4 bit bằng thuật toán <!-- raw HTML omitted -->GPTQ<!-- raw HTML omitted --> và sau đó chuyển đổi sang Core ML) sẽ có hiệu lực. Apple nói rằng Core ML cũng hỗ trợ <!-- raw HTML omitted -->chuyển đổi nhận biết lượng tử hóa<!-- raw HTML omitted -->, nhưng đối với các mô hình lớn, kết quả tốt nhất đạt được với việc sử dụng GPU và 16 bit.</li>
<li><!-- raw HTML omitted -->Bộ nhớ đệm khóa-giá trị (KV cache):<!-- raw HTML omitted --> Trong các mô hình tạo ngôn ngữ (như GPT-2, Llama), <!-- raw HTML omitted -->bộ nhớ đệm theo đầu chú ý<!-- raw HTML omitted --> (khóa giá trị quá khứ) được sử dụng để cải thiện hiệu suất trong việc tạo token tiếp theo. Có thể khó hỗ trợ cơ chế này trong Core ML vì nó yêu cầu nhúng một cấu trúc đệ quy trong mô hình. Trong ví dụ Llama của Apple, người ta thấy rằng họ đã xuất một mô hình đơn giản hơn bằng cách <!-- raw HTML omitted -->vô hiệu hóa<!-- raw HTML omitted --> bộ nhớ đệm KV ở giai đoạn đầu (<!-- raw HTML omitted -->use_cache=False<!-- raw HTML omitted -->). Sau đó, họ thảo luận về các cách quản lý điều này trong một phiên bản được tối ưu hóa. Đó là, nếu bạn đang chuyển đổi một mô hình ngôn ngữ, việc chuyển đổi một mô hình tạo ra toàn bộ câu cùng một lúc (theo lô) dễ dàng hơn là chuyển đổi một cấu trúc tạo ra các token đơn lẻ từng bước.</li>
<li><!-- raw HTML omitted -->Khả năng tương thích phiên bản Core ML Tools:<!-- raw HTML omitted --> Điều quan trọng là sử dụng các phiên bản PyTorch và coremltools tương thích. Ví dụ, tính đến năm 2023, các phiên bản coremltools 6+ được tích hợp tốt hơn với PyTorch 2 và đã hỗ trợ hầu hết các phép toán Transformer. Tại WWDC 2023, Apple thậm chí còn công bố các gói Swift để <!-- raw HTML omitted -->chạy trực tiếp các mô hình Transformers trong Core ML<!-- raw HTML omitted --> (như Stable Diffusion Core ML, Swift Transformers). Điều này cho thấy khía cạnh Core ML đã phát triển đến mức nào.</li>
</ul>
<p><!-- raw HTML omitted -->Hiệu suất:<!-- raw HTML omitted --> Trên các thiết bị Apple, các mô hình chạy với Core ML nhận được sự tăng tốc phần cứng từ các đơn vị như <!-- raw HTML omitted -->Neural Engine<!-- raw HTML omitted --> và GPU. Theo số liệu do Apple công bố, mô hình Llama 3.1 với 8 tỷ tham số đạt tốc độ tạo khoảng 33 token/giây trên chip M1 Max. Đây là một giá trị rất ấn tượng và có thể mang lại trải nghiệm mượt mà cho LLM trên thiết bị trong sử dụng hàng ngày. Tất nhiên, hiệu suất này đạt được trong một kịch bản mà mô hình chạy ở chế độ 16 bit và sử dụng GPU. Có thể chạy một mô hình nhỏ hơn (ví dụ: 1-2 tỷ tham số, 4 bit) với tốc độ hợp lý trên iPhone – ví dụ, cộng đồng đã báo cáo rằng một mô hình trò chuyện 70 triệu tham số mang lại phản hồi gần như thời gian thực trên iPhone đời 2024. Những kết quả này cho thấy <!-- raw HTML omitted -->sức mạnh của phần cứng chuyên dụng<!-- raw HTML omitted --> trong thiết bị di động. Core ML cung cấp một cách lý tưởng để sử dụng sức mạnh này. Hugging Face đang hợp tác với Apple để xuất bản nhiều <!-- raw HTML omitted -->mô hình tương thích Core ML<!-- raw HTML omitted --> (ví dụ: các mô hình như Stable Diffusion Core ML, MobileNet Core ML được liệt kê dưới tổ chức <!-- raw HTML omitted -->apple<!-- raw HTML omitted --> trên Hugging Face Hub).</p>
<p><!-- raw HTML omitted -->Ví dụ Thực tế:<!-- raw HTML omitted --> Được biết Apple sử dụng một mô hình Transformer nhỏ trong ứng dụng <!-- raw HTML omitted -->Mail<!-- raw HTML omitted --> của mình để phát hiện thư rác. Mô hình này được nhúng trong ứng dụng ở định dạng Core ML và xác định trên thiết bị xem các email đến có phải là thư rác hay không. Tương tự, các ứng dụng bàn phím di động như <!-- raw HTML omitted -->SwiftKey<!-- raw HTML omitted --> và <!-- raw HTML omitted -->Gboard<!-- raw HTML omitted --> đã bắt đầu chạy các phiên bản nhỏ của các mô hình ngôn ngữ dựa trên Transformer (như DistilGPT-2 hoặc các LM nhỏ tùy chỉnh) trên thiết bị để dự đoán văn bản. Tất cả các ứng dụng này biên dịch và sử dụng mô hình với Core ML hoặc các framework tương tự. Từ góc độ nhà phát triển, khi bạn thêm tệp <!-- raw HTML omitted -->.mlmodel<!-- raw HTML omitted --> vào dự án Xcode của mình, một <!-- raw HTML omitted -->lớp Swift/Obj-C<!-- raw HTML omitted --> sẽ tự động được tạo ra, và bạn có thể dễ dàng cung cấp đầu vào và nhận đầu ra từ mô hình thông qua lớp này. Điều này đã làm cho học máy trong hệ sinh thái Apple trở nên rất dễ tiếp cận.</p>
<h3 id="tensorflow-lite-và-các-cách-tiếp-cận-khác">TensorFlow Lite và Các Cách Tiếp cận Khác</h3>
<p>Một phương pháp phổ biến khác trong hệ sinh thái Android là sử dụng <!-- raw HTML omitted -->TensorFlow Lite (TFLite)<!-- raw HTML omitted -->. TFLite là một giải pháp được tối ưu hóa để chạy các mô hình TensorFlow trên thiết bị di động và thiết bị nhúng, chuyển đổi mô hình thành định dạng nhị phân đặc biệt với phần mở rộng <!-- raw HTML omitted -->.tflite<!-- raw HTML omitted -->. Ưu điểm của TFLite là hỗ trợ phần cứng rộng rãi do Google phát triển và kích thước runtime nhỏ của nó (khoảng ~1-2MB). Trên nhiều thiết bị Android, TFLite có thể thực hiện tăng tốc với NNAPI hoặc trực tiếp với cơ chế GPU Delegate của riêng nó. TFLite thường được sử dụng, đặc biệt cho các tác vụ xử lý ảnh và NLP cơ bản.</p>
<p>Thư viện Hugging Face Optimum cũng hỗ trợ chuyển đổi các mô hình Transformers sang định dạng TFLite. Bạn có thể trước tiên chuyển đổi mô hình PyTorch sang TF và sau đó tạo tệp .tflite bằng lệnh <!-- raw HTML omitted -->optimum-cli export tflite<!-- raw HTML omitted -->. Ví dụ, để chuyển đổi mô hình BERT-base:</p>
<p>bash
pip install optimum[exporters-tf]  # Plugin cần thiết cho xuất TFLite
optimum-cli export tflite &ndash;model google/bert-base-uncased &ndash;sequence_length 128 bert_tflite/</p>
<p>Lệnh này lấy mô hình BERT base chính thức từ Hugging Face Hub, cố định kích thước đầu vào thành 128 và tạo ra một mô hình TFLite. Các log được hiển thị trong đầu ra cho biết nó đã được lưu thành công. Trong quá trình chuyển đổi, các tính năng lượng tử hóa sau huấn luyện của TFLite cũng có thể được sử dụng nếu mong muốn. Ví dụ, có thể giảm trọng số xuống FP16 với một tham số như <!-- raw HTML omitted -->&ndash;quantize float16<!-- raw HTML omitted --> (Optimum cũng hỗ trợ các cấu hình TFLite).</p>
<p>Sử dụng TFLite, tương tự như ONNX, dựa trên việc thêm một thư viện vào ứng dụng và cung cấp đầu vào cho mô hình. Trên Android, lớp <!-- raw HTML omitted -->TensorFlow Lite Interpreter<!-- raw HTML omitted --> thường được sử dụng; bạn tải tệp .tflite, cung cấp dữ liệu dưới dạng ByteBuffer và nhận đầu ra. Hugging Face cũng đã bắt đầu lưu trữ mô hình ở định dạng TFLite trên Hub. Ví dụ, một phiên bản lượng tử hóa .tflite của một mô hình bộ chuyển đổi câu nhỏ dựa trên <strong>MiniLM</strong> có sẵn. Điều này cho phép các nhà phát triển tải trực tiếp mô hình TFLite và tích hợp nó vào ứng dụng của họ.</p>
<p>Ngoài ra, đối với những người thích sử dụng PyTorch, có thể đề cập đến cách tiếp cận <!-- raw HTML omitted -->PyTorch Mobile<!-- raw HTML omitted --> (TorchScript). PyTorch cho phép bạn chuyển đổi mô hình của mình sang TorchScript và chạy nó trên Android/iOS với API C++. Tuy nhiên, vì runtime của PyTorch Mobile tương đối lớn (~5MB+) và hỗ trợ GPU của nó bị hạn chế (không hỗ trợ GPU trên Android, hạn chế với Metal trên iOS), ONNX/TFLite thường được ưu tiên cho các tác vụ quan trọng về hiệu suất. Tuy nhiên, những người đã sử dụng hệ sinh thái PyTorch và cần chuyển đổi mô hình của họ nhanh chóng có thể chọn con đường TorchScript. Các mô hình Hugging Face Transformers có thể được tải trực tiếp theo cách tương thích TorchScript với tham số <!-- raw HTML omitted -->.from_pretrained(&hellip;, torchscript=True)<!-- raw HTML omitted -->. Sau đó, bạn có thể lấy tệp .pt để sử dụng trên thiết bị di động bằng cách gọi <!-- raw HTML omitted -->traced_model.save(&ldquo;model.pt&rdquo;)<!-- raw HTML omitted -->.</p>
<p>Tóm lại, có nhiều tùy chọn để chạy mô hình trên thiết bị di động, và việc lựa chọn nên dựa trên nhu cầu của dự án:</p>
<ul>
<li><!-- raw HTML omitted -->ONNX Runtime:<!-- raw HTML omitted --> Độc lập với framework, linh hoạt và hiệu suất cao. Kích thước nhị phân hơi lớn hơn, nhưng bao gồm cả iOS và Android với một giải pháp duy nhất.</li>
<li><!-- raw HTML omitted -->Core ML:<!-- raw HTML omitted --> Mang lại hiệu suất tốt nhất cho iOS/macOS, dành riêng cho hệ sinh thái Apple.</li>
<li><!-- raw HTML omitted -->TensorFlow Lite:<!-- raw HTML omitted --> Phổ biến cho Android (và iOS), runtime rất nhỏ và nhanh, nhưng nói chung dễ dàng tích hợp với các mô hình dựa trên TF hơn.</li>
<li><!-- raw HTML omitted -->PyTorch Mobile:<!-- raw HTML omitted --> Cung cấp khả năng dễ dàng chuyển đổi mô hình PyTorch hiện có, nhưng sự trưởng thành của nó trên thiết bị di động không cao như các mô hình khác.</li>
</ul>
<p>Như có thể thấy, Hugging Face Optimum có thể cung cấp chuyển đổi sang các định dạng như ONNX và TFLite và áp dụng <!-- raw HTML omitted -->lượng tử hóa<!-- raw HTML omitted --> ở các định dạng đó dưới một ô dù duy nhất. Bằng cách này, một nhà phát triển muốn chạy một mô hình Transformer trên thiết bị di động có thể đạt được mục tiêu của mình bằng cách chuyển đổi nó sang định dạng phù hợp với Optimum và sau đó sử dụng thư viện di động tương ứng.</p>
<h2 id="tối-ưu-hóa-và-tích-hợp-với-hugging-face-optimum">Tối ưu hóa và Tích hợp với Hugging Face Optimum</h2>
<p><!-- raw HTML omitted -->Hugging Face Optimum<!-- raw HTML omitted -->, mà chúng tôi đã đề cập trong nhiều ngữ cảnh ở trên, là một bộ công cụ đơn giản hóa việc tối ưu hóa mô hình. Để tóm tắt, Optimum cung cấp:</p>
<ul>
<li><!-- raw HTML omitted -->Tích hợp Bộ tăng tốc Phần cứng:<!-- raw HTML omitted --> Optimum tích hợp các thư viện dành riêng cho các hệ sinh thái phần cứng và phần mềm khác nhau với Transformers. Ví dụ, mô-đun <!-- raw HTML omitted -->optimum.onnxruntime<!-- raw HTML omitted --> dành cho ONNX Runtime, <!-- raw HTML omitted -->optimum.intel<!-- raw HTML omitted --> dành cho các công cụ mã nguồn mở của Intel (OpenVINO, Intel Neural Compressor, v.v.), <!-- raw HTML omitted -->optimum.habana<!-- raw HTML omitted --> dành cho INTEL Gaudi :) , và <!-- raw HTML omitted -->optimum.graphcore<!-- raw HTML omitted --> dành cho IPU. Trong phạm vi chủ đề của chúng ta, các mô-đun <!-- raw HTML omitted -->optimum.onnxruntime<!-- raw HTML omitted --> và <!-- raw HTML omitted -->optimum.exporters<!-- raw HTML omitted --> nổi bật nhất.</li>
<li><!-- raw HTML omitted -->Giao diện Lượng tử hóa Dễ dàng:<!-- raw HTML omitted --> Optimum trình bày công cụ lượng tử hóa của ONNX Runtime dưới dạng giao diện Python đơn giản hoặc CLI. Việc lấy một mô hình Transformers (ví dụ: mô hình .onnx từ một thư mục hoặc trực tiếp từ HF Hub) và giảm nó xuống 8 bit bằng cách sử dụng lớp <!-- raw HTML omitted -->ORTQuantizer<!-- raw HTML omitted --> là rất thiết thực. Chúng tôi đã chỉ ra cách sử dụng này trong ví dụ mã ở trên. Các chế độ lượng tử hóa được hỗ trợ bao gồm <!-- raw HTML omitted -->động<!-- raw HTML omitted -->, <!-- raw HTML omitted -->tĩnh (với hiệu chuẩn)<!-- raw HTML omitted -->, và các chế độ được tối ưu hóa cho các tập lệnh bộ xử lý cụ thể. Ví dụ, bạn có thể chọn cài đặt tốt nhất cho CPU di động dựa trên ARM bằng cách gọi <!-- raw HTML omitted -->AutoQuantizationConfig.arm64()<!-- raw HTML omitted -->. Optimum cũng đã bắt đầu tích hợp các kỹ thuật lượng tử hóa mới hơn như <!-- raw HTML omitted -->GPTQ<!-- raw HTML omitted --> (lượng tử hóa LLM 4 bit).</li>
<li><!-- raw HTML omitted -->Công cụ Biên dịch và Chuyển đổi:<!-- raw HTML omitted --> Optimum xử lý việc chuyển đổi trong một bước duy nhất với các lệnh <!-- raw HTML omitted -->optimum-cli export onnx<!-- raw HTML omitted --> hoặc <!-- raw HTML omitted -->export tflite<!-- raw HTML omitted -->. Bên dưới các lệnh này, các trình chuyển đổi của Transformers hoặc ONNX thực sự đang chạy, nhưng Optimum giúp bạn dễ dàng đặt các tham số chính xác và tránh sự không khớp. Ví dụ, khi chuyển đổi sang TFLite, nếu phiên bản TensorFlow của mô hình không tồn tại ở chế độ nền, nó sẽ tự động tạo một mô hình TF và sau đó gọi Trình chuyển đổi TFLite.</li>
<li><!-- raw HTML omitted -->Tích hợp Intel Neural Compressor và NNCF:<!-- raw HTML omitted --> Nếu bạn muốn thực hiện tối ưu hóa nâng cao hơn (như kết hợp lượng tử hóa + cắt tỉa + chưng cất trong quá trình huấn luyện), mô-đun Optimum-Intel cho phép bạn thực hiện điều này trực tiếp với API Trainer. Ví dụ, áp dụng <!-- raw HTML omitted -->huấn luyện nhận biết lượng tử hóa (QAT)<!-- raw HTML omitted --> trong quá trình huấn luyện hoặc thực hiện <!-- raw HTML omitted -->cắt tỉa có cấu trúc<!-- raw HTML omitted --> với mục tiêu độ thưa cụ thể là có thể với Optimum. Các tính năng này dành cho các nhà phát triển mô hình hơn, các công cụ được tích hợp vào giai đoạn huấn luyện; nhưng kết quả là <!-- raw HTML omitted -->các mô hình siêu hiệu quả<!-- raw HTML omitted --> chạy trên thiết bị di động. Ví dụ, sử dụng Optimum-Intel, cắt tỉa có cấu trúc 90% + chưng cất đã được áp dụng cho một mô hình BERT, và mô hình kết quả đã được làm cho chạy nhanh hơn 10 lần trong khi vẫn giữ được 99% độ chính xác của bản gốc (các nghiên cứu như vậy tồn tại trong tài liệu).</li>
</ul>
<p>Nói tóm lại, Hugging Face Optimum là một công cụ quan trọng cả cho việc <!-- raw HTML omitted -->tối ưu hóa và triển khai các mô hình được huấn luyện trước<!-- raw HTML omitted --> và cho việc <!-- raw HTML omitted -->đơn giản hóa nghiên cứu nén mô hình<!-- raw HTML omitted -->. Các nhà phát triển di động được hưởng sự tiện lợi khi giảm các quy trình chuyển đổi phức tạp thành các lệnh đơn giản bằng cách sử dụng Optimum. Ví dụ, ngay cả một nhà phát triển di động có kiến thức Python hạn chế cũng có thể lượng tử hóa và chuyển đổi một mô hình bằng Optimum CLI và sau đó tích hợp nó vào ứng dụng di động của họ. Tài liệu của Hugging Face khuyến khích điều này, nói rằng, &ldquo;nếu bạn muốn chạy các mô hình của mình với hiệu quả tối đa, hãy xem thư viện Optimum.&rdquo;</p>
<h2 id="kết-luận-và-các-kịch-bản-ví-dụ">Kết luận và Các kịch bản Ví dụ</h2>
<p>Việc chạy các mô hình Transformer lớn trên thiết bị di động và thiết bị biên đã trở nên khả thi khi áp dụng các kỹ thuật phù hợp. Với các phương pháp như <!-- raw HTML omitted -->lượng tử hóa<!-- raw HTML omitted -->, <!-- raw HTML omitted -->chưng cất<!-- raw HTML omitted --> và <!-- raw HTML omitted -->cắt tỉa<!-- raw HTML omitted -->, mà chúng ta đã thảo luận trong bài viết này, việc phù hợp và tăng tốc các mô hình khổng lồ cho môi trường di động giờ đây là thực tế phổ biến. Đặc biệt, các đổi mới trong hệ sinh thái <!-- raw HTML omitted -->Hugging Face<!-- raw HTML omitted --> đã làm cho quy trình này trở nên dễ tiếp cận hơn bao giờ hết. Ví dụ, một nhà phát triển ứng dụng di động có thể dễ dàng thêm tính năng phân loại văn bản nhanh chóng và ngoại tuyến bằng cách tải xuống mô hình <!-- raw HTML omitted -->DistilBERT<!-- raw HTML omitted --> từ Hugging Face, chuyển đổi nó sang định dạng ONNX + INT8 bằng Optimum và chạy nó với ORT trong ứng dụng của họ.</p>
<p>Các ví dụ thực tế cho thấy xu hướng này đang ngày càng tăng: Apple đang thúc đẩy ML trên thiết bị trong các ứng dụng và công cụ phát triển của riêng mình nhờ Core ML; Google đang làm việc trên các mô hình ngôn ngữ được tích hợp vào điện thoại Android (NLP trên thiết bị trên thiết bị Pixel); và các công ty như Meta và NVIDIA đang khuyến khích việc sử dụng chúng trên Biên bằng cách phát hành các <!-- raw HTML omitted -->phiên bản nhỏ (Alpaca, Minitron, v.v.)<!-- raw HTML omitted --> của các mô hình lớn. Hugging Face Hub hiện có các thẻ và bộ sưu tập cho <!-- raw HTML omitted -->các mô hình tương thích với thiết bị di động<!-- raw HTML omitted -->, và các phiên bản lượng tử hóa của nhiều mô hình đang được chia sẻ.</p>
<p>Tóm lại, thông điệp cho các nhà phát triển di động và kỹ sư trí tuệ nhân tạo là: <!-- raw HTML omitted -->Bằng cách sử dụng các công cụ phù hợp, bạn có thể mang lại lợi ích của các mô hình Transformer lớn cho người dùng của mình trong môi trường di động.<!-- raw HTML omitted --> Với các kỹ thuật được đề cập trong bài viết này, ứng dụng của bạn có thể cung cấp các câu trả lời thông minh mà không cần gửi dữ liệu người dùng lên đám mây và có thể xử lý văn bản, hình ảnh hoặc âm thanh theo thời gian thực. Hơn nữa, bạn có thể thực hiện điều này trong khi cung cấp trải nghiệm thân thiện với pin và nhanh chóng bằng cách tận dụng tối đa phần cứng của thiết bị. Nhờ các công cụ mã nguồn mở đang phát triển (Optimum, ONNX Runtime, Core ML Tools, v.v.) và việc thu nhỏ liên tục các mô hình &ldquo;lớn&rdquo;, việc tích hợp AI vào một chiếc túi giờ đây không còn là một giấc mơ, mà là hiện thực của ngày hôm nay.</p>
<h3 id="link-bài-viết-gốc"><a href="https://huggingface.co/blog/tugrulkaya/running-large-transformer-models-on-mobile">Link bài viết gốc</a></h3>


    <ul id="taxonomy" class="not-prose flex items-center space-x-4 flex-nowrap whitespace-nowrap overflow-x-auto">

      <li class="font-semibold my-4">Tags:</li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/ai/">Ai</a></li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/7-ng%C3%A0y-tr%C6%B0%E1%BB%9Bc/">7 Ngày Trước</a></li>
        <li ><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300" href="/tags/huggingface.co/">Huggingface.co</a></li>
    </ul>
</div>

    <footer id="content-footer" class="not-prose">

      

      <div id="author-box" class="my-8 md:my-14 border p-8 bg-zinc-100 rounded-2xl">

        <img class="w-20 h-20 rounded-full" src="" alt="">

        <h2 class="text-xl font-bold my-4">Writter by : </h2>

        <p class="mb-4"></p>

        <ul class="flex flex-wrap space-x-4">
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://facebook.com/luat.skyai/" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                <path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951z"/>
              </svg>
              <span class="ml-2">Facebook</span>
            </a>
          </li>
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://facebook.com/luat.skyai/" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                  <path d="M12.6.75h2.454l-5.36 6.142L16 15.25h-4.937l-3.867-5.07-4.425 5.07H.316l5.733-6.57L0 .75h5.063l3.495 4.633L12.601.75Zm-.86 13.028h1.36L4.323 2.145H2.865l8.875 11.633Z"/>
              </svg>
              <span class="ml-2">Twitter</span>
            </a>
          </li>
          <li>
            <a class="flex items-center decoration-auto hover:underline" href="https://luat.skyai.vn/" target="_blank" rel="noopener">
              <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
                    <path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.917 3.917 0 0 0-1.417.923A3.927 3.927 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.916 3.916 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.926 3.926 0 0 0-.923-1.417A3.911 3.911 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0h.003zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599.28.28.453.546.598.92.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.47 2.47 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.478 2.478 0 0 1-.92-.598 2.48 2.48 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233 0-2.136.008-2.388.046-3.231.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92.28-.28.546-.453.92-.598.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045v.002zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92zm-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217zm0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334z"/>

              </svg>
              <span class="ml-2">Instagram</span>
            </a>
          </li>

        </ul>
        
      </div>

      <div id="related-post" class="">

        <h2 class="text-xl md:text-2xl font-bold mb-6 md:mb-8">Recommended for You</h2><div class="grid grid-cols-1 md:grid-cols-2 gap-6"><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl">
	
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-11-10/d6e9ef/"></a>

	<figure class="w-full aspect-video overflow-hidden rounded-3xl">

			<img class="object-cover group-hover:scale-105 transition duration-500 cursor-pointer" src="https://cdn-avatars.huggingface.co/v1/production/uploads/667d6f513a685cd30f542fef/ipB8pJRbVIK2FJOOEGUjC.png" alt="Bảng xếp hạng so sánh- AI - Từ lượt bình chọn của người dùng đến bảng xếp hạng mô hình dựa trên sự tham gia" style="width: 100%; height: 100%; object-fit: cover;">

	  </figure>

	<div class="p-6">

		<time datetime="2025-11-10T00:00:00&#43;00:00">Nov 10, 2025</time>

		<h3 class="my-4 text-2xl font-bold">Bảng xếp hạng so sánh- AI - Từ lượt bình chọn của người dùng đến bảng xếp hạng mô hình dựa trên sự tham gia</h3>

		<p class="text-normal leading-normal text-zinc-500 line-clamp-2">Bảng xếp hạng so sánh- AI - Từ lượt bình chọn của người dùng đến bảng xếp hạng mô hình dựa trên sự tham gia</p>

	</div>
</article><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl">
	
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2025-11-10/fd0ff0/"></a>

	<figure class="w-full aspect-video overflow-hidden rounded-3xl">

			<img class="object-cover group-hover:scale-105 transition duration-500 cursor-pointer" src="https://cdn-avatars.huggingface.co/v1/production/uploads/6051e59531c5be7f3dd5ebc9/T0o_eKK5gmZoG-qbbj3lM.jpeg" alt="Hướng tới An toàn do Cộng đồng Quản lý" style="width: 100%; height: 100%; object-fit: cover;">

	  </figure>

	<div class="p-6">

		<time datetime="2025-11-10T00:00:00&#43;00:00">Nov 10, 2025</time>

		<h3 class="my-4 text-2xl font-bold">Hướng tới An toàn do Cộng đồng Quản lý</h3>

		<p class="text-normal leading-normal text-zinc-500 line-clamp-2">Hướng tới An toàn do Cộng đồng Quản lý</p>

	</div>
</article>
          
        </div>
      </div>

    </footer>

  </article>

  
  <aside class="md:col-span-1"><div class="lg:sticky lg:top-8"><div class="rounded-2xl p-4 bg-zinc-100 mb-10">

		<img class="aspect-video rounded" src="/images/email-3249062_1280_by-Muhammad-Ribkhan-Pixabay.jpg" alt="Banner Email Marketing">

		<p class="text-right text-xs mt-2 leading-none text-zinc-500">Adversitiment</p>

</div><div class="space-y-6">

		<h2 class="font-bold text-xl mb-8">Recent Post</h2>

		<article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2026-01-24/6d02b0/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://www.anthropic.com" alt="Anthropic bổ nhiệm Irina Ghose làm Giám đốc Điều hành Ấn Độ trước lễ khai trương văn phòng Bengaluru" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Anthropic bổ nhiệm Irina Ghose làm Giám đốc Điều hành Ấn Độ trước lễ khai trương văn phòng Bengaluru</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2026-01-24/727a3f/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://www.anthropic.com" alt="Anthropic và Teach For All ra mắt sáng kiến đào tạo AI toàn cầu cho các nhà giáo dục" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Anthropic và Teach For All ra mắt sáng kiến đào tạo AI toàn cầu cho các nhà giáo dục</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2026-01-24/e5d49f/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://cdn-uploads.huggingface.co/production/uploads/64c47f731d44fc06afc80953/fB0GHxhoG5McKZtklVj_I.png" alt="AssetOpsBench- Thu hẹp khoảng cách giữa các tiêu chuẩn đánh giá tác tử AI và thực tế công nghiệp" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">AssetOpsBench- Thu hẹp khoảng cách giữa các tiêu chuẩn đánh giá tác tử AI và thực tế công nghiệp</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2026-01-24/f4be01/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://www.anthropic.com" alt="Báo cáo Chỉ số Kinh tế Anthropic- các nguyên tắc kinh tế cơ bản" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Báo cáo Chỉ số Kinh tế Anthropic- các nguyên tắc kinh tế cơ bản</h3>
	</div>
</article><article class="relative group flex flex-row">
	<a class="insert-link" href="https://aitoday.skyai.vn/posts/2026-01-24/e123e7/"></a>
	<figure class="basis-1/3 aspect-square overflow-hidden rounded-2xl bg-zinc-100">

		  <img class="object-cover h-full w-full group-hover:scale-105 transition duration-500 cursor-pointer" src="https://www.anthropic.com" alt="Các nhà khoa học đang sử dụng Claude để đẩy nhanh nghiên cứu và khám phá" style="width: 100%; height: 100%; object-fit: cover;" style="width: 100%; height: 100%; object-fit: cover;">

	</figure>
	<div class="basis-2/3 self-center ml-4">
		<h3 class="font-bold group-hover:underline decoration-auto">Các nhà khoa học đang sử dụng Claude để đẩy nhanh nghiên cứu và khám phá</h3>
	</div>
</article>
		
	</div>

</div>
</aside>

</div>
  
</main>
  <footer class="bg-zinc-100 py-10 md:py-14"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8">

  <div class="flex flex-wrap space-y-6 mb-4">

    <div class="w-full md:w-3/5 flex flex-col space-y-4 md:pr-8 lg:pr-10">

      <a class="flex items-center group" href="https://aitoday.skyai.vn/">
        <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" fill="currentColor" class="mr-2 group-hover:animate-spin" viewBox="0 0 16 16">
          <path d="M8 11a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/>
          <path d="M13.997 5.17a5 5 0 0 0-8.101-4.09A5 5 0 0 0 1.28 9.342a5 5 0 0 0 8.336 5.109 3.5 3.5 0 0 0 5.201-4.065 3.001 3.001 0 0 0-.822-5.216zm-1-.034a1 1 0 0 0 .668.977 2.001 2.001 0 0 1 .547 3.478 1 1 0 0 0-.341 1.113 2.5 2.5 0 0 1-3.715 2.905 1 1 0 0 0-1.262.152 4 4 0 0 1-6.67-4.087 1 1 0 0 0-.2-1 4 4 0 0 1 3.693-6.61 1 1 0 0 0 .8-.2 4 4 0 0 1 6.48 3.273z"/>
        </svg>
        
        <span class="text-2xl font-semibold uppercase">AI Today - SkyAI</span>
      </a>

      <p class="font-semibold">
        Theo dõi các tiến bộ mới nhất về Trí tuệ nhân tạo.<br>Trực tiếp từ các nhà phát hành AI trên thế giới.
      </p>

      <p>Đem trí tuệ nhân tạo đến mọi người dân, doanh nghiệp Việt, góp phần giúp Việt Nam phát triển mạnh mẽ trong kỷ nguyên số. Nội dung được cập nhật tự động bằng máy.</p>

    </div>

    <div class="self-center flex flex-col w-full md:w-2/5">

<ul id="social-media" class="flex items-center space-x-4">
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.facebook.com/luat.skyai/" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 16 16">
				<path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.twitter.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M12.6.75h2.454l-5.36 6.142L16 15.25h-4.937l-3.867-5.07-4.425 5.07H.316l5.733-6.57L0 .75h5.063l3.495 4.633L12.601.75Zm-.86 13.028h1.36L4.323 2.145H2.865l8.875 11.633Z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.github.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
			</svg>
		</a>
	</li>
	<li>
		<a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href="https://www.instagram.com/#" target="_blank" rel="noopener noreferrer">
			<svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="currentColor" viewBox="0 0 16 16">
				  <path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.917 3.917 0 0 0-1.417.923A3.927 3.927 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.916 3.916 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.926 3.926 0 0 0-.923-1.417A3.911 3.911 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0h.003zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599.28.28.453.546.598.92.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.47 2.47 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.478 2.478 0 0 1-.92-.598 2.48 2.48 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233 0-2.136.008-2.388.046-3.231.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92.28-.28.546-.453.92-.598.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045v.002zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92zm-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217zm0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334z"></path>
			</svg>
		</a>
	</li>
</ul>

    </div>
    
  </div>

  <div class="my-8">
    <ul class="flex items-center space-x-4">
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/">Home</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="">About</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/tags/">Tags</a></li>
      
        <li><a class="decoration-auto hover:underline font-semibold" href="/tags/">Taxonomy</a></li>
      
    </ul>
  </div>

  <div class="border-t pt-4">

    <p class="text-sm">Copyright © 2026. All rights reserved. <a href="https://home.skyai.vn/" target="_blank" rel="noopener">SkyAI.vn</a></p>

  </div>
  
</div>
</footer>
      <script defer src="/js/insertoggle.474f9b0e08021c6519cff4e46df14ccf148285b2d3a23d6321d6e10f25c291fb.js" integrity="sha256-R0&#43;bDggCHGUZz/TkbfFMzxSChbLToj1jIdbhDyXCkfs=" crossorigin="anonymous"></script>
</body>
</html>