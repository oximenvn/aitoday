<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>June 27, 2025 on AI Today - SkyAI</title>
    <link>https://aitoday.skyai.vn/tags/june-27-2025/</link>
    <description>Recent content in June 27, 2025 on AI Today - SkyAI</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 28 Jun 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://aitoday.skyai.vn/tags/june-27-2025/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Mô hình hóa động lực hội thoại tự nhiên với Tương tác Liền mạch</title>
      <link>https://aitoday.skyai.vn/posts/2025-06-28/f6e3c8/</link>
      <pubDate>Sat, 28 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://aitoday.skyai.vn/posts/2025-06-28/f6e3c8/</guid>
      <description>&lt;h1 id=&#34;mô-hình-hóa-động-lực-hội-thoại-tự-nhiên-với-tương-tác-liền-mạch&#34;&gt;Mô hình hóa động lực hội thoại tự nhiên với Tương tác liền mạch&lt;/h1&gt;&#xA;&lt;h3 id=&#34;tóm-tắt&#34;&gt;Tóm tắt:&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Khi chúng ta nỗ lực xây dựng tương lai của kết nối giữa con người và công nghệ hỗ trợ, chúng ta sẽ cần các mô hình có thể tạo ra biểu cảm khuôn mặt và cử chỉ cơ thể dựa trên các đầu vào nghe nhìn từ hai người. Meta Fundamental AI Research (FAIR) đang giới thiệu một loạt các mô hình chuyển động hành vi nghe nhìn để đáp ứng nhu cầu đó.&lt;/li&gt;&#xA;&lt;li&gt;Các mô hình của chúng tôi cho phép tạo ra các avatar hoàn toàn thể hiện trong video 2D và dưới dạng Codec Avatars 3D và có thể cho phép tạo ra các tác nhân ảo tự nhiên và tương tác hơn, những tác nhân này có thể tham gia vào các cuộc trò chuyện với các cử chỉ và biểu cảm giống con người, chuyển đổi công nghệ hiện diện từ xa trong cài đặt VR và AR.&lt;/li&gt;&#xA;&lt;li&gt;Chúng tôi cũng đang phát hành Bộ dữ liệu Tương tác Liền mạch, một bộ dữ liệu quy mô lớn với hơn 4.000 giờ tương tác giữa hai người và hơn 4.000 người tham gia duy nhất, mà chúng tôi đã sử dụng để đào tạo các mô hình của mình. Bộ dữ liệu của chúng tôi ghi lại một loạt các tương tác trực tiếp giữa hai cá nhân, cho phép các mô hình hành vi nghe nhìn của chúng tôi hiểu và tạo ra các hành vi xã hội giống con người.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;p&gt;Giao tiếp giữa mọi người giống như một điệu nhảy, mỗi người liên tục điều chỉnh những gì họ nói, cách họ nói và cách họ ra hiệu. Mô hình hóa động lực hội thoại hai bên, hay còn gọi là nhị nguyên, đòi hỏi phải hiểu mối quan hệ đa phương thức giữa các tín hiệu xã hội bằng lời nói, giọng nói và hình ảnh — và các hành vi giữa các cá nhân giữa mọi người, chẳng hạn như lắng nghe, đồng bộ hóa trực quan và thay phiên nhau. Khi các tác nhân ảo trở thành những trợ lý quan trọng trong cuộc sống hàng ngày của chúng ta, điều quan trọng là các hệ thống này có thể hiển thị các mô hình hội thoại tự nhiên này. Hôm nay, nhóm Nghiên cứu AI Cơ bản Meta (FAIR) cùng với phòng thí nghiệm Codec Avatars của Meta và phòng thí nghiệm Core AI đang giới thiệu một loạt Mô hình Chuyển động Nhị nguyên khám phá các lĩnh vực mới của AI xã hội. Các mô hình này kết xuất lời nói do con người hoặc mô hình ngôn ngữ tạo ra giữa hai cá nhân thành các cử chỉ toàn thân đa dạng, biểu cảm và các hành vi lắng nghe tích cực, cho phép tạo ra các avatar hoàn toàn thể hiện trong video 2D và dưới dạng Codec Avatars 3D. Các mô hình xử lý các đầu vào âm thanh và hình ảnh để ghi lại các động lực hội thoại sắc thái với tiềm năng cuối cùng là tạo ra các tác nhân ảo tương tác, tự nhiên hơn, có thể tham gia vào các tương tác xã hội giống con người trên nhiều cài đặt nhập vai khác nhau.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
