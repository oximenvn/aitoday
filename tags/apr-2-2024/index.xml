<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apr 2, 2024 on AI Today - SkyAI</title>
    <link>https://aitoday.skyai.vn/tags/apr-2-2024/</link>
    <description>Recent content in Apr 2, 2024 on AI Today - SkyAI</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Nov 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://aitoday.skyai.vn/tags/apr-2-2024/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bẻ khóa nhiều lượt</title>
      <link>https://aitoday.skyai.vn/posts/2025-11-10/6c75de/</link>
      <pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://aitoday.skyai.vn/posts/2025-11-10/6c75de/</guid>
      <description>&lt;h1 id=&#34;many-shot-jailbreaking&#34;&gt;Many-shot jailbreaking&lt;/h1&gt;&#xA;&lt;p&gt;Ngày 2 tháng 4 năm 2024&lt;/p&gt;&#xA;&lt;p&gt;Anthropic đã điều tra một kỹ thuật &amp;ldquo;bẻ khóa&amp;rdquo; (jailbreaking) - một phương pháp có thể được sử dụng để né tránh các biện pháp an toàn đã được thiết lập bởi các nhà phát triển mô hình ngôn ngữ lớn (LLM). Kỹ thuật này, được gọi là &amp;ldquo;many-shot jailbreaking&amp;rdquo;, có hiệu quả trên các mô hình của Anthropic và cả các mô hình do các công ty AI khác sản xuất. Chúng tôi đã thông báo cho các nhà phát triển AI khác về lỗ hổng này trước đó và đã triển khai các biện pháp giảm thiểu trên hệ thống của mình.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
